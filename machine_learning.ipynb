{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5XLSnFHrxIc"
   },
   "source": [
    "## Machine Learning project\n",
    "[MI-net, instance space](#section_id)\n",
    "[MI-net deep supervision, instance space](#MInetdeepsuper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBvZXBosr_uG",
    "outputId": "1936e27c-5e3f-4272-b413-f602a4b73b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/chlorochrule/cknn (from -r requirements.txt (line 9))\n",
      "  Cloning https://github.com/chlorochrule/cknn to c:\\users\\seven\\appdata\\local\\temp\\pip-req-build-ix2t65th\n",
      "  Resolved https://github.com/chlorochrule/cknn to commit 7d05c5049da72a573bd486fca6647f8b0376243c\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting misvm\n",
      "  Cloning https://github.com/garydoranjr/misvm.git to c:\\users\\seven\\appdata\\local\\temp\\pip-install-5h6zfwts\\misvm_5380924db1b440b8999ab150a7cc8595\n",
      "  Resolved https://github.com/garydoranjr/misvm.git to commit b2118fe04d98c00436bdf8a0e4bbfb6082c5751c\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: keras in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.11.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from -r requirements.txt (line 2)) (0.0.post1)\n",
      "Requirement already satisfied: numpy in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from -r requirements.txt (line 3)) (1.23.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from -r requirements.txt (line 5)) (3.6.2)\n",
      "Requirement already satisfied: jupyter in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from -r requirements.txt (line 6)) (1.0.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from -r requirements.txt (line 7)) (1.9.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from -r requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from -r requirements.txt (line 10)) (2.11.0)\n",
      "Requirement already satisfied: cvxopt in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from -r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2022.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.0.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (9.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: notebook in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter->-r requirements.txt (line 6)) (6.5.2)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter->-r requirements.txt (line 6)) (5.4.0)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter->-r requirements.txt (line 6)) (7.2.5)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter->-r requirements.txt (line 6)) (6.4.4)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter->-r requirements.txt (line 6)) (6.17.1)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter->-r requirements.txt (line 6)) (8.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from cknn==0.1.0->-r requirements.txt (line 9)) (1.1.3)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow->-r requirements.txt (line 10)) (2.11.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (22.11.23)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (4.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (1.14.1)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (2.1.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (1.50.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (2.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (0.28.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (14.0.6)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (3.19.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (3.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (58.1.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (24.0.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (0.1.6)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (5.5.0)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (1.5.6)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (8.6.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (7.4.7)\n",
      "Requirement already satisfied: psutil in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (5.9.4)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (6.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipywidgets->jupyter->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipywidgets->jupyter->-r requirements.txt (line 6)) (4.0.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter-console->jupyter->-r requirements.txt (line 6)) (3.0.33)\n",
      "Requirement already satisfied: pygments in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter-console->jupyter->-r requirements.txt (line 6)) (2.13.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (2.1.1)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (1.2.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (5.0.1)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (2.0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (5.0.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (1.5.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (4.11.1)\n",
      "Requirement already satisfied: nbformat>=5.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (5.7.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from notebook->jupyter->-r requirements.txt (line 6)) (0.17.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from notebook->jupyter->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from notebook->jupyter->-r requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from notebook->jupyter->-r requirements.txt (line 6)) (21.3.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from notebook->jupyter->-r requirements.txt (line 6)) (0.15.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from notebook->jupyter->-r requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from qtconsole->jupyter->-r requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from scikit-learn->cknn==0.1.0->-r requirements.txt (line 9)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from scikit-learn->cknn==0.1.0->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (0.38.4)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.7.5)\n",
      "Requirement already satisfied: stack-data in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.6.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.18.2)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->-r requirements.txt (line 6)) (0.4)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter-core>=4.7->nbconvert->jupyter->-r requirements.txt (line 6)) (305)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter-core>=4.7->nbconvert->jupyter->-r requirements.txt (line 6)) (2.5.4)\n",
      "Requirement already satisfied: notebook-shim>=0.1.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbclassic>=0.4.7->notebook->jupyter->-r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbclassic>=0.4.7->notebook->jupyter->-r requirements.txt (line 6)) (1.23.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbformat>=5.1->nbconvert->jupyter->-r requirements.txt (line 6)) (4.17.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from nbformat>=5.1->nbconvert->jupyter->-r requirements.txt (line 6)) (2.16.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->-r requirements.txt (line 6)) (0.2.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (2.14.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from terminado>=0.8.3->notebook->jupyter->-r requirements.txt (line 6)) (2.0.9)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from argon2-cffi->notebook->jupyter->-r requirements.txt (line 6)) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 6)) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 6)) (0.5.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->-r requirements.txt (line 6)) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->-r requirements.txt (line 6)) (0.19.2)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->-r requirements.txt (line 6)) (3.6.2)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->-r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (1.26.12)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r requirements.txt (line 6)) (1.15.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r requirements.txt (line 6)) (2.21)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\seven\\documents\\github\\machinelearning\\venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 10)) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/chlorochrule/cknn 'C:\\Users\\seven\\AppData\\Local\\Temp\\pip-req-build-ix2t65th'\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/garydoranjr/misvm.git 'C:\\Users\\seven\\AppData\\Local\\Temp\\pip-install-5h6zfwts\\misvm_5380924db1b440b8999ab150a7cc8595'\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75T7R_ehr3MU"
   },
   "source": [
    "### Embedded-Space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI-net\n",
    "<a id='section_id'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "_sDih52xsYL9",
    "outputId": "6e803c46-abda-4946-cbbe-6098086e21da"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from random import shuffle\n",
    "import argparse\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Layer, Dropout\n",
    "\n",
    "from mil_nets.dataset import load_dataset\n",
    "from mil_nets.layer import Feature_pooling\n",
    "from mil_nets.metrics import bag_accuracy\n",
    "from mil_nets.objectives import bag_loss\n",
    "from mil_nets.utils import convertToBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xRf0rM6oahO",
    "outputId": "f0adfdcc-8304-439b-97fd-da021eb8608e"
   },
   "outputs": [],
   "source": [
    "def test_eval(model, test_set):\n",
    "    \"\"\"Evaluate on testing set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training MI-Net model.\n",
    "    test_set : list\n",
    "        A list of testing set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on testing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_test_batch = len(test_set)\n",
    "    test_loss = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    test_acc = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    for ibatch, batch in enumerate(test_set):\n",
    "        result = model.test_on_batch({'input':batch[0].astype(np.float32)}, {'fp':batch[1].astype(np.float32)})\n",
    "        test_loss[ibatch] = result[0]\n",
    "        test_acc[ibatch][0] = result[1]\n",
    "    return np.mean(test_loss), np.mean(test_acc)\n",
    "\n",
    "def train_eval(model, train_set):\n",
    "    \"\"\"Evaluate on training set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training MI-Net model.\n",
    "    train_set : list\n",
    "        A list of training set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on traing set..astype(np.float32)\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_train_batch = len(train_set)\n",
    "    train_loss = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    train_acc = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    shuffle(train_set)\n",
    "    for ibatch, batch in enumerate(train_set):\n",
    "        result = model.train_on_batch({'input':batch[0].astype(np.float32)}, {'fp':batch[1].astype(np.float32)})\n",
    "        train_loss[ibatch] = result[0]\n",
    "        train_acc[ibatch][0] = result[1]\n",
    "    return np.mean(train_loss), np.mean(train_acc)\n",
    "\n",
    "def MI_Net(dataset):\n",
    "    \"\"\"Train and evaluate on MI-Net.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    dataset : dict\n",
    "        A dictionary contains all dataset information. We split train/test by keys.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_acc : float\n",
    "        Testing accuracy of MI-Net.\n",
    "    \"\"\"\n",
    "    weight_decay=0.005\n",
    "    init_lr=5e-4\n",
    "    pooling_mode='max'\n",
    "    momentum=0.9\n",
    "    max_epoch=50\n",
    "    # load data and convert type\n",
    "    train_bags = dataset['train']\n",
    "    test_bags = dataset['test']\n",
    "\n",
    "    # convert bag to batch\n",
    "    train_set = convertToBatch(train_bags)\n",
    "    test_set = convertToBatch(test_bags)\n",
    "    dimension = train_set[0][0].shape[1]\n",
    "\n",
    "    # data: instance feature, n*d, n = number of training instance\n",
    "    data_input = Input(shape=(dimension,), dtype='float32', name='input')\n",
    "\n",
    "    # fully-connected\n",
    "    fc1 = Dense(256, activation='relu', kernel_regularizer=l2(weight_decay))(data_input)\n",
    "    fc2 = Dense(128, activation='relu', kernel_regularizer=l2(weight_decay))(fc1)\n",
    "    fc3 = Dense(64, activation='relu', kernel_regularizer=l2(weight_decay))(fc2)\n",
    "\n",
    "    # dropout\n",
    "    dropout = Dropout(rate=0.5)(fc3)\n",
    "\n",
    "    # features pooling\n",
    "    fp = Feature_pooling(output_dim=1, kernel_regularizer=l2(weight_decay), pooling_mode=pooling_mode, name='fp')(dropout)\n",
    "\n",
    "    model = Model(inputs=[data_input], outputs=[fp])\n",
    "    sgd = SGD(lr=init_lr, decay=1e-4, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss=bag_loss, optimizer=sgd, metrics=[bag_accuracy])\n",
    "\n",
    "    # train model\n",
    "    t1 = time.time()\n",
    "    num_batch = len(train_set)\n",
    "    for epoch in range(max_epoch):\n",
    "        train_loss, train_acc = train_eval(model, train_set)\n",
    "        test_loss, test_acc = test_eval(model, test_set)\n",
    "        print('epoch=', epoch, '  train_loss= {:.3f}'.format(train_loss), '  train_acc= {:.3f}'.format(train_acc), '  test_loss={:.3f}'.format(test_loss), '  test_acc= {:.3f}'.format(test_acc))\n",
    "    t2 = time.time()\n",
    "    print('run time:', (t2-t1) / 60, 'min')\n",
    "    print('test_acc={:.3f}'.format(test_acc))\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xRf0rM6oahO",
    "outputId": "f0adfdcc-8304-439b-97fd-da021eb8608e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run= 0   fold= 0\n",
      "epoch= 0   train_loss= 2.966   train_acc= 0.598   test_loss=2.917   test_acc= 0.500\n",
      "epoch= 1   train_loss= 2.684   train_acc= 0.817   test_loss=2.781   test_acc= 0.800\n",
      "epoch= 2   train_loss= 2.531   train_acc= 0.939   test_loss=2.699   test_acc= 0.700\n",
      "epoch= 3   train_loss= 2.424   train_acc= 0.939   test_loss=2.654   test_acc= 0.800\n",
      "epoch= 4   train_loss= 2.362   train_acc= 0.976   test_loss=2.609   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.351   train_acc= 0.951   test_loss=2.544   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.305   train_acc= 0.976   test_loss=2.535   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.284   train_acc= 0.976   test_loss=2.495   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.223   train_acc= 0.988   test_loss=2.463   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.218   train_acc= 0.988   test_loss=2.431   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.203   train_acc= 0.988   test_loss=2.427   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.169   train_acc= 0.988   test_loss=2.400   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.149   train_acc= 0.976   test_loss=2.346   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.120   train_acc= 1.000   test_loss=2.333   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.102   train_acc= 1.000   test_loss=2.343   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.083   train_acc= 1.000   test_loss=2.279   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.074   train_acc= 1.000   test_loss=2.275   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.051   train_acc= 1.000   test_loss=2.254   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.039   train_acc= 1.000   test_loss=2.270   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.023   train_acc= 1.000   test_loss=2.257   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.017   train_acc= 1.000   test_loss=2.257   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.004   train_acc= 1.000   test_loss=2.210   test_acc= 0.900\n",
      "epoch= 22   train_loss= 1.985   train_acc= 1.000   test_loss=2.204   test_acc= 0.900\n",
      "epoch= 23   train_loss= 1.963   train_acc= 1.000   test_loss=2.199   test_acc= 0.900\n",
      "epoch= 24   train_loss= 1.955   train_acc= 1.000   test_loss=2.173   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.938   train_acc= 1.000   test_loss=2.158   test_acc= 0.900\n",
      "epoch= 26   train_loss= 1.925   train_acc= 1.000   test_loss=2.144   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.913   train_acc= 1.000   test_loss=2.134   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.902   train_acc= 1.000   test_loss=2.113   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.894   train_acc= 1.000   test_loss=2.099   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.873   train_acc= 1.000   test_loss=2.090   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.868   train_acc= 1.000   test_loss=2.077   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.858   train_acc= 1.000   test_loss=2.074   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.840   train_acc= 1.000   test_loss=2.058   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.829   train_acc= 1.000   test_loss=2.033   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.820   train_acc= 1.000   test_loss=2.029   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.808   train_acc= 1.000   test_loss=2.021   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.798   train_acc= 1.000   test_loss=2.002   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.784   train_acc= 1.000   test_loss=1.993   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.773   train_acc= 1.000   test_loss=1.980   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.764   train_acc= 1.000   test_loss=1.965   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.757   train_acc= 1.000   test_loss=1.965   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.742   train_acc= 1.000   test_loss=1.944   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.740   train_acc= 1.000   test_loss=1.943   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.723   train_acc= 1.000   test_loss=1.926   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.714   train_acc= 1.000   test_loss=1.905   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.705   train_acc= 1.000   test_loss=1.896   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.697   train_acc= 1.000   test_loss=1.891   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.682   train_acc= 1.000   test_loss=1.878   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.674   train_acc= 1.000   test_loss=1.867   test_acc= 0.900\n",
      "run time: 0.7228395819664002 min\n",
      "test_acc=0.900\n",
      "run= 0   fold= 1\n",
      "epoch= 0   train_loss= 3.021   train_acc= 0.610   test_loss=2.864   test_acc= 0.800\n",
      "epoch= 1   train_loss= 2.669   train_acc= 0.817   test_loss=2.729   test_acc= 0.800\n",
      "epoch= 2   train_loss= 2.468   train_acc= 0.939   test_loss=2.716   test_acc= 0.800\n",
      "epoch= 3   train_loss= 2.432   train_acc= 0.927   test_loss=2.703   test_acc= 0.800\n",
      "epoch= 4   train_loss= 2.384   train_acc= 0.939   test_loss=2.653   test_acc= 0.800\n",
      "epoch= 5   train_loss= 2.322   train_acc= 0.988   test_loss=2.662   test_acc= 0.800\n",
      "epoch= 6   train_loss= 2.274   train_acc= 0.988   test_loss=2.649   test_acc= 0.800\n",
      "epoch= 7   train_loss= 2.262   train_acc= 0.976   test_loss=2.602   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.238   train_acc= 0.976   test_loss=2.602   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.189   train_acc= 1.000   test_loss=2.625   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.162   train_acc= 1.000   test_loss=2.609   test_acc= 0.800\n",
      "epoch= 11   train_loss= 2.149   train_acc= 1.000   test_loss=2.604   test_acc= 0.800\n",
      "epoch= 12   train_loss= 2.137   train_acc= 1.000   test_loss=2.613   test_acc= 0.800\n",
      "epoch= 13   train_loss= 2.116   train_acc= 1.000   test_loss=2.582   test_acc= 0.800\n",
      "epoch= 14   train_loss= 2.106   train_acc= 1.000   test_loss=2.580   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.079   train_acc= 1.000   test_loss=2.578   test_acc= 0.800\n",
      "epoch= 16   train_loss= 2.061   train_acc= 1.000   test_loss=2.579   test_acc= 0.800\n",
      "epoch= 17   train_loss= 2.059   train_acc= 1.000   test_loss=2.562   test_acc= 0.800\n",
      "epoch= 18   train_loss= 2.033   train_acc= 1.000   test_loss=2.562   test_acc= 0.800\n",
      "epoch= 19   train_loss= 2.016   train_acc= 1.000   test_loss=2.554   test_acc= 0.800\n",
      "epoch= 20   train_loss= 2.011   train_acc= 1.000   test_loss=2.539   test_acc= 0.800\n",
      "epoch= 21   train_loss= 1.988   train_acc= 1.000   test_loss=2.528   test_acc= 0.800\n",
      "epoch= 22   train_loss= 1.979   train_acc= 1.000   test_loss=2.514   test_acc= 0.800\n",
      "epoch= 23   train_loss= 1.969   train_acc= 1.000   test_loss=2.508   test_acc= 0.800\n",
      "epoch= 24   train_loss= 1.959   train_acc= 0.988   test_loss=2.484   test_acc= 0.800\n",
      "epoch= 25   train_loss= 1.939   train_acc= 1.000   test_loss=2.474   test_acc= 0.800\n",
      "epoch= 26   train_loss= 1.933   train_acc= 1.000   test_loss=2.475   test_acc= 0.800\n",
      "epoch= 27   train_loss= 1.925   train_acc= 1.000   test_loss=2.443   test_acc= 0.800\n",
      "epoch= 28   train_loss= 1.902   train_acc= 1.000   test_loss=2.437   test_acc= 0.800\n",
      "epoch= 29   train_loss= 1.913   train_acc= 0.988   test_loss=2.376   test_acc= 0.800\n",
      "epoch= 30   train_loss= 1.882   train_acc= 1.000   test_loss=2.393   test_acc= 0.800\n",
      "epoch= 31   train_loss= 1.863   train_acc= 1.000   test_loss=2.401   test_acc= 0.800\n",
      "epoch= 32   train_loss= 1.856   train_acc= 1.000   test_loss=2.389   test_acc= 0.800\n",
      "epoch= 33   train_loss= 1.841   train_acc= 1.000   test_loss=2.381   test_acc= 0.800\n",
      "epoch= 34   train_loss= 1.833   train_acc= 1.000   test_loss=2.385   test_acc= 0.800\n",
      "epoch= 35   train_loss= 1.822   train_acc= 1.000   test_loss=2.387   test_acc= 0.800\n",
      "epoch= 36   train_loss= 1.814   train_acc= 1.000   test_loss=2.363   test_acc= 0.800\n",
      "epoch= 37   train_loss= 1.792   train_acc= 1.000   test_loss=2.353   test_acc= 0.800\n",
      "epoch= 38   train_loss= 1.787   train_acc= 1.000   test_loss=2.338   test_acc= 0.800\n",
      "epoch= 39   train_loss= 1.774   train_acc= 1.000   test_loss=2.330   test_acc= 0.800\n",
      "epoch= 40   train_loss= 1.767   train_acc= 1.000   test_loss=2.306   test_acc= 0.800\n",
      "epoch= 41   train_loss= 1.752   train_acc= 1.000   test_loss=2.304   test_acc= 0.800\n",
      "epoch= 42   train_loss= 1.746   train_acc= 1.000   test_loss=2.280   test_acc= 0.800\n",
      "epoch= 43   train_loss= 1.738   train_acc= 1.000   test_loss=2.267   test_acc= 0.800\n",
      "epoch= 44   train_loss= 1.729   train_acc= 1.000   test_loss=2.270   test_acc= 0.800\n",
      "epoch= 45   train_loss= 1.711   train_acc= 1.000   test_loss=2.267   test_acc= 0.800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 46   train_loss= 1.704   train_acc= 1.000   test_loss=2.261   test_acc= 0.800\n",
      "epoch= 47   train_loss= 1.692   train_acc= 1.000   test_loss=2.253   test_acc= 0.800\n",
      "epoch= 48   train_loss= 1.683   train_acc= 1.000   test_loss=2.248   test_acc= 0.800\n",
      "epoch= 49   train_loss= 1.674   train_acc= 1.000   test_loss=2.240   test_acc= 0.800\n",
      "run time: 0.7714118798573811 min\n",
      "test_acc=0.800\n",
      "run= 0   fold= 2\n",
      "epoch= 0   train_loss= 2.976   train_acc= 0.602   test_loss=2.805   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.680   train_acc= 0.843   test_loss=2.599   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.536   train_acc= 0.952   test_loss=2.561   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.489   train_acc= 0.916   test_loss=2.583   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.416   train_acc= 0.952   test_loss=2.438   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.368   train_acc= 0.964   test_loss=2.453   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.333   train_acc= 0.964   test_loss=2.442   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.291   train_acc= 1.000   test_loss=2.389   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.267   train_acc= 0.988   test_loss=2.362   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.252   train_acc= 0.976   test_loss=2.465   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.233   train_acc= 0.988   test_loss=2.347   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.177   train_acc= 0.988   test_loss=2.348   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.162   train_acc= 1.000   test_loss=2.312   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.140   train_acc= 1.000   test_loss=2.318   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.121   train_acc= 1.000   test_loss=2.288   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.103   train_acc= 1.000   test_loss=2.286   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.097   train_acc= 1.000   test_loss=2.278   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.072   train_acc= 1.000   test_loss=2.251   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.059   train_acc= 1.000   test_loss=2.228   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.034   train_acc= 1.000   test_loss=2.210   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.021   train_acc= 1.000   test_loss=2.178   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.012   train_acc= 1.000   test_loss=2.175   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.997   train_acc= 1.000   test_loss=2.169   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.982   train_acc= 1.000   test_loss=2.145   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.979   train_acc= 1.000   test_loss=2.122   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.953   train_acc= 1.000   test_loss=2.108   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.941   train_acc= 1.000   test_loss=2.118   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.928   train_acc= 1.000   test_loss=2.105   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.919   train_acc= 1.000   test_loss=2.073   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.905   train_acc= 1.000   test_loss=2.043   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.897   train_acc= 1.000   test_loss=2.087   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.880   train_acc= 1.000   test_loss=2.026   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.868   train_acc= 1.000   test_loss=2.012   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.853   train_acc= 1.000   test_loss=1.993   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.847   train_acc= 1.000   test_loss=2.010   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.835   train_acc= 1.000   test_loss=1.982   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.823   train_acc= 1.000   test_loss=1.967   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.807   train_acc= 1.000   test_loss=1.949   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.797   train_acc= 1.000   test_loss=1.955   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.791   train_acc= 1.000   test_loss=1.932   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.777   train_acc= 1.000   test_loss=1.923   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.765   train_acc= 1.000   test_loss=1.914   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.764   train_acc= 1.000   test_loss=1.910   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.745   train_acc= 1.000   test_loss=1.906   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.737   train_acc= 1.000   test_loss=1.887   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.725   train_acc= 1.000   test_loss=1.886   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.715   train_acc= 1.000   test_loss=1.872   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.704   train_acc= 1.000   test_loss=1.864   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.695   train_acc= 1.000   test_loss=1.849   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.684   train_acc= 1.000   test_loss=1.826   test_acc= 0.889\n",
      "run time: 0.862456218401591 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 3\n",
      "epoch= 0   train_loss= 2.900   train_acc= 0.639   test_loss=2.878   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.571   train_acc= 0.880   test_loss=2.668   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.488   train_acc= 0.916   test_loss=2.647   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.439   train_acc= 0.928   test_loss=2.634   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.374   train_acc= 0.964   test_loss=2.535   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.315   train_acc= 0.976   test_loss=2.524   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.279   train_acc= 0.988   test_loss=2.532   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.283   train_acc= 0.976   test_loss=2.552   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.229   train_acc= 1.000   test_loss=2.410   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.206   train_acc= 0.988   test_loss=2.419   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.176   train_acc= 1.000   test_loss=2.423   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.147   train_acc= 1.000   test_loss=2.455   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.135   train_acc= 1.000   test_loss=2.398   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.116   train_acc= 1.000   test_loss=2.388   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.106   train_acc= 1.000   test_loss=2.319   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.084   train_acc= 1.000   test_loss=2.338   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.066   train_acc= 1.000   test_loss=2.331   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.052   train_acc= 1.000   test_loss=2.299   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.042   train_acc= 1.000   test_loss=2.297   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.028   train_acc= 1.000   test_loss=2.318   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.005   train_acc= 1.000   test_loss=2.303   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.006   train_acc= 1.000   test_loss=2.312   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.981   train_acc= 1.000   test_loss=2.271   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.977   train_acc= 1.000   test_loss=2.225   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.954   train_acc= 1.000   test_loss=2.218   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.938   train_acc= 1.000   test_loss=2.204   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.926   train_acc= 1.000   test_loss=2.217   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.917   train_acc= 1.000   test_loss=2.158   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.900   train_acc= 1.000   test_loss=2.148   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.890   train_acc= 1.000   test_loss=2.172   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.878   train_acc= 1.000   test_loss=2.133   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.866   train_acc= 1.000   test_loss=2.123   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.854   train_acc= 1.000   test_loss=2.121   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.839   train_acc= 1.000   test_loss=2.120   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.834   train_acc= 1.000   test_loss=2.111   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.820   train_acc= 1.000   test_loss=2.100   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.809   train_acc= 1.000   test_loss=2.090   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.796   train_acc= 1.000   test_loss=2.072   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.785   train_acc= 1.000   test_loss=2.060   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.773   train_acc= 1.000   test_loss=2.043   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.765   train_acc= 1.000   test_loss=2.035   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.766   train_acc= 1.000   test_loss=2.020   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 42   train_loss= 1.749   train_acc= 1.000   test_loss=2.023   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.734   train_acc= 1.000   test_loss=2.005   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.723   train_acc= 1.000   test_loss=2.006   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.713   train_acc= 1.000   test_loss=1.982   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.709   train_acc= 1.000   test_loss=1.981   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.696   train_acc= 1.000   test_loss=1.954   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.686   train_acc= 1.000   test_loss=1.946   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.675   train_acc= 1.000   test_loss=1.952   test_acc= 0.889\n",
      "run time: 0.7658559322357178 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 4\n",
      "epoch= 0   train_loss= 2.933   train_acc= 0.651   test_loss=2.745   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.653   train_acc= 0.867   test_loss=2.536   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.506   train_acc= 0.928   test_loss=2.675   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.424   train_acc= 0.952   test_loss=2.502   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.348   train_acc= 0.976   test_loss=2.460   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.295   train_acc= 1.000   test_loss=2.433   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.298   train_acc= 0.976   test_loss=2.390   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.259   train_acc= 0.988   test_loss=2.348   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.236   train_acc= 0.976   test_loss=2.331   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.204   train_acc= 1.000   test_loss=2.394   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.179   train_acc= 1.000   test_loss=2.310   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.150   train_acc= 1.000   test_loss=2.305   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.132   train_acc= 1.000   test_loss=2.285   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.108   train_acc= 1.000   test_loss=2.260   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.099   train_acc= 1.000   test_loss=2.279   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.100   train_acc= 0.988   test_loss=2.202   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.066   train_acc= 1.000   test_loss=2.220   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.050   train_acc= 1.000   test_loss=2.204   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.039   train_acc= 1.000   test_loss=2.153   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.022   train_acc= 1.000   test_loss=2.153   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.008   train_acc= 1.000   test_loss=2.156   test_acc= 1.000\n",
      "epoch= 21   train_loss= 1.999   train_acc= 1.000   test_loss=2.133   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.978   train_acc= 1.000   test_loss=2.137   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.969   train_acc= 1.000   test_loss=2.111   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.956   train_acc= 1.000   test_loss=2.107   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.941   train_acc= 1.000   test_loss=2.089   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.928   train_acc= 1.000   test_loss=2.066   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.915   train_acc= 1.000   test_loss=2.046   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.903   train_acc= 1.000   test_loss=2.050   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.892   train_acc= 1.000   test_loss=2.040   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.882   train_acc= 1.000   test_loss=2.029   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.868   train_acc= 1.000   test_loss=2.022   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.853   train_acc= 1.000   test_loss=2.002   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.843   train_acc= 1.000   test_loss=2.017   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.830   train_acc= 1.000   test_loss=2.018   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.817   train_acc= 1.000   test_loss=1.984   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.807   train_acc= 1.000   test_loss=1.970   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.799   train_acc= 1.000   test_loss=1.943   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.788   train_acc= 1.000   test_loss=1.927   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.779   train_acc= 1.000   test_loss=1.908   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.766   train_acc= 1.000   test_loss=1.915   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.755   train_acc= 1.000   test_loss=1.913   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.743   train_acc= 1.000   test_loss=1.883   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.734   train_acc= 1.000   test_loss=1.896   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.721   train_acc= 1.000   test_loss=1.886   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.714   train_acc= 1.000   test_loss=1.861   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.703   train_acc= 1.000   test_loss=1.867   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.692   train_acc= 1.000   test_loss=1.852   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.685   train_acc= 1.000   test_loss=1.850   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.672   train_acc= 1.000   test_loss=1.833   test_acc= 0.889\n",
      "run time: 0.7318607012430827 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 5\n",
      "epoch= 0   train_loss= 2.955   train_acc= 0.699   test_loss=2.750   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.630   train_acc= 0.867   test_loss=2.652   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.503   train_acc= 0.904   test_loss=2.610   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.380   train_acc= 0.976   test_loss=2.546   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.329   train_acc= 0.976   test_loss=2.610   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.311   train_acc= 0.976   test_loss=2.489   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.268   train_acc= 1.000   test_loss=2.463   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.226   train_acc= 1.000   test_loss=2.493   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.205   train_acc= 1.000   test_loss=2.459   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.207   train_acc= 0.976   test_loss=2.468   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.173   train_acc= 1.000   test_loss=2.449   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.139   train_acc= 1.000   test_loss=2.419   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.128   train_acc= 1.000   test_loss=2.453   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.120   train_acc= 0.988   test_loss=2.419   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.092   train_acc= 1.000   test_loss=2.395   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.079   train_acc= 1.000   test_loss=2.381   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.064   train_acc= 1.000   test_loss=2.351   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.045   train_acc= 1.000   test_loss=2.364   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.032   train_acc= 1.000   test_loss=2.401   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.017   train_acc= 1.000   test_loss=2.337   test_acc= 0.889\n",
      "epoch= 20   train_loss= 1.998   train_acc= 1.000   test_loss=2.316   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.987   train_acc= 1.000   test_loss=2.310   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.979   train_acc= 1.000   test_loss=2.296   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.963   train_acc= 1.000   test_loss=2.263   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.951   train_acc= 1.000   test_loss=2.247   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.938   train_acc= 1.000   test_loss=2.237   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.919   train_acc= 1.000   test_loss=2.253   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.907   train_acc= 1.000   test_loss=2.228   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.893   train_acc= 1.000   test_loss=2.232   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.882   train_acc= 1.000   test_loss=2.234   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.873   train_acc= 1.000   test_loss=2.177   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.859   train_acc= 1.000   test_loss=2.181   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.848   train_acc= 1.000   test_loss=2.144   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.840   train_acc= 1.000   test_loss=2.189   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.824   train_acc= 1.000   test_loss=2.160   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.812   train_acc= 1.000   test_loss=2.106   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.802   train_acc= 1.000   test_loss=2.113   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.792   train_acc= 1.000   test_loss=2.120   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 38   train_loss= 1.780   train_acc= 1.000   test_loss=2.100   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.771   train_acc= 1.000   test_loss=2.094   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.760   train_acc= 1.000   test_loss=2.094   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.753   train_acc= 1.000   test_loss=2.077   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.742   train_acc= 1.000   test_loss=2.076   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.729   train_acc= 1.000   test_loss=2.070   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.719   train_acc= 1.000   test_loss=2.024   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.713   train_acc= 1.000   test_loss=2.114   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.702   train_acc= 1.000   test_loss=2.060   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.688   train_acc= 1.000   test_loss=2.033   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.681   train_acc= 1.000   test_loss=2.003   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.673   train_acc= 1.000   test_loss=2.003   test_acc= 0.889\n",
      "run time: 0.7262582659721375 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 6\n",
      "epoch= 0   train_loss= 2.879   train_acc= 0.687   test_loss=2.864   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.640   train_acc= 0.867   test_loss=2.823   test_acc= 0.556\n",
      "epoch= 2   train_loss= 2.533   train_acc= 0.928   test_loss=2.729   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.444   train_acc= 0.940   test_loss=2.683   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.375   train_acc= 0.964   test_loss=2.730   test_acc= 0.556\n",
      "epoch= 5   train_loss= 2.348   train_acc= 0.964   test_loss=2.750   test_acc= 0.667\n",
      "epoch= 6   train_loss= 2.308   train_acc= 0.976   test_loss=2.658   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.280   train_acc= 0.988   test_loss=2.625   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.275   train_acc= 0.976   test_loss=2.559   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.233   train_acc= 0.976   test_loss=2.666   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.195   train_acc= 1.000   test_loss=2.602   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.194   train_acc= 0.988   test_loss=2.500   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.167   train_acc= 0.988   test_loss=2.487   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.150   train_acc= 0.976   test_loss=2.512   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.111   train_acc= 1.000   test_loss=2.468   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.107   train_acc= 1.000   test_loss=2.459   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.087   train_acc= 1.000   test_loss=2.457   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.070   train_acc= 1.000   test_loss=2.442   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.048   train_acc= 1.000   test_loss=2.414   test_acc= 0.667\n",
      "epoch= 19   train_loss= 2.036   train_acc= 1.000   test_loss=2.405   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.029   train_acc= 1.000   test_loss=2.362   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.010   train_acc= 1.000   test_loss=2.331   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.994   train_acc= 1.000   test_loss=2.358   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.981   train_acc= 1.000   test_loss=2.300   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.977   train_acc= 1.000   test_loss=2.294   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.954   train_acc= 1.000   test_loss=2.295   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.941   train_acc= 1.000   test_loss=2.271   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.931   train_acc= 1.000   test_loss=2.252   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.924   train_acc= 1.000   test_loss=2.228   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.905   train_acc= 1.000   test_loss=2.239   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.887   train_acc= 1.000   test_loss=2.232   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.878   train_acc= 1.000   test_loss=2.198   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.866   train_acc= 1.000   test_loss=2.195   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.850   train_acc= 1.000   test_loss=2.172   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.843   train_acc= 1.000   test_loss=2.157   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.834   train_acc= 1.000   test_loss=2.142   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.824   train_acc= 1.000   test_loss=2.132   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.809   train_acc= 1.000   test_loss=2.144   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.803   train_acc= 1.000   test_loss=2.174   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.790   train_acc= 1.000   test_loss=2.128   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.778   train_acc= 1.000   test_loss=2.078   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.765   train_acc= 1.000   test_loss=2.104   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.752   train_acc= 1.000   test_loss=2.071   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.743   train_acc= 1.000   test_loss=2.049   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.735   train_acc= 1.000   test_loss=2.039   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.724   train_acc= 1.000   test_loss=2.032   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.713   train_acc= 1.000   test_loss=2.020   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.704   train_acc= 1.000   test_loss=2.017   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.695   train_acc= 1.000   test_loss=2.012   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.688   train_acc= 1.000   test_loss=2.004   test_acc= 0.778\n",
      "run time: 0.750025216738383 min\n",
      "test_acc=0.778\n",
      "run= 0   fold= 7\n",
      "epoch= 0   train_loss= 2.963   train_acc= 0.627   test_loss=2.784   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.572   train_acc= 0.855   test_loss=2.698   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.502   train_acc= 0.880   test_loss=2.637   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.376   train_acc= 0.964   test_loss=2.589   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.340   train_acc= 0.976   test_loss=2.619   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.320   train_acc= 0.988   test_loss=2.564   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.269   train_acc= 0.976   test_loss=2.543   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.238   train_acc= 1.000   test_loss=2.512   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.216   train_acc= 1.000   test_loss=2.495   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.193   train_acc= 0.988   test_loss=2.510   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.163   train_acc= 1.000   test_loss=2.470   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.146   train_acc= 1.000   test_loss=2.445   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.126   train_acc= 1.000   test_loss=2.428   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.114   train_acc= 1.000   test_loss=2.419   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.099   train_acc= 1.000   test_loss=2.398   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.090   train_acc= 1.000   test_loss=2.417   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.067   train_acc= 1.000   test_loss=2.379   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.054   train_acc= 1.000   test_loss=2.374   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.029   train_acc= 1.000   test_loss=2.356   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.021   train_acc= 1.000   test_loss=2.327   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.006   train_acc= 1.000   test_loss=2.313   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.988   train_acc= 1.000   test_loss=2.298   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.979   train_acc= 1.000   test_loss=2.285   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.964   train_acc= 1.000   test_loss=2.265   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.948   train_acc= 1.000   test_loss=2.257   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.935   train_acc= 1.000   test_loss=2.247   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.931   train_acc= 1.000   test_loss=2.236   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.907   train_acc= 1.000   test_loss=2.209   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.896   train_acc= 1.000   test_loss=2.200   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.888   train_acc= 1.000   test_loss=2.186   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.874   train_acc= 1.000   test_loss=2.180   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.864   train_acc= 1.000   test_loss=2.171   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.851   train_acc= 1.000   test_loss=2.144   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.836   train_acc= 1.000   test_loss=2.131   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 34   train_loss= 1.826   train_acc= 1.000   test_loss=2.126   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.819   train_acc= 1.000   test_loss=2.122   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.805   train_acc= 1.000   test_loss=2.103   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.794   train_acc= 1.000   test_loss=2.092   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.782   train_acc= 1.000   test_loss=2.079   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.772   train_acc= 1.000   test_loss=2.068   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.763   train_acc= 1.000   test_loss=2.059   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.751   train_acc= 1.000   test_loss=2.053   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.743   train_acc= 1.000   test_loss=2.049   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.730   train_acc= 1.000   test_loss=2.036   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.717   train_acc= 1.000   test_loss=2.028   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.709   train_acc= 1.000   test_loss=2.017   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.702   train_acc= 1.000   test_loss=1.999   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.688   train_acc= 1.000   test_loss=1.990   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.680   train_acc= 1.000   test_loss=1.979   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.682   train_acc= 1.000   test_loss=1.976   test_acc= 0.889\n",
      "run time: 0.7191857020060222 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 8\n",
      "epoch= 0   train_loss= 2.901   train_acc= 0.663   test_loss=2.760   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.620   train_acc= 0.819   test_loss=2.557   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.498   train_acc= 0.940   test_loss=2.574   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.412   train_acc= 0.964   test_loss=2.490   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.350   train_acc= 0.964   test_loss=2.463   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.319   train_acc= 0.976   test_loss=2.461   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.310   train_acc= 0.988   test_loss=2.389   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.270   train_acc= 0.976   test_loss=2.318   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.256   train_acc= 0.976   test_loss=2.326   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.205   train_acc= 1.000   test_loss=2.317   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.190   train_acc= 1.000   test_loss=2.355   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.174   train_acc= 0.988   test_loss=2.309   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.137   train_acc= 1.000   test_loss=2.312   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.123   train_acc= 1.000   test_loss=2.230   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.103   train_acc= 1.000   test_loss=2.244   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.092   train_acc= 1.000   test_loss=2.216   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.067   train_acc= 1.000   test_loss=2.201   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.055   train_acc= 1.000   test_loss=2.190   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.053   train_acc= 1.000   test_loss=2.165   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.027   train_acc= 1.000   test_loss=2.168   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.011   train_acc= 1.000   test_loss=2.143   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.000   train_acc= 1.000   test_loss=2.156   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.986   train_acc= 1.000   test_loss=2.126   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.969   train_acc= 1.000   test_loss=2.128   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.956   train_acc= 1.000   test_loss=2.128   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.952   train_acc= 1.000   test_loss=2.096   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.934   train_acc= 1.000   test_loss=2.064   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.917   train_acc= 1.000   test_loss=2.081   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.909   train_acc= 1.000   test_loss=2.095   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.897   train_acc= 1.000   test_loss=2.040   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.881   train_acc= 1.000   test_loss=2.028   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.865   train_acc= 1.000   test_loss=2.018   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.862   train_acc= 1.000   test_loss=2.028   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.846   train_acc= 1.000   test_loss=2.025   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.840   train_acc= 1.000   test_loss=2.024   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.822   train_acc= 1.000   test_loss=2.000   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.811   train_acc= 1.000   test_loss=2.004   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.798   train_acc= 1.000   test_loss=1.973   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.787   train_acc= 1.000   test_loss=1.951   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.782   train_acc= 1.000   test_loss=1.979   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.766   train_acc= 1.000   test_loss=1.941   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.758   train_acc= 1.000   test_loss=1.922   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.744   train_acc= 1.000   test_loss=1.937   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.736   train_acc= 1.000   test_loss=1.917   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.724   train_acc= 1.000   test_loss=1.887   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.712   train_acc= 1.000   test_loss=1.881   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.704   train_acc= 1.000   test_loss=1.883   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.693   train_acc= 1.000   test_loss=1.867   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.683   train_acc= 1.000   test_loss=1.861   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.674   train_acc= 1.000   test_loss=1.861   test_acc= 0.889\n",
      "run time: 0.7323010285695394 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 9\n",
      "epoch= 0   train_loss= 2.938   train_acc= 0.675   test_loss=3.023   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.662   train_acc= 0.855   test_loss=2.907   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.494   train_acc= 0.952   test_loss=2.753   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.443   train_acc= 0.940   test_loss=2.864   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.399   train_acc= 0.940   test_loss=2.708   test_acc= 0.556\n",
      "epoch= 5   train_loss= 2.343   train_acc= 0.976   test_loss=2.757   test_acc= 0.667\n",
      "epoch= 6   train_loss= 2.281   train_acc= 1.000   test_loss=2.763   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.261   train_acc= 1.000   test_loss=2.694   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.253   train_acc= 0.988   test_loss=2.640   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.224   train_acc= 1.000   test_loss=2.555   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.198   train_acc= 0.988   test_loss=2.590   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.170   train_acc= 1.000   test_loss=2.691   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.148   train_acc= 1.000   test_loss=2.696   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.131   train_acc= 1.000   test_loss=2.656   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.106   train_acc= 1.000   test_loss=2.587   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.092   train_acc= 1.000   test_loss=2.566   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.079   train_acc= 1.000   test_loss=2.570   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.060   train_acc= 1.000   test_loss=2.570   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.051   train_acc= 1.000   test_loss=2.576   test_acc= 0.667\n",
      "epoch= 19   train_loss= 2.033   train_acc= 1.000   test_loss=2.568   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.017   train_acc= 1.000   test_loss=2.552   test_acc= 0.667\n",
      "epoch= 21   train_loss= 2.008   train_acc= 0.988   test_loss=2.593   test_acc= 0.667\n",
      "epoch= 22   train_loss= 1.992   train_acc= 1.000   test_loss=2.494   test_acc= 0.667\n",
      "epoch= 23   train_loss= 1.975   train_acc= 1.000   test_loss=2.399   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.961   train_acc= 1.000   test_loss=2.419   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.964   train_acc= 1.000   test_loss=2.456   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.936   train_acc= 1.000   test_loss=2.445   test_acc= 0.667\n",
      "epoch= 27   train_loss= 1.921   train_acc= 1.000   test_loss=2.416   test_acc= 0.667\n",
      "epoch= 28   train_loss= 1.906   train_acc= 1.000   test_loss=2.393   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.898   train_acc= 1.000   test_loss=2.394   test_acc= 0.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 30   train_loss= 1.888   train_acc= 1.000   test_loss=2.414   test_acc= 0.667\n",
      "epoch= 31   train_loss= 1.870   train_acc= 1.000   test_loss=2.369   test_acc= 0.667\n",
      "epoch= 32   train_loss= 1.858   train_acc= 1.000   test_loss=2.370   test_acc= 0.667\n",
      "epoch= 33   train_loss= 1.846   train_acc= 1.000   test_loss=2.318   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.835   train_acc= 1.000   test_loss=2.298   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.822   train_acc= 1.000   test_loss=2.314   test_acc= 0.667\n",
      "epoch= 36   train_loss= 1.812   train_acc= 1.000   test_loss=2.313   test_acc= 0.667\n",
      "epoch= 37   train_loss= 1.806   train_acc= 1.000   test_loss=2.352   test_acc= 0.667\n",
      "epoch= 38   train_loss= 1.797   train_acc= 1.000   test_loss=2.312   test_acc= 0.667\n",
      "epoch= 39   train_loss= 1.783   train_acc= 1.000   test_loss=2.318   test_acc= 0.667\n",
      "epoch= 40   train_loss= 1.773   train_acc= 1.000   test_loss=2.278   test_acc= 0.667\n",
      "epoch= 41   train_loss= 1.761   train_acc= 1.000   test_loss=2.276   test_acc= 0.667\n",
      "epoch= 42   train_loss= 1.749   train_acc= 1.000   test_loss=2.287   test_acc= 0.667\n",
      "epoch= 43   train_loss= 1.741   train_acc= 1.000   test_loss=2.240   test_acc= 0.667\n",
      "epoch= 44   train_loss= 1.727   train_acc= 1.000   test_loss=2.224   test_acc= 0.667\n",
      "epoch= 45   train_loss= 1.721   train_acc= 1.000   test_loss=2.246   test_acc= 0.667\n",
      "epoch= 46   train_loss= 1.710   train_acc= 1.000   test_loss=2.250   test_acc= 0.667\n",
      "epoch= 47   train_loss= 1.702   train_acc= 1.000   test_loss=2.211   test_acc= 0.667\n",
      "epoch= 48   train_loss= 1.688   train_acc= 1.000   test_loss=2.179   test_acc= 0.667\n",
      "epoch= 49   train_loss= 1.679   train_acc= 1.000   test_loss=2.155   test_acc= 0.778\n",
      "run time: 0.7477924545605977 min\n",
      "test_acc=0.778\n",
      "run= 1   fold= 0\n",
      "epoch= 0   train_loss= 2.982   train_acc= 0.598   test_loss=2.933   test_acc= 0.700\n",
      "epoch= 1   train_loss= 2.684   train_acc= 0.841   test_loss=2.697   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.543   train_acc= 0.915   test_loss=2.608   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.526   train_acc= 0.902   test_loss=2.604   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.421   train_acc= 0.951   test_loss=2.541   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.377   train_acc= 0.963   test_loss=2.457   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.349   train_acc= 0.963   test_loss=2.382   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.327   train_acc= 0.976   test_loss=2.363   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.257   train_acc= 1.000   test_loss=2.343   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.244   train_acc= 0.988   test_loss=2.299   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.227   train_acc= 0.988   test_loss=2.292   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.186   train_acc= 1.000   test_loss=2.272   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.183   train_acc= 1.000   test_loss=2.234   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.140   train_acc= 1.000   test_loss=2.217   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.141   train_acc= 1.000   test_loss=2.196   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.117   train_acc= 1.000   test_loss=2.174   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.111   train_acc= 1.000   test_loss=2.191   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.091   train_acc= 1.000   test_loss=2.134   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.067   train_acc= 1.000   test_loss=2.125   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.058   train_acc= 1.000   test_loss=2.114   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.042   train_acc= 1.000   test_loss=2.096   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.023   train_acc= 1.000   test_loss=2.085   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.009   train_acc= 1.000   test_loss=2.070   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.995   train_acc= 1.000   test_loss=2.053   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.980   train_acc= 1.000   test_loss=2.041   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.970   train_acc= 1.000   test_loss=2.022   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.955   train_acc= 1.000   test_loss=2.010   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.936   train_acc= 1.000   test_loss=2.000   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.934   train_acc= 1.000   test_loss=1.984   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.915   train_acc= 1.000   test_loss=1.983   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.907   train_acc= 1.000   test_loss=1.954   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.889   train_acc= 1.000   test_loss=1.949   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.883   train_acc= 1.000   test_loss=1.945   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.870   train_acc= 1.000   test_loss=1.931   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.852   train_acc= 1.000   test_loss=1.915   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.845   train_acc= 1.000   test_loss=1.902   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.834   train_acc= 1.000   test_loss=1.888   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.821   train_acc= 1.000   test_loss=1.875   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.810   train_acc= 1.000   test_loss=1.865   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.799   train_acc= 1.000   test_loss=1.855   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.789   train_acc= 1.000   test_loss=1.841   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.779   train_acc= 1.000   test_loss=1.836   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.774   train_acc= 1.000   test_loss=1.814   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.757   train_acc= 1.000   test_loss=1.808   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.747   train_acc= 1.000   test_loss=1.801   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.749   train_acc= 1.000   test_loss=1.796   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.728   train_acc= 1.000   test_loss=1.781   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.719   train_acc= 1.000   test_loss=1.771   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.707   train_acc= 1.000   test_loss=1.761   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.699   train_acc= 1.000   test_loss=1.754   test_acc= 1.000\n",
      "run time: 0.7382492661476135 min\n",
      "test_acc=1.000\n",
      "run= 1   fold= 1\n",
      "epoch= 0   train_loss= 3.010   train_acc= 0.549   test_loss=2.845   test_acc= 0.800\n",
      "epoch= 1   train_loss= 2.650   train_acc= 0.878   test_loss=2.764   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.466   train_acc= 0.939   test_loss=2.693   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.395   train_acc= 0.951   test_loss=2.649   test_acc= 0.800\n",
      "epoch= 4   train_loss= 2.346   train_acc= 0.988   test_loss=2.634   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.306   train_acc= 0.976   test_loss=2.615   test_acc= 0.700\n",
      "epoch= 6   train_loss= 2.263   train_acc= 0.988   test_loss=2.570   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.259   train_acc= 0.988   test_loss=2.573   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.229   train_acc= 0.988   test_loss=2.572   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.185   train_acc= 1.000   test_loss=2.527   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.176   train_acc= 1.000   test_loss=2.518   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.163   train_acc= 1.000   test_loss=2.486   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.127   train_acc= 1.000   test_loss=2.461   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.111   train_acc= 1.000   test_loss=2.449   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.097   train_acc= 1.000   test_loss=2.421   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.076   train_acc= 1.000   test_loss=2.405   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.062   train_acc= 1.000   test_loss=2.387   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.061   train_acc= 0.988   test_loss=2.391   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.041   train_acc= 1.000   test_loss=2.369   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.017   train_acc= 1.000   test_loss=2.358   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.007   train_acc= 1.000   test_loss=2.333   test_acc= 0.900\n",
      "epoch= 21   train_loss= 1.991   train_acc= 1.000   test_loss=2.321   test_acc= 0.900\n",
      "epoch= 22   train_loss= 1.983   train_acc= 1.000   test_loss=2.315   test_acc= 0.900\n",
      "epoch= 23   train_loss= 1.962   train_acc= 1.000   test_loss=2.304   test_acc= 0.900\n",
      "epoch= 24   train_loss= 1.947   train_acc= 1.000   test_loss=2.270   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.939   train_acc= 1.000   test_loss=2.261   test_acc= 0.900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 26   train_loss= 1.930   train_acc= 1.000   test_loss=2.255   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.910   train_acc= 1.000   test_loss=2.244   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.900   train_acc= 1.000   test_loss=2.227   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.887   train_acc= 1.000   test_loss=2.221   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.876   train_acc= 1.000   test_loss=2.214   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.861   train_acc= 1.000   test_loss=2.202   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.850   train_acc= 1.000   test_loss=2.187   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.840   train_acc= 1.000   test_loss=2.181   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.826   train_acc= 1.000   test_loss=2.158   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.815   train_acc= 1.000   test_loss=2.143   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.808   train_acc= 1.000   test_loss=2.139   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.794   train_acc= 1.000   test_loss=2.130   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.781   train_acc= 1.000   test_loss=2.122   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.774   train_acc= 1.000   test_loss=2.112   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.764   train_acc= 1.000   test_loss=2.092   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.755   train_acc= 1.000   test_loss=2.084   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.744   train_acc= 1.000   test_loss=2.071   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.736   train_acc= 1.000   test_loss=2.074   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.722   train_acc= 1.000   test_loss=2.056   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.713   train_acc= 1.000   test_loss=2.040   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.702   train_acc= 1.000   test_loss=2.037   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.694   train_acc= 1.000   test_loss=2.016   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.682   train_acc= 1.000   test_loss=2.016   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.674   train_acc= 1.000   test_loss=1.999   test_acc= 0.900\n",
      "run time: 0.7273608485857646 min\n",
      "test_acc=0.900\n",
      "run= 1   fold= 2\n",
      "epoch= 0   train_loss= 2.925   train_acc= 0.614   test_loss=2.928   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.581   train_acc= 0.867   test_loss=2.875   test_acc= 0.556\n",
      "epoch= 2   train_loss= 2.492   train_acc= 0.916   test_loss=2.859   test_acc= 0.556\n",
      "epoch= 3   train_loss= 2.398   train_acc= 0.952   test_loss=2.774   test_acc= 0.556\n",
      "epoch= 4   train_loss= 2.365   train_acc= 0.976   test_loss=2.777   test_acc= 0.556\n",
      "epoch= 5   train_loss= 2.308   train_acc= 0.988   test_loss=2.728   test_acc= 0.556\n",
      "epoch= 6   train_loss= 2.278   train_acc= 0.988   test_loss=2.704   test_acc= 0.556\n",
      "epoch= 7   train_loss= 2.244   train_acc= 0.988   test_loss=2.664   test_acc= 0.556\n",
      "epoch= 8   train_loss= 2.223   train_acc= 0.988   test_loss=2.689   test_acc= 0.556\n",
      "epoch= 9   train_loss= 2.195   train_acc= 1.000   test_loss=2.656   test_acc= 0.556\n",
      "epoch= 10   train_loss= 2.171   train_acc= 0.988   test_loss=2.642   test_acc= 0.556\n",
      "epoch= 11   train_loss= 2.163   train_acc= 1.000   test_loss=2.651   test_acc= 0.556\n",
      "epoch= 12   train_loss= 2.133   train_acc= 1.000   test_loss=2.601   test_acc= 0.556\n",
      "epoch= 13   train_loss= 2.121   train_acc= 0.988   test_loss=2.582   test_acc= 0.556\n",
      "epoch= 14   train_loss= 2.100   train_acc= 1.000   test_loss=2.584   test_acc= 0.556\n",
      "epoch= 15   train_loss= 2.085   train_acc= 1.000   test_loss=2.599   test_acc= 0.556\n",
      "epoch= 16   train_loss= 2.064   train_acc= 1.000   test_loss=2.578   test_acc= 0.556\n",
      "epoch= 17   train_loss= 2.049   train_acc= 1.000   test_loss=2.562   test_acc= 0.556\n",
      "epoch= 18   train_loss= 2.035   train_acc= 1.000   test_loss=2.533   test_acc= 0.556\n",
      "epoch= 19   train_loss= 2.019   train_acc= 1.000   test_loss=2.531   test_acc= 0.556\n",
      "epoch= 20   train_loss= 2.005   train_acc= 1.000   test_loss=2.512   test_acc= 0.556\n",
      "epoch= 21   train_loss= 1.993   train_acc= 1.000   test_loss=2.492   test_acc= 0.556\n",
      "epoch= 22   train_loss= 1.980   train_acc= 1.000   test_loss=2.515   test_acc= 0.556\n",
      "epoch= 23   train_loss= 1.962   train_acc= 1.000   test_loss=2.486   test_acc= 0.556\n",
      "epoch= 24   train_loss= 1.952   train_acc= 1.000   test_loss=2.468   test_acc= 0.556\n",
      "epoch= 25   train_loss= 1.940   train_acc= 1.000   test_loss=2.451   test_acc= 0.556\n",
      "epoch= 26   train_loss= 1.931   train_acc= 1.000   test_loss=2.457   test_acc= 0.556\n",
      "epoch= 27   train_loss= 1.909   train_acc= 1.000   test_loss=2.432   test_acc= 0.556\n",
      "epoch= 28   train_loss= 1.900   train_acc= 1.000   test_loss=2.419   test_acc= 0.556\n",
      "epoch= 29   train_loss= 1.888   train_acc= 1.000   test_loss=2.388   test_acc= 0.556\n",
      "epoch= 30   train_loss= 1.874   train_acc= 1.000   test_loss=2.376   test_acc= 0.556\n",
      "epoch= 31   train_loss= 1.862   train_acc= 1.000   test_loss=2.375   test_acc= 0.556\n",
      "epoch= 32   train_loss= 1.852   train_acc= 1.000   test_loss=2.348   test_acc= 0.556\n",
      "epoch= 33   train_loss= 1.843   train_acc= 1.000   test_loss=2.318   test_acc= 0.556\n",
      "epoch= 34   train_loss= 1.828   train_acc= 1.000   test_loss=2.317   test_acc= 0.556\n",
      "epoch= 35   train_loss= 1.819   train_acc= 1.000   test_loss=2.307   test_acc= 0.556\n",
      "epoch= 36   train_loss= 1.808   train_acc= 1.000   test_loss=2.305   test_acc= 0.556\n",
      "epoch= 37   train_loss= 1.798   train_acc= 1.000   test_loss=2.273   test_acc= 0.556\n",
      "epoch= 38   train_loss= 1.781   train_acc= 1.000   test_loss=2.257   test_acc= 0.556\n",
      "epoch= 39   train_loss= 1.773   train_acc= 1.000   test_loss=2.250   test_acc= 0.556\n",
      "epoch= 40   train_loss= 1.761   train_acc= 1.000   test_loss=2.242   test_acc= 0.556\n",
      "epoch= 41   train_loss= 1.751   train_acc= 1.000   test_loss=2.226   test_acc= 0.556\n",
      "epoch= 42   train_loss= 1.745   train_acc= 1.000   test_loss=2.221   test_acc= 0.556\n",
      "epoch= 43   train_loss= 1.729   train_acc= 1.000   test_loss=2.216   test_acc= 0.556\n",
      "epoch= 44   train_loss= 1.724   train_acc= 1.000   test_loss=2.183   test_acc= 0.556\n",
      "epoch= 45   train_loss= 1.710   train_acc= 1.000   test_loss=2.175   test_acc= 0.556\n",
      "epoch= 46   train_loss= 1.703   train_acc= 1.000   test_loss=2.173   test_acc= 0.556\n",
      "epoch= 47   train_loss= 1.690   train_acc= 1.000   test_loss=2.165   test_acc= 0.556\n",
      "epoch= 48   train_loss= 1.681   train_acc= 1.000   test_loss=2.164   test_acc= 0.556\n",
      "epoch= 49   train_loss= 1.673   train_acc= 1.000   test_loss=2.176   test_acc= 0.556\n",
      "run time: 0.7532631158828735 min\n",
      "test_acc=0.556\n",
      "run= 1   fold= 3\n",
      "epoch= 0   train_loss= 2.913   train_acc= 0.614   test_loss=2.869   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.604   train_acc= 0.855   test_loss=2.783   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.495   train_acc= 0.904   test_loss=2.740   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.416   train_acc= 0.940   test_loss=2.723   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.344   train_acc= 0.976   test_loss=2.743   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.336   train_acc= 0.976   test_loss=2.628   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.306   train_acc= 0.964   test_loss=2.621   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.251   train_acc= 0.988   test_loss=2.601   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.227   train_acc= 0.988   test_loss=2.564   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.205   train_acc= 0.988   test_loss=2.539   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.187   train_acc= 1.000   test_loss=2.517   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.151   train_acc= 1.000   test_loss=2.522   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.130   train_acc= 1.000   test_loss=2.482   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.122   train_acc= 1.000   test_loss=2.464   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.125   train_acc= 0.964   test_loss=2.442   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.104   train_acc= 0.988   test_loss=2.453   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.083   train_acc= 1.000   test_loss=2.393   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.059   train_acc= 1.000   test_loss=2.402   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.040   train_acc= 1.000   test_loss=2.371   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.026   train_acc= 1.000   test_loss=2.353   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.007   train_acc= 1.000   test_loss=2.335   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.997   train_acc= 1.000   test_loss=2.316   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 22   train_loss= 1.983   train_acc= 1.000   test_loss=2.300   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.966   train_acc= 1.000   test_loss=2.292   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.956   train_acc= 1.000   test_loss=2.254   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.945   train_acc= 1.000   test_loss=2.252   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.933   train_acc= 1.000   test_loss=2.240   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.923   train_acc= 1.000   test_loss=2.238   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.908   train_acc= 1.000   test_loss=2.229   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.894   train_acc= 1.000   test_loss=2.227   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.880   train_acc= 1.000   test_loss=2.196   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.869   train_acc= 1.000   test_loss=2.187   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.850   train_acc= 1.000   test_loss=2.169   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.846   train_acc= 1.000   test_loss=2.156   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.832   train_acc= 1.000   test_loss=2.151   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.818   train_acc= 1.000   test_loss=2.132   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.808   train_acc= 1.000   test_loss=2.132   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.798   train_acc= 1.000   test_loss=2.128   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.786   train_acc= 1.000   test_loss=2.110   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.777   train_acc= 1.000   test_loss=2.096   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.764   train_acc= 1.000   test_loss=2.086   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.760   train_acc= 1.000   test_loss=2.074   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.743   train_acc= 1.000   test_loss=2.049   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.739   train_acc= 1.000   test_loss=2.043   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.723   train_acc= 1.000   test_loss=2.027   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.713   train_acc= 1.000   test_loss=2.024   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.706   train_acc= 1.000   test_loss=2.010   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.693   train_acc= 1.000   test_loss=2.002   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.686   train_acc= 1.000   test_loss=1.983   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.678   train_acc= 1.000   test_loss=1.959   test_acc= 0.889\n",
      "run time: 0.7503283023834229 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 4\n",
      "epoch= 0   train_loss= 2.985   train_acc= 0.602   test_loss=2.818   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.645   train_acc= 0.843   test_loss=2.824   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.532   train_acc= 0.916   test_loss=2.733   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.460   train_acc= 0.964   test_loss=2.692   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.386   train_acc= 0.976   test_loss=2.629   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.316   train_acc= 0.964   test_loss=2.567   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.304   train_acc= 0.988   test_loss=2.545   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.272   train_acc= 0.976   test_loss=2.523   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.250   train_acc= 0.988   test_loss=2.524   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.214   train_acc= 0.988   test_loss=2.496   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.202   train_acc= 0.988   test_loss=2.464   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.171   train_acc= 1.000   test_loss=2.421   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.160   train_acc= 1.000   test_loss=2.396   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.124   train_acc= 1.000   test_loss=2.361   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.100   train_acc= 1.000   test_loss=2.349   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.096   train_acc= 1.000   test_loss=2.324   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.069   train_acc= 1.000   test_loss=2.313   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.061   train_acc= 1.000   test_loss=2.289   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.051   train_acc= 1.000   test_loss=2.280   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.024   train_acc= 1.000   test_loss=2.254   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.013   train_acc= 1.000   test_loss=2.251   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.998   train_acc= 1.000   test_loss=2.228   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.987   train_acc= 1.000   test_loss=2.218   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.969   train_acc= 1.000   test_loss=2.197   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.961   train_acc= 1.000   test_loss=2.194   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.952   train_acc= 1.000   test_loss=2.166   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.946   train_acc= 0.988   test_loss=2.164   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.922   train_acc= 1.000   test_loss=2.138   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.911   train_acc= 1.000   test_loss=2.132   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.901   train_acc= 1.000   test_loss=2.107   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.882   train_acc= 1.000   test_loss=2.097   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.873   train_acc= 1.000   test_loss=2.098   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.856   train_acc= 1.000   test_loss=2.084   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.848   train_acc= 1.000   test_loss=2.069   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.837   train_acc= 1.000   test_loss=2.056   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.824   train_acc= 1.000   test_loss=2.046   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.816   train_acc= 1.000   test_loss=2.032   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.803   train_acc= 1.000   test_loss=2.027   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.792   train_acc= 1.000   test_loss=2.007   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.786   train_acc= 1.000   test_loss=1.998   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.769   train_acc= 1.000   test_loss=1.989   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.757   train_acc= 1.000   test_loss=1.974   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.749   train_acc= 1.000   test_loss=1.970   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.736   train_acc= 1.000   test_loss=1.955   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.731   train_acc= 1.000   test_loss=1.944   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.721   train_acc= 1.000   test_loss=1.933   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.708   train_acc= 1.000   test_loss=1.924   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.695   train_acc= 1.000   test_loss=1.911   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.686   train_acc= 1.000   test_loss=1.909   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.677   train_acc= 1.000   test_loss=1.894   test_acc= 1.000\n",
      "run time: 0.7368460536003113 min\n",
      "test_acc=1.000\n",
      "run= 1   fold= 5\n",
      "epoch= 0   train_loss= 2.891   train_acc= 0.699   test_loss=2.774   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.650   train_acc= 0.843   test_loss=2.758   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.508   train_acc= 0.892   test_loss=2.572   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.451   train_acc= 0.916   test_loss=2.552   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.355   train_acc= 0.964   test_loss=2.498   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.351   train_acc= 0.976   test_loss=2.540   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.293   train_acc= 0.964   test_loss=2.463   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.278   train_acc= 0.964   test_loss=2.425   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.226   train_acc= 1.000   test_loss=2.425   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.200   train_acc= 1.000   test_loss=2.419   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.195   train_acc= 0.988   test_loss=2.376   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.163   train_acc= 0.988   test_loss=2.358   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.133   train_acc= 1.000   test_loss=2.348   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.131   train_acc= 1.000   test_loss=2.303   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.099   train_acc= 1.000   test_loss=2.288   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.089   train_acc= 1.000   test_loss=2.294   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.067   train_acc= 1.000   test_loss=2.288   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.057   train_acc= 1.000   test_loss=2.274   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 18   train_loss= 2.041   train_acc= 1.000   test_loss=2.245   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.029   train_acc= 1.000   test_loss=2.217   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.011   train_acc= 1.000   test_loss=2.202   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.997   train_acc= 1.000   test_loss=2.209   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.985   train_acc= 1.000   test_loss=2.191   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.968   train_acc= 1.000   test_loss=2.161   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.954   train_acc= 1.000   test_loss=2.142   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.940   train_acc= 1.000   test_loss=2.122   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.930   train_acc= 1.000   test_loss=2.114   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.916   train_acc= 1.000   test_loss=2.097   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.906   train_acc= 1.000   test_loss=2.104   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.889   train_acc= 1.000   test_loss=2.079   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.877   train_acc= 1.000   test_loss=2.069   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.864   train_acc= 1.000   test_loss=2.053   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.853   train_acc= 1.000   test_loss=2.054   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.846   train_acc= 1.000   test_loss=2.062   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.833   train_acc= 1.000   test_loss=2.043   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.820   train_acc= 1.000   test_loss=2.028   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.808   train_acc= 1.000   test_loss=2.009   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.802   train_acc= 1.000   test_loss=2.002   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.786   train_acc= 1.000   test_loss=1.983   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.779   train_acc= 1.000   test_loss=1.971   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.766   train_acc= 1.000   test_loss=1.963   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.754   train_acc= 1.000   test_loss=1.960   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.744   train_acc= 1.000   test_loss=1.944   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.734   train_acc= 1.000   test_loss=1.940   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.725   train_acc= 1.000   test_loss=1.926   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.714   train_acc= 1.000   test_loss=1.912   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.704   train_acc= 1.000   test_loss=1.909   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.695   train_acc= 1.000   test_loss=1.900   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.684   train_acc= 1.000   test_loss=1.894   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.677   train_acc= 1.000   test_loss=1.877   test_acc= 0.889\n",
      "run time: 1.001091436545054 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 6\n",
      "epoch= 0   train_loss= 2.922   train_acc= 0.687   test_loss=2.734   test_acc= 1.000\n",
      "epoch= 1   train_loss= 2.585   train_acc= 0.916   test_loss=2.714   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.471   train_acc= 0.952   test_loss=2.591   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.448   train_acc= 0.976   test_loss=2.558   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.362   train_acc= 0.964   test_loss=2.502   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.320   train_acc= 0.976   test_loss=2.489   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.314   train_acc= 0.976   test_loss=2.479   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.253   train_acc= 0.976   test_loss=2.397   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.238   train_acc= 1.000   test_loss=2.377   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.221   train_acc= 0.988   test_loss=2.388   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.197   train_acc= 0.988   test_loss=2.315   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.179   train_acc= 1.000   test_loss=2.340   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.152   train_acc= 1.000   test_loss=2.323   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.128   train_acc= 1.000   test_loss=2.257   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.114   train_acc= 1.000   test_loss=2.243   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.095   train_acc= 1.000   test_loss=2.244   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.081   train_acc= 1.000   test_loss=2.269   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.066   train_acc= 1.000   test_loss=2.233   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.050   train_acc= 1.000   test_loss=2.179   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.038   train_acc= 1.000   test_loss=2.181   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.013   train_acc= 1.000   test_loss=2.176   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.009   train_acc= 1.000   test_loss=2.165   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.991   train_acc= 1.000   test_loss=2.158   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.984   train_acc= 1.000   test_loss=2.106   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.975   train_acc= 0.988   test_loss=2.144   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.951   train_acc= 1.000   test_loss=2.087   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.936   train_acc= 1.000   test_loss=2.072   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.926   train_acc= 1.000   test_loss=2.060   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.914   train_acc= 1.000   test_loss=2.045   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.899   train_acc= 1.000   test_loss=2.037   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.888   train_acc= 1.000   test_loss=2.054   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.874   train_acc= 1.000   test_loss=2.028   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.865   train_acc= 1.000   test_loss=2.007   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.854   train_acc= 1.000   test_loss=1.964   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.845   train_acc= 1.000   test_loss=1.957   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.837   train_acc= 1.000   test_loss=1.970   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.817   train_acc= 1.000   test_loss=1.957   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.808   train_acc= 1.000   test_loss=1.945   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.800   train_acc= 1.000   test_loss=1.952   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.806   train_acc= 0.988   test_loss=1.968   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.774   train_acc= 1.000   test_loss=1.916   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.763   train_acc= 1.000   test_loss=1.908   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.753   train_acc= 1.000   test_loss=1.893   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.743   train_acc= 1.000   test_loss=1.873   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.730   train_acc= 1.000   test_loss=1.862   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.724   train_acc= 1.000   test_loss=1.843   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.711   train_acc= 1.000   test_loss=1.831   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.702   train_acc= 1.000   test_loss=1.827   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.691   train_acc= 1.000   test_loss=1.820   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.685   train_acc= 1.000   test_loss=1.824   test_acc= 0.889\n",
      "run time: 0.7494821349779764 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 7\n",
      "epoch= 0   train_loss= 2.985   train_acc= 0.651   test_loss=2.857   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.655   train_acc= 0.855   test_loss=2.695   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.544   train_acc= 0.892   test_loss=2.594   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.434   train_acc= 0.940   test_loss=2.582   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.378   train_acc= 0.952   test_loss=2.601   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.322   train_acc= 0.988   test_loss=2.554   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.302   train_acc= 0.964   test_loss=2.538   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.249   train_acc= 1.000   test_loss=2.511   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.232   train_acc= 0.988   test_loss=2.513   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.208   train_acc= 1.000   test_loss=2.486   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.175   train_acc= 1.000   test_loss=2.483   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.160   train_acc= 1.000   test_loss=2.485   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.135   train_acc= 1.000   test_loss=2.454   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.130   train_acc= 1.000   test_loss=2.454   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 14   train_loss= 2.100   train_acc= 1.000   test_loss=2.448   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.088   train_acc= 1.000   test_loss=2.453   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.076   train_acc= 1.000   test_loss=2.428   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.059   train_acc= 1.000   test_loss=2.427   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.046   train_acc= 1.000   test_loss=2.438   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.030   train_acc= 1.000   test_loss=2.414   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.020   train_acc= 1.000   test_loss=2.400   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.003   train_acc= 1.000   test_loss=2.387   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.998   train_acc= 1.000   test_loss=2.392   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.973   train_acc= 1.000   test_loss=2.360   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.959   train_acc= 1.000   test_loss=2.358   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.944   train_acc= 1.000   test_loss=2.351   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.936   train_acc= 1.000   test_loss=2.320   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.919   train_acc= 1.000   test_loss=2.308   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.906   train_acc= 1.000   test_loss=2.303   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.899   train_acc= 1.000   test_loss=2.269   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.885   train_acc= 1.000   test_loss=2.265   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.870   train_acc= 1.000   test_loss=2.267   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.861   train_acc= 1.000   test_loss=2.257   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.854   train_acc= 1.000   test_loss=2.262   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.835   train_acc= 1.000   test_loss=2.255   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.825   train_acc= 1.000   test_loss=2.235   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.810   train_acc= 1.000   test_loss=2.224   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.803   train_acc= 1.000   test_loss=2.214   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.790   train_acc= 1.000   test_loss=2.201   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.780   train_acc= 1.000   test_loss=2.188   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.774   train_acc= 1.000   test_loss=2.190   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.758   train_acc= 1.000   test_loss=2.180   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.747   train_acc= 1.000   test_loss=2.172   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.738   train_acc= 1.000   test_loss=2.167   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.728   train_acc= 1.000   test_loss=2.160   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.719   train_acc= 1.000   test_loss=2.134   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.707   train_acc= 1.000   test_loss=2.121   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.701   train_acc= 1.000   test_loss=2.120   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.689   train_acc= 1.000   test_loss=2.114   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.678   train_acc= 1.000   test_loss=2.095   test_acc= 0.889\n",
      "run time: 0.8016034523646037 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 8\n",
      "epoch= 0   train_loss= 2.986   train_acc= 0.639   test_loss=2.656   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.686   train_acc= 0.819   test_loss=2.553   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.490   train_acc= 0.952   test_loss=2.462   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.426   train_acc= 0.952   test_loss=2.390   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.394   train_acc= 0.952   test_loss=2.401   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.353   train_acc= 0.964   test_loss=2.389   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.308   train_acc= 0.988   test_loss=2.370   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.251   train_acc= 1.000   test_loss=2.341   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.240   train_acc= 0.988   test_loss=2.368   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.197   train_acc= 1.000   test_loss=2.373   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.190   train_acc= 0.988   test_loss=2.301   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.166   train_acc= 1.000   test_loss=2.304   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.147   train_acc= 1.000   test_loss=2.266   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.136   train_acc= 1.000   test_loss=2.280   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.123   train_acc= 0.988   test_loss=2.255   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.104   train_acc= 1.000   test_loss=2.231   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.079   train_acc= 1.000   test_loss=2.212   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.062   train_acc= 1.000   test_loss=2.223   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.056   train_acc= 0.988   test_loss=2.170   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.038   train_acc= 1.000   test_loss=2.164   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.030   train_acc= 1.000   test_loss=2.188   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.007   train_acc= 1.000   test_loss=2.171   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.000   train_acc= 1.000   test_loss=2.153   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.978   train_acc= 1.000   test_loss=2.132   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.964   train_acc= 1.000   test_loss=2.109   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.947   train_acc= 1.000   test_loss=2.105   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.939   train_acc= 1.000   test_loss=2.079   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.926   train_acc= 1.000   test_loss=2.073   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.909   train_acc= 1.000   test_loss=2.069   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.900   train_acc= 1.000   test_loss=2.053   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.900   train_acc= 0.988   test_loss=2.019   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.874   train_acc= 1.000   test_loss=2.050   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.867   train_acc= 1.000   test_loss=2.012   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.863   train_acc= 1.000   test_loss=2.042   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.839   train_acc= 1.000   test_loss=1.995   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.827   train_acc= 1.000   test_loss=1.989   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.818   train_acc= 1.000   test_loss=1.971   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.807   train_acc= 1.000   test_loss=1.949   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.796   train_acc= 1.000   test_loss=1.943   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.786   train_acc= 1.000   test_loss=1.940   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.788   train_acc= 0.988   test_loss=1.917   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.763   train_acc= 1.000   test_loss=1.916   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.753   train_acc= 1.000   test_loss=1.911   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.742   train_acc= 1.000   test_loss=1.905   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.732   train_acc= 1.000   test_loss=1.898   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.720   train_acc= 1.000   test_loss=1.888   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.712   train_acc= 1.000   test_loss=1.893   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.703   train_acc= 1.000   test_loss=1.898   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.694   train_acc= 1.000   test_loss=1.875   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.687   train_acc= 1.000   test_loss=1.860   test_acc= 0.889\n",
      "run time: 0.7720719377199808 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 9\n",
      "epoch= 0   train_loss= 2.936   train_acc= 0.566   test_loss=2.782   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.600   train_acc= 0.904   test_loss=2.631   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.506   train_acc= 0.940   test_loss=2.578   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.451   train_acc= 0.928   test_loss=2.528   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.371   train_acc= 0.952   test_loss=2.460   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.339   train_acc= 0.964   test_loss=2.427   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.278   train_acc= 0.988   test_loss=2.410   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.261   train_acc= 0.988   test_loss=2.409   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.249   train_acc= 0.976   test_loss=2.360   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.233   train_acc= 0.976   test_loss=2.362   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 10   train_loss= 2.187   train_acc= 1.000   test_loss=2.286   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.178   train_acc= 1.000   test_loss=2.317   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.146   train_acc= 1.000   test_loss=2.287   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.150   train_acc= 0.988   test_loss=2.211   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.122   train_acc= 1.000   test_loss=2.240   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.100   train_acc= 1.000   test_loss=2.222   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.079   train_acc= 1.000   test_loss=2.212   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.064   train_acc= 1.000   test_loss=2.169   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.048   train_acc= 1.000   test_loss=2.150   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.047   train_acc= 0.988   test_loss=2.143   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.019   train_acc= 1.000   test_loss=2.148   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.009   train_acc= 1.000   test_loss=2.113   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.984   train_acc= 1.000   test_loss=2.111   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.976   train_acc= 1.000   test_loss=2.093   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.964   train_acc= 1.000   test_loss=2.080   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.953   train_acc= 1.000   test_loss=2.057   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.938   train_acc= 1.000   test_loss=2.053   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.924   train_acc= 1.000   test_loss=2.027   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.911   train_acc= 1.000   test_loss=2.035   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.898   train_acc= 1.000   test_loss=2.011   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.890   train_acc= 1.000   test_loss=1.993   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.871   train_acc= 1.000   test_loss=1.978   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.862   train_acc= 1.000   test_loss=1.964   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.851   train_acc= 1.000   test_loss=1.976   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.843   train_acc= 1.000   test_loss=1.950   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.827   train_acc= 1.000   test_loss=1.941   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.816   train_acc= 1.000   test_loss=1.960   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.804   train_acc= 1.000   test_loss=1.937   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.797   train_acc= 1.000   test_loss=1.904   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.786   train_acc= 1.000   test_loss=1.914   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.773   train_acc= 1.000   test_loss=1.906   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.760   train_acc= 1.000   test_loss=1.888   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.753   train_acc= 1.000   test_loss=1.868   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.738   train_acc= 1.000   test_loss=1.848   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.732   train_acc= 1.000   test_loss=1.853   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.721   train_acc= 1.000   test_loss=1.828   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.713   train_acc= 1.000   test_loss=1.811   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.702   train_acc= 1.000   test_loss=1.807   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.691   train_acc= 1.000   test_loss=1.792   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.680   train_acc= 1.000   test_loss=1.785   test_acc= 1.000\n",
      "run time: 0.7461323658625285 min\n",
      "test_acc=1.000\n",
      "run= 2   fold= 0\n",
      "epoch= 0   train_loss= 3.006   train_acc= 0.610   test_loss=2.858   test_acc= 0.700\n",
      "epoch= 1   train_loss= 2.625   train_acc= 0.817   test_loss=2.865   test_acc= 0.500\n",
      "epoch= 2   train_loss= 2.533   train_acc= 0.902   test_loss=2.712   test_acc= 0.800\n",
      "epoch= 3   train_loss= 2.445   train_acc= 0.976   test_loss=2.631   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.356   train_acc= 0.963   test_loss=2.609   test_acc= 0.800\n",
      "epoch= 5   train_loss= 2.355   train_acc= 0.963   test_loss=2.543   test_acc= 0.800\n",
      "epoch= 6   train_loss= 2.287   train_acc= 0.988   test_loss=2.511   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.261   train_acc= 0.976   test_loss=2.501   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.232   train_acc= 1.000   test_loss=2.469   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.202   train_acc= 1.000   test_loss=2.463   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.192   train_acc= 0.976   test_loss=2.412   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.170   train_acc= 1.000   test_loss=2.423   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.147   train_acc= 1.000   test_loss=2.380   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.125   train_acc= 1.000   test_loss=2.350   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.104   train_acc= 1.000   test_loss=2.331   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.089   train_acc= 1.000   test_loss=2.304   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.071   train_acc= 1.000   test_loss=2.287   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.056   train_acc= 1.000   test_loss=2.271   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.037   train_acc= 1.000   test_loss=2.250   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.027   train_acc= 1.000   test_loss=2.231   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.007   train_acc= 1.000   test_loss=2.214   test_acc= 0.900\n",
      "epoch= 21   train_loss= 1.992   train_acc= 1.000   test_loss=2.201   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.986   train_acc= 1.000   test_loss=2.179   test_acc= 0.900\n",
      "epoch= 23   train_loss= 1.996   train_acc= 0.988   test_loss=2.158   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.972   train_acc= 0.988   test_loss=2.150   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.947   train_acc= 1.000   test_loss=2.130   test_acc= 0.900\n",
      "epoch= 26   train_loss= 1.930   train_acc= 1.000   test_loss=2.113   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.916   train_acc= 1.000   test_loss=2.099   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.903   train_acc= 1.000   test_loss=2.079   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.890   train_acc= 1.000   test_loss=2.069   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.879   train_acc= 1.000   test_loss=2.058   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.866   train_acc= 1.000   test_loss=2.044   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.855   train_acc= 1.000   test_loss=2.031   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.844   train_acc= 1.000   test_loss=2.016   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.837   train_acc= 1.000   test_loss=2.002   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.821   train_acc= 1.000   test_loss=1.993   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.819   train_acc= 1.000   test_loss=1.981   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.799   train_acc= 1.000   test_loss=1.966   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.788   train_acc= 1.000   test_loss=1.951   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.778   train_acc= 1.000   test_loss=1.947   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.770   train_acc= 1.000   test_loss=1.934   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.760   train_acc= 1.000   test_loss=1.921   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.751   train_acc= 1.000   test_loss=1.910   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.737   train_acc= 1.000   test_loss=1.906   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.726   train_acc= 1.000   test_loss=1.891   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.716   train_acc= 1.000   test_loss=1.881   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.709   train_acc= 1.000   test_loss=1.877   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.704   train_acc= 1.000   test_loss=1.859   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.688   train_acc= 1.000   test_loss=1.846   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.680   train_acc= 1.000   test_loss=1.838   test_acc= 1.000\n",
      "run time: 0.7434316158294678 min\n",
      "test_acc=1.000\n",
      "run= 2   fold= 1\n",
      "epoch= 0   train_loss= 2.960   train_acc= 0.646   test_loss=2.813   test_acc= 0.700\n",
      "epoch= 1   train_loss= 2.654   train_acc= 0.829   test_loss=2.675   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.533   train_acc= 0.915   test_loss=2.598   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.442   train_acc= 0.927   test_loss=2.553   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.355   train_acc= 0.988   test_loss=2.493   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.335   train_acc= 0.963   test_loss=2.453   test_acc= 0.900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 6   train_loss= 2.292   train_acc= 0.988   test_loss=2.461   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.272   train_acc= 0.976   test_loss=2.400   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.236   train_acc= 0.976   test_loss=2.384   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.200   train_acc= 1.000   test_loss=2.374   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.182   train_acc= 1.000   test_loss=2.349   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.162   train_acc= 1.000   test_loss=2.288   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.142   train_acc= 0.988   test_loss=2.324   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.126   train_acc= 1.000   test_loss=2.253   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.123   train_acc= 1.000   test_loss=2.251   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.096   train_acc= 0.988   test_loss=2.218   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.088   train_acc= 0.988   test_loss=2.196   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.062   train_acc= 1.000   test_loss=2.185   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.051   train_acc= 1.000   test_loss=2.183   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.039   train_acc= 1.000   test_loss=2.200   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.012   train_acc= 1.000   test_loss=2.170   test_acc= 0.900\n",
      "epoch= 21   train_loss= 1.999   train_acc= 1.000   test_loss=2.145   test_acc= 0.900\n",
      "epoch= 22   train_loss= 1.986   train_acc= 1.000   test_loss=2.155   test_acc= 0.900\n",
      "epoch= 23   train_loss= 1.978   train_acc= 1.000   test_loss=2.131   test_acc= 0.900\n",
      "epoch= 24   train_loss= 1.960   train_acc= 1.000   test_loss=2.118   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.943   train_acc= 1.000   test_loss=2.100   test_acc= 0.900\n",
      "epoch= 26   train_loss= 1.938   train_acc= 1.000   test_loss=2.077   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.919   train_acc= 1.000   test_loss=2.081   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.907   train_acc= 1.000   test_loss=2.050   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.893   train_acc= 1.000   test_loss=2.043   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.885   train_acc= 1.000   test_loss=2.029   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.868   train_acc= 1.000   test_loss=2.015   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.859   train_acc= 1.000   test_loss=1.994   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.855   train_acc= 1.000   test_loss=1.982   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.835   train_acc= 1.000   test_loss=1.993   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.828   train_acc= 1.000   test_loss=1.981   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.815   train_acc= 1.000   test_loss=1.950   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.800   train_acc= 1.000   test_loss=1.944   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.791   train_acc= 1.000   test_loss=1.931   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.778   train_acc= 1.000   test_loss=1.922   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.775   train_acc= 1.000   test_loss=1.911   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.762   train_acc= 1.000   test_loss=1.896   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.748   train_acc= 1.000   test_loss=1.894   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.742   train_acc= 1.000   test_loss=1.867   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.728   train_acc= 1.000   test_loss=1.875   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.718   train_acc= 1.000   test_loss=1.880   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.709   train_acc= 1.000   test_loss=1.854   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.701   train_acc= 1.000   test_loss=1.869   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.690   train_acc= 1.000   test_loss=1.850   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.684   train_acc= 1.000   test_loss=1.847   test_acc= 0.900\n",
      "run time: 1.0483480493227642 min\n",
      "test_acc=0.900\n",
      "run= 2   fold= 2\n",
      "epoch= 0   train_loss= 2.898   train_acc= 0.675   test_loss=2.668   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.646   train_acc= 0.819   test_loss=2.673   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.537   train_acc= 0.916   test_loss=2.587   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.413   train_acc= 0.976   test_loss=2.471   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.370   train_acc= 0.952   test_loss=2.438   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.337   train_acc= 0.952   test_loss=2.440   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.284   train_acc= 0.976   test_loss=2.375   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.260   train_acc= 0.988   test_loss=2.365   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.230   train_acc= 1.000   test_loss=2.323   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.219   train_acc= 0.976   test_loss=2.321   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.179   train_acc= 1.000   test_loss=2.279   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.165   train_acc= 0.988   test_loss=2.271   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.145   train_acc= 1.000   test_loss=2.264   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.117   train_acc= 1.000   test_loss=2.232   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.106   train_acc= 1.000   test_loss=2.220   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.093   train_acc= 1.000   test_loss=2.217   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.073   train_acc= 1.000   test_loss=2.205   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.056   train_acc= 1.000   test_loss=2.180   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.044   train_acc= 1.000   test_loss=2.181   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.026   train_acc= 1.000   test_loss=2.158   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.014   train_acc= 1.000   test_loss=2.156   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.001   train_acc= 1.000   test_loss=2.128   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.980   train_acc= 1.000   test_loss=2.110   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.970   train_acc= 1.000   test_loss=2.084   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.954   train_acc= 1.000   test_loss=2.075   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.942   train_acc= 1.000   test_loss=2.066   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.934   train_acc= 1.000   test_loss=2.056   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.915   train_acc= 1.000   test_loss=2.040   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.902   train_acc= 1.000   test_loss=2.016   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.893   train_acc= 1.000   test_loss=2.015   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.877   train_acc= 1.000   test_loss=2.001   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.885   train_acc= 0.988   test_loss=1.992   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.854   train_acc= 1.000   test_loss=1.988   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.844   train_acc= 1.000   test_loss=1.975   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.838   train_acc= 1.000   test_loss=1.967   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.819   train_acc= 1.000   test_loss=1.952   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.809   train_acc= 1.000   test_loss=1.940   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.799   train_acc= 1.000   test_loss=1.932   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.789   train_acc= 1.000   test_loss=1.920   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.775   train_acc= 1.000   test_loss=1.908   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.764   train_acc= 1.000   test_loss=1.899   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.753   train_acc= 1.000   test_loss=1.885   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.744   train_acc= 1.000   test_loss=1.873   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.734   train_acc= 1.000   test_loss=1.865   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.728   train_acc= 1.000   test_loss=1.855   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.713   train_acc= 1.000   test_loss=1.843   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.703   train_acc= 1.000   test_loss=1.833   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.698   train_acc= 1.000   test_loss=1.817   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.685   train_acc= 1.000   test_loss=1.812   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.677   train_acc= 1.000   test_loss=1.801   test_acc= 0.889\n",
      "run time: 1.0263395309448242 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 3\n",
      "epoch= 0   train_loss= 2.898   train_acc= 0.723   test_loss=2.953   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.633   train_acc= 0.867   test_loss=2.916   test_acc= 0.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2   train_loss= 2.463   train_acc= 0.952   test_loss=2.753   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.415   train_acc= 0.976   test_loss=2.675   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.346   train_acc= 0.952   test_loss=2.706   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.325   train_acc= 0.976   test_loss=2.685   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.264   train_acc= 1.000   test_loss=2.735   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.266   train_acc= 0.988   test_loss=2.635   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.227   train_acc= 0.976   test_loss=2.713   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.203   train_acc= 0.988   test_loss=2.652   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.187   train_acc= 0.988   test_loss=2.608   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.163   train_acc= 0.988   test_loss=2.680   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.135   train_acc= 1.000   test_loss=2.635   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.120   train_acc= 1.000   test_loss=2.621   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.101   train_acc= 1.000   test_loss=2.570   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.085   train_acc= 1.000   test_loss=2.586   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.074   train_acc= 1.000   test_loss=2.541   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.054   train_acc= 1.000   test_loss=2.530   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.043   train_acc= 1.000   test_loss=2.557   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.023   train_acc= 1.000   test_loss=2.521   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.012   train_acc= 1.000   test_loss=2.507   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.994   train_acc= 1.000   test_loss=2.532   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.986   train_acc= 1.000   test_loss=2.520   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.972   train_acc= 1.000   test_loss=2.488   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.956   train_acc= 1.000   test_loss=2.446   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.944   train_acc= 1.000   test_loss=2.450   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.929   train_acc= 1.000   test_loss=2.446   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.922   train_acc= 1.000   test_loss=2.440   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.904   train_acc= 1.000   test_loss=2.430   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.895   train_acc= 1.000   test_loss=2.482   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.880   train_acc= 1.000   test_loss=2.459   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.865   train_acc= 1.000   test_loss=2.426   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.857   train_acc= 1.000   test_loss=2.404   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.843   train_acc= 1.000   test_loss=2.387   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.833   train_acc= 1.000   test_loss=2.382   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.819   train_acc= 1.000   test_loss=2.342   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.809   train_acc= 1.000   test_loss=2.335   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.798   train_acc= 1.000   test_loss=2.330   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.790   train_acc= 1.000   test_loss=2.307   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.780   train_acc= 1.000   test_loss=2.260   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.768   train_acc= 1.000   test_loss=2.280   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.757   train_acc= 1.000   test_loss=2.276   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.745   train_acc= 1.000   test_loss=2.265   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.734   train_acc= 1.000   test_loss=2.232   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.724   train_acc= 1.000   test_loss=2.234   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.713   train_acc= 1.000   test_loss=2.236   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.703   train_acc= 1.000   test_loss=2.241   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.695   train_acc= 1.000   test_loss=2.206   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.684   train_acc= 1.000   test_loss=2.204   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.675   train_acc= 1.000   test_loss=2.198   test_acc= 0.889\n",
      "run time: 0.764215079943339 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 4\n",
      "epoch= 0   train_loss= 2.914   train_acc= 0.614   test_loss=2.814   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.617   train_acc= 0.855   test_loss=2.818   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.447   train_acc= 0.952   test_loss=2.738   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.421   train_acc= 0.940   test_loss=2.726   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.347   train_acc= 0.952   test_loss=2.647   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.359   train_acc= 0.940   test_loss=2.614   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.275   train_acc= 0.988   test_loss=2.678   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.253   train_acc= 0.988   test_loss=2.694   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.235   train_acc= 0.988   test_loss=2.547   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.192   train_acc= 1.000   test_loss=2.592   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.174   train_acc= 1.000   test_loss=2.543   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.151   train_acc= 1.000   test_loss=2.550   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.132   train_acc= 1.000   test_loss=2.499   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.111   train_acc= 1.000   test_loss=2.539   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.096   train_acc= 1.000   test_loss=2.504   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.079   train_acc= 1.000   test_loss=2.509   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.062   train_acc= 1.000   test_loss=2.469   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.053   train_acc= 1.000   test_loss=2.480   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.033   train_acc= 1.000   test_loss=2.430   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.017   train_acc= 1.000   test_loss=2.423   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.005   train_acc= 1.000   test_loss=2.409   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.985   train_acc= 1.000   test_loss=2.417   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.974   train_acc= 1.000   test_loss=2.400   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.960   train_acc= 1.000   test_loss=2.357   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.951   train_acc= 1.000   test_loss=2.380   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.938   train_acc= 1.000   test_loss=2.351   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.922   train_acc= 1.000   test_loss=2.317   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.908   train_acc= 1.000   test_loss=2.318   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.897   train_acc= 1.000   test_loss=2.317   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.895   train_acc= 1.000   test_loss=2.271   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.879   train_acc= 1.000   test_loss=2.270   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.859   train_acc= 1.000   test_loss=2.305   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.852   train_acc= 1.000   test_loss=2.260   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.836   train_acc= 1.000   test_loss=2.252   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.826   train_acc= 1.000   test_loss=2.244   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.814   train_acc= 1.000   test_loss=2.243   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.807   train_acc= 1.000   test_loss=2.251   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.792   train_acc= 1.000   test_loss=2.218   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.790   train_acc= 1.000   test_loss=2.195   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.782   train_acc= 1.000   test_loss=2.252   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.768   train_acc= 1.000   test_loss=2.214   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.752   train_acc= 1.000   test_loss=2.200   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.739   train_acc= 1.000   test_loss=2.151   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.733   train_acc= 1.000   test_loss=2.150   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.718   train_acc= 1.000   test_loss=2.128   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.707   train_acc= 1.000   test_loss=2.116   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.697   train_acc= 1.000   test_loss=2.107   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.687   train_acc= 1.000   test_loss=2.089   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 48   train_loss= 1.678   train_acc= 1.000   test_loss=2.065   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.674   train_acc= 1.000   test_loss=2.073   test_acc= 0.889\n",
      "run time: 0.765680197874705 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 5\n",
      "epoch= 0   train_loss= 2.920   train_acc= 0.663   test_loss=2.847   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.679   train_acc= 0.795   test_loss=2.653   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.529   train_acc= 0.892   test_loss=2.593   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.460   train_acc= 0.964   test_loss=2.595   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.381   train_acc= 0.964   test_loss=2.594   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.343   train_acc= 0.976   test_loss=2.466   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.290   train_acc= 0.988   test_loss=2.485   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.288   train_acc= 0.976   test_loss=2.438   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.233   train_acc= 1.000   test_loss=2.397   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.219   train_acc= 0.988   test_loss=2.359   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.194   train_acc= 1.000   test_loss=2.316   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.185   train_acc= 0.976   test_loss=2.289   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.161   train_acc= 1.000   test_loss=2.252   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.119   train_acc= 1.000   test_loss=2.270   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.107   train_acc= 1.000   test_loss=2.238   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.093   train_acc= 1.000   test_loss=2.228   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.065   train_acc= 1.000   test_loss=2.202   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.066   train_acc= 1.000   test_loss=2.191   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.042   train_acc= 1.000   test_loss=2.156   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.032   train_acc= 1.000   test_loss=2.155   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.017   train_acc= 1.000   test_loss=2.156   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.002   train_acc= 1.000   test_loss=2.118   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.990   train_acc= 1.000   test_loss=2.107   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.969   train_acc= 1.000   test_loss=2.071   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.960   train_acc= 1.000   test_loss=2.070   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.947   train_acc= 1.000   test_loss=2.059   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.931   train_acc= 1.000   test_loss=2.045   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.918   train_acc= 1.000   test_loss=2.035   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.910   train_acc= 1.000   test_loss=2.014   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.891   train_acc= 1.000   test_loss=2.014   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.879   train_acc= 1.000   test_loss=2.003   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.870   train_acc= 1.000   test_loss=1.991   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.855   train_acc= 1.000   test_loss=1.974   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.842   train_acc= 1.000   test_loss=1.964   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.832   train_acc= 1.000   test_loss=1.944   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.820   train_acc= 1.000   test_loss=1.941   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.823   train_acc= 1.000   test_loss=1.922   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.800   train_acc= 1.000   test_loss=1.920   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.793   train_acc= 1.000   test_loss=1.893   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.778   train_acc= 1.000   test_loss=1.888   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.768   train_acc= 1.000   test_loss=1.888   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.754   train_acc= 1.000   test_loss=1.868   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.747   train_acc= 1.000   test_loss=1.861   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.735   train_acc= 1.000   test_loss=1.846   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.726   train_acc= 1.000   test_loss=1.835   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.714   train_acc= 1.000   test_loss=1.827   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.705   train_acc= 1.000   test_loss=1.822   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.695   train_acc= 1.000   test_loss=1.813   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.688   train_acc= 1.000   test_loss=1.794   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.675   train_acc= 1.000   test_loss=1.797   test_acc= 1.000\n",
      "run time: 0.7176204323768616 min\n",
      "test_acc=1.000\n",
      "run= 2   fold= 6\n",
      "epoch= 0   train_loss= 2.977   train_acc= 0.651   test_loss=2.850   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.671   train_acc= 0.819   test_loss=2.595   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.548   train_acc= 0.892   test_loss=2.641   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.439   train_acc= 0.952   test_loss=2.591   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.350   train_acc= 0.976   test_loss=2.618   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.354   train_acc= 0.964   test_loss=2.548   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.268   train_acc= 1.000   test_loss=2.540   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.287   train_acc= 0.964   test_loss=2.557   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.224   train_acc= 1.000   test_loss=2.491   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.200   train_acc= 0.988   test_loss=2.495   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.178   train_acc= 1.000   test_loss=2.514   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.169   train_acc= 0.988   test_loss=2.475   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.142   train_acc= 0.988   test_loss=2.455   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.117   train_acc= 1.000   test_loss=2.484   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.107   train_acc= 1.000   test_loss=2.461   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.083   train_acc= 1.000   test_loss=2.411   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.070   train_acc= 1.000   test_loss=2.415   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.048   train_acc= 1.000   test_loss=2.417   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.036   train_acc= 1.000   test_loss=2.391   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.021   train_acc= 1.000   test_loss=2.392   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.013   train_acc= 1.000   test_loss=2.364   test_acc= 0.778\n",
      "epoch= 21   train_loss= 1.989   train_acc= 1.000   test_loss=2.360   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.981   train_acc= 1.000   test_loss=2.351   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.965   train_acc= 1.000   test_loss=2.320   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.951   train_acc= 1.000   test_loss=2.325   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.939   train_acc= 1.000   test_loss=2.299   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.927   train_acc= 1.000   test_loss=2.318   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.918   train_acc= 1.000   test_loss=2.293   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.898   train_acc= 1.000   test_loss=2.284   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.890   train_acc= 1.000   test_loss=2.267   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.880   train_acc= 1.000   test_loss=2.256   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.864   train_acc= 1.000   test_loss=2.259   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.851   train_acc= 1.000   test_loss=2.232   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.843   train_acc= 1.000   test_loss=2.234   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.829   train_acc= 1.000   test_loss=2.218   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.823   train_acc= 1.000   test_loss=2.199   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.807   train_acc= 1.000   test_loss=2.208   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.795   train_acc= 1.000   test_loss=2.201   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.786   train_acc= 1.000   test_loss=2.168   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.775   train_acc= 1.000   test_loss=2.190   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.762   train_acc= 1.000   test_loss=2.169   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.760   train_acc= 1.000   test_loss=2.173   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.743   train_acc= 1.000   test_loss=2.155   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.732   train_acc= 1.000   test_loss=2.139   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 44   train_loss= 1.721   train_acc= 1.000   test_loss=2.126   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.713   train_acc= 1.000   test_loss=2.123   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.703   train_acc= 1.000   test_loss=2.114   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.692   train_acc= 1.000   test_loss=2.100   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.689   train_acc= 1.000   test_loss=2.096   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.678   train_acc= 1.000   test_loss=2.093   test_acc= 0.778\n",
      "run time: 0.7465438326199849 min\n",
      "test_acc=0.778\n",
      "run= 2   fold= 7\n",
      "epoch= 0   train_loss= 2.881   train_acc= 0.675   test_loss=2.705   test_acc= 1.000\n",
      "epoch= 1   train_loss= 2.650   train_acc= 0.867   test_loss=2.701   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.521   train_acc= 0.928   test_loss=2.661   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.457   train_acc= 0.928   test_loss=2.488   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.400   train_acc= 0.940   test_loss=2.447   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.322   train_acc= 0.988   test_loss=2.398   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.310   train_acc= 0.964   test_loss=2.357   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.255   train_acc= 0.988   test_loss=2.380   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.242   train_acc= 0.988   test_loss=2.313   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.210   train_acc= 0.988   test_loss=2.307   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.193   train_acc= 0.988   test_loss=2.258   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.166   train_acc= 0.988   test_loss=2.251   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.149   train_acc= 1.000   test_loss=2.219   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.126   train_acc= 1.000   test_loss=2.209   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.115   train_acc= 1.000   test_loss=2.206   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.094   train_acc= 1.000   test_loss=2.198   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.083   train_acc= 1.000   test_loss=2.170   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.061   train_acc= 1.000   test_loss=2.145   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.046   train_acc= 1.000   test_loss=2.151   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.030   train_acc= 1.000   test_loss=2.154   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.014   train_acc= 1.000   test_loss=2.102   test_acc= 1.000\n",
      "epoch= 21   train_loss= 1.999   train_acc= 1.000   test_loss=2.083   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.994   train_acc= 1.000   test_loss=2.058   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.969   train_acc= 1.000   test_loss=2.070   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.957   train_acc= 1.000   test_loss=2.054   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.949   train_acc= 1.000   test_loss=2.017   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.935   train_acc= 1.000   test_loss=2.000   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.921   train_acc= 1.000   test_loss=1.996   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.911   train_acc= 1.000   test_loss=2.002   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.897   train_acc= 1.000   test_loss=1.990   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.886   train_acc= 1.000   test_loss=1.983   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.875   train_acc= 1.000   test_loss=1.962   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.868   train_acc= 1.000   test_loss=1.928   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.843   train_acc= 1.000   test_loss=1.917   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.836   train_acc= 1.000   test_loss=1.922   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.826   train_acc= 1.000   test_loss=1.930   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.811   train_acc= 1.000   test_loss=1.910   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.800   train_acc= 1.000   test_loss=1.900   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.792   train_acc= 1.000   test_loss=1.831   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.782   train_acc= 1.000   test_loss=1.823   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.770   train_acc= 1.000   test_loss=1.834   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.756   train_acc= 1.000   test_loss=1.820   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.748   train_acc= 1.000   test_loss=1.810   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.739   train_acc= 1.000   test_loss=1.795   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.728   train_acc= 1.000   test_loss=1.789   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.717   train_acc= 1.000   test_loss=1.776   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.710   train_acc= 1.000   test_loss=1.769   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.697   train_acc= 1.000   test_loss=1.764   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.688   train_acc= 1.000   test_loss=1.771   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.680   train_acc= 1.000   test_loss=1.766   test_acc= 1.000\n",
      "run time: 0.7523416161537171 min\n",
      "test_acc=1.000\n",
      "run= 2   fold= 8\n",
      "epoch= 0   train_loss= 2.819   train_acc= 0.747   test_loss=2.921   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.602   train_acc= 0.904   test_loss=2.643   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.495   train_acc= 0.904   test_loss=2.614   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.433   train_acc= 0.940   test_loss=2.637   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.367   train_acc= 0.964   test_loss=2.521   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.302   train_acc= 0.988   test_loss=2.496   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.292   train_acc= 0.964   test_loss=2.467   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.282   train_acc= 0.988   test_loss=2.419   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.220   train_acc= 1.000   test_loss=2.365   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.212   train_acc= 1.000   test_loss=2.339   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.177   train_acc= 1.000   test_loss=2.330   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.164   train_acc= 1.000   test_loss=2.295   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.153   train_acc= 0.988   test_loss=2.319   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.138   train_acc= 1.000   test_loss=2.252   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.108   train_acc= 1.000   test_loss=2.231   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.114   train_acc= 0.988   test_loss=2.205   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.083   train_acc= 1.000   test_loss=2.177   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.066   train_acc= 1.000   test_loss=2.162   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.051   train_acc= 1.000   test_loss=2.129   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.028   train_acc= 1.000   test_loss=2.119   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.012   train_acc= 1.000   test_loss=2.106   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.002   train_acc= 1.000   test_loss=2.106   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.987   train_acc= 1.000   test_loss=2.091   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.979   train_acc= 1.000   test_loss=2.070   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.959   train_acc= 1.000   test_loss=2.058   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.949   train_acc= 1.000   test_loss=2.046   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.940   train_acc= 1.000   test_loss=2.017   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.922   train_acc= 1.000   test_loss=2.007   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.908   train_acc= 1.000   test_loss=2.000   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.898   train_acc= 1.000   test_loss=1.988   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.884   train_acc= 1.000   test_loss=1.971   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.871   train_acc= 1.000   test_loss=1.955   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.864   train_acc= 1.000   test_loss=1.946   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.847   train_acc= 1.000   test_loss=1.923   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.839   train_acc= 1.000   test_loss=1.922   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.825   train_acc= 1.000   test_loss=1.901   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.812   train_acc= 1.000   test_loss=1.894   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.806   train_acc= 1.000   test_loss=1.872   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.800   train_acc= 1.000   test_loss=1.880   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.783   train_acc= 1.000   test_loss=1.857   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 40   train_loss= 1.774   train_acc= 1.000   test_loss=1.849   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.762   train_acc= 1.000   test_loss=1.837   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.749   train_acc= 1.000   test_loss=1.829   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.745   train_acc= 1.000   test_loss=1.814   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.728   train_acc= 1.000   test_loss=1.801   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.721   train_acc= 1.000   test_loss=1.787   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.709   train_acc= 1.000   test_loss=1.779   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.705   train_acc= 1.000   test_loss=1.762   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.690   train_acc= 1.000   test_loss=1.762   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.682   train_acc= 1.000   test_loss=1.749   test_acc= 1.000\n",
      "run time: 0.725391165415446 min\n",
      "test_acc=1.000\n",
      "run= 2   fold= 9\n",
      "epoch= 0   train_loss= 2.959   train_acc= 0.614   test_loss=2.886   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.612   train_acc= 0.916   test_loss=2.855   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.501   train_acc= 0.904   test_loss=2.774   test_acc= 0.556\n",
      "epoch= 3   train_loss= 2.380   train_acc= 0.988   test_loss=2.739   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.349   train_acc= 0.976   test_loss=2.726   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.296   train_acc= 1.000   test_loss=2.709   test_acc= 0.667\n",
      "epoch= 6   train_loss= 2.262   train_acc= 0.988   test_loss=2.691   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.241   train_acc= 1.000   test_loss=2.697   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.203   train_acc= 1.000   test_loss=2.712   test_acc= 0.667\n",
      "epoch= 9   train_loss= 2.197   train_acc= 0.988   test_loss=2.623   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.165   train_acc= 1.000   test_loss=2.587   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.148   train_acc= 1.000   test_loss=2.603   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.129   train_acc= 1.000   test_loss=2.643   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.114   train_acc= 1.000   test_loss=2.604   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.101   train_acc= 1.000   test_loss=2.571   test_acc= 0.667\n",
      "epoch= 15   train_loss= 2.085   train_acc= 1.000   test_loss=2.551   test_acc= 0.667\n",
      "epoch= 16   train_loss= 2.060   train_acc= 1.000   test_loss=2.529   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.047   train_acc= 1.000   test_loss=2.502   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.032   train_acc= 1.000   test_loss=2.537   test_acc= 0.667\n",
      "epoch= 19   train_loss= 2.019   train_acc= 1.000   test_loss=2.489   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.012   train_acc= 1.000   test_loss=2.467   test_acc= 0.667\n",
      "epoch= 21   train_loss= 1.989   train_acc= 1.000   test_loss=2.491   test_acc= 0.667\n",
      "epoch= 22   train_loss= 1.977   train_acc= 1.000   test_loss=2.465   test_acc= 0.667\n",
      "epoch= 23   train_loss= 1.961   train_acc= 1.000   test_loss=2.449   test_acc= 0.667\n",
      "epoch= 24   train_loss= 1.946   train_acc= 1.000   test_loss=2.422   test_acc= 0.667\n",
      "epoch= 25   train_loss= 1.933   train_acc= 1.000   test_loss=2.403   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.919   train_acc= 1.000   test_loss=2.403   test_acc= 0.667\n",
      "epoch= 27   train_loss= 1.908   train_acc= 1.000   test_loss=2.388   test_acc= 0.667\n",
      "epoch= 28   train_loss= 1.894   train_acc= 1.000   test_loss=2.354   test_acc= 0.667\n",
      "epoch= 29   train_loss= 1.887   train_acc= 1.000   test_loss=2.379   test_acc= 0.667\n",
      "epoch= 30   train_loss= 1.876   train_acc= 1.000   test_loss=2.365   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.866   train_acc= 1.000   test_loss=2.332   test_acc= 0.667\n",
      "epoch= 32   train_loss= 1.851   train_acc= 1.000   test_loss=2.365   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.842   train_acc= 1.000   test_loss=2.286   test_acc= 0.667\n",
      "epoch= 34   train_loss= 1.828   train_acc= 1.000   test_loss=2.315   test_acc= 0.667\n",
      "epoch= 35   train_loss= 1.814   train_acc= 1.000   test_loss=2.303   test_acc= 0.667\n",
      "epoch= 36   train_loss= 1.801   train_acc= 1.000   test_loss=2.300   test_acc= 0.667\n",
      "epoch= 37   train_loss= 1.793   train_acc= 1.000   test_loss=2.269   test_acc= 0.667\n",
      "epoch= 38   train_loss= 1.782   train_acc= 1.000   test_loss=2.265   test_acc= 0.667\n",
      "epoch= 39   train_loss= 1.768   train_acc= 1.000   test_loss=2.248   test_acc= 0.667\n",
      "epoch= 40   train_loss= 1.760   train_acc= 1.000   test_loss=2.243   test_acc= 0.667\n",
      "epoch= 41   train_loss= 1.747   train_acc= 1.000   test_loss=2.245   test_acc= 0.667\n",
      "epoch= 42   train_loss= 1.738   train_acc= 1.000   test_loss=2.234   test_acc= 0.667\n",
      "epoch= 43   train_loss= 1.731   train_acc= 1.000   test_loss=2.239   test_acc= 0.667\n",
      "epoch= 44   train_loss= 1.733   train_acc= 0.988   test_loss=2.232   test_acc= 0.667\n",
      "epoch= 45   train_loss= 1.710   train_acc= 1.000   test_loss=2.221   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.697   train_acc= 1.000   test_loss=2.206   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.687   train_acc= 1.000   test_loss=2.197   test_acc= 0.667\n",
      "epoch= 48   train_loss= 1.678   train_acc= 1.000   test_loss=2.180   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.667   train_acc= 1.000   test_loss=2.160   test_acc= 0.778\n",
      "run time: 0.7092860658963521 min\n",
      "test_acc=0.778\n",
      "run= 3   fold= 0\n",
      "epoch= 0   train_loss= 2.946   train_acc= 0.622   test_loss=2.927   test_acc= 0.400\n",
      "epoch= 1   train_loss= 2.684   train_acc= 0.866   test_loss=2.760   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.549   train_acc= 0.902   test_loss=2.687   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.397   train_acc= 0.963   test_loss=2.616   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.371   train_acc= 0.976   test_loss=2.534   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.340   train_acc= 0.963   test_loss=2.486   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.302   train_acc= 0.988   test_loss=2.488   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.258   train_acc= 0.988   test_loss=2.351   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.218   train_acc= 1.000   test_loss=2.324   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.201   train_acc= 1.000   test_loss=2.305   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.179   train_acc= 1.000   test_loss=2.267   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.153   train_acc= 1.000   test_loss=2.248   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.142   train_acc= 0.988   test_loss=2.227   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.125   train_acc= 0.988   test_loss=2.217   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.095   train_acc= 1.000   test_loss=2.205   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.081   train_acc= 1.000   test_loss=2.173   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.066   train_acc= 1.000   test_loss=2.160   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.047   train_acc= 1.000   test_loss=2.135   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.044   train_acc= 1.000   test_loss=2.130   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.020   train_acc= 1.000   test_loss=2.102   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.003   train_acc= 1.000   test_loss=2.075   test_acc= 1.000\n",
      "epoch= 21   train_loss= 1.986   train_acc= 1.000   test_loss=2.068   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.984   train_acc= 1.000   test_loss=2.043   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.962   train_acc= 1.000   test_loss=2.033   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.951   train_acc= 1.000   test_loss=2.016   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.942   train_acc= 1.000   test_loss=2.022   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.924   train_acc= 1.000   test_loss=1.999   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.909   train_acc= 1.000   test_loss=1.977   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.899   train_acc= 1.000   test_loss=1.977   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.885   train_acc= 1.000   test_loss=1.952   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.878   train_acc= 1.000   test_loss=1.940   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.859   train_acc= 1.000   test_loss=1.924   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.847   train_acc= 1.000   test_loss=1.910   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.840   train_acc= 1.000   test_loss=1.896   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.827   train_acc= 1.000   test_loss=1.878   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.820   train_acc= 1.000   test_loss=1.896   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 36   train_loss= 1.801   train_acc= 1.000   test_loss=1.870   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.793   train_acc= 1.000   test_loss=1.855   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.782   train_acc= 1.000   test_loss=1.844   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.771   train_acc= 1.000   test_loss=1.828   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.760   train_acc= 1.000   test_loss=1.817   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.748   train_acc= 1.000   test_loss=1.803   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.739   train_acc= 1.000   test_loss=1.795   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.729   train_acc= 1.000   test_loss=1.782   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.722   train_acc= 1.000   test_loss=1.784   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.712   train_acc= 1.000   test_loss=1.775   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.698   train_acc= 1.000   test_loss=1.762   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.694   train_acc= 1.000   test_loss=1.757   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.681   train_acc= 1.000   test_loss=1.741   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.675   train_acc= 1.000   test_loss=1.729   test_acc= 1.000\n",
      "run time: 0.7119500517845154 min\n",
      "test_acc=1.000\n",
      "run= 3   fold= 1\n",
      "epoch= 0   train_loss= 2.880   train_acc= 0.671   test_loss=2.724   test_acc= 0.900\n",
      "epoch= 1   train_loss= 2.608   train_acc= 0.854   test_loss=2.686   test_acc= 0.700\n",
      "epoch= 2   train_loss= 2.502   train_acc= 0.890   test_loss=2.578   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.395   train_acc= 0.976   test_loss=2.594   test_acc= 0.800\n",
      "epoch= 4   train_loss= 2.366   train_acc= 0.963   test_loss=2.566   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.288   train_acc= 1.000   test_loss=2.522   test_acc= 0.800\n",
      "epoch= 6   train_loss= 2.288   train_acc= 0.988   test_loss=2.496   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.256   train_acc= 0.976   test_loss=2.440   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.227   train_acc= 1.000   test_loss=2.435   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.199   train_acc= 1.000   test_loss=2.434   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.176   train_acc= 1.000   test_loss=2.420   test_acc= 0.800\n",
      "epoch= 11   train_loss= 2.152   train_acc= 1.000   test_loss=2.403   test_acc= 0.800\n",
      "epoch= 12   train_loss= 2.142   train_acc= 1.000   test_loss=2.404   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.123   train_acc= 1.000   test_loss=2.385   test_acc= 0.800\n",
      "epoch= 14   train_loss= 2.111   train_acc= 1.000   test_loss=2.372   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.089   train_acc= 1.000   test_loss=2.358   test_acc= 0.800\n",
      "epoch= 16   train_loss= 2.077   train_acc= 1.000   test_loss=2.358   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.080   train_acc= 0.988   test_loss=2.352   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.061   train_acc= 1.000   test_loss=2.320   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.033   train_acc= 1.000   test_loss=2.288   test_acc= 0.800\n",
      "epoch= 20   train_loss= 2.034   train_acc= 0.988   test_loss=2.287   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.003   train_acc= 1.000   test_loss=2.270   test_acc= 0.900\n",
      "epoch= 22   train_loss= 1.996   train_acc= 1.000   test_loss=2.258   test_acc= 0.900\n",
      "epoch= 23   train_loss= 1.976   train_acc= 1.000   test_loss=2.245   test_acc= 0.900\n",
      "epoch= 24   train_loss= 1.960   train_acc= 1.000   test_loss=2.221   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.952   train_acc= 1.000   test_loss=2.204   test_acc= 0.800\n",
      "epoch= 26   train_loss= 1.934   train_acc= 1.000   test_loss=2.190   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.926   train_acc= 1.000   test_loss=2.186   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.911   train_acc= 1.000   test_loss=2.170   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.901   train_acc= 1.000   test_loss=2.170   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.887   train_acc= 1.000   test_loss=2.150   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.874   train_acc= 1.000   test_loss=2.137   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.862   train_acc= 1.000   test_loss=2.122   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.854   train_acc= 1.000   test_loss=2.112   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.839   train_acc= 1.000   test_loss=2.106   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.828   train_acc= 1.000   test_loss=2.096   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.817   train_acc= 1.000   test_loss=2.081   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.807   train_acc= 1.000   test_loss=2.071   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.793   train_acc= 1.000   test_loss=2.057   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.781   train_acc= 1.000   test_loss=2.045   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.771   train_acc= 1.000   test_loss=2.040   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.770   train_acc= 1.000   test_loss=2.038   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.753   train_acc= 1.000   test_loss=2.023   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.744   train_acc= 1.000   test_loss=2.019   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.733   train_acc= 1.000   test_loss=2.016   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.724   train_acc= 1.000   test_loss=2.008   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.710   train_acc= 1.000   test_loss=1.988   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.705   train_acc= 1.000   test_loss=1.973   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.694   train_acc= 1.000   test_loss=1.948   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.682   train_acc= 1.000   test_loss=1.949   test_acc= 0.900\n",
      "run time: 0.7048299352327982 min\n",
      "test_acc=0.900\n",
      "run= 3   fold= 2\n",
      "epoch= 0   train_loss= 2.900   train_acc= 0.675   test_loss=2.781   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.656   train_acc= 0.867   test_loss=2.712   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.528   train_acc= 0.867   test_loss=2.593   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.436   train_acc= 0.952   test_loss=2.548   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.383   train_acc= 0.976   test_loss=2.499   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.324   train_acc= 0.976   test_loss=2.450   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.312   train_acc= 0.976   test_loss=2.428   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.270   train_acc= 0.976   test_loss=2.412   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.230   train_acc= 0.988   test_loss=2.408   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.188   train_acc= 1.000   test_loss=2.391   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.183   train_acc= 1.000   test_loss=2.371   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.169   train_acc= 1.000   test_loss=2.369   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.144   train_acc= 1.000   test_loss=2.364   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.130   train_acc= 0.988   test_loss=2.355   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.107   train_acc= 1.000   test_loss=2.320   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.089   train_acc= 1.000   test_loss=2.299   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.074   train_acc= 1.000   test_loss=2.277   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.071   train_acc= 1.000   test_loss=2.281   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.044   train_acc= 1.000   test_loss=2.287   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.053   train_acc= 0.988   test_loss=2.283   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.021   train_acc= 1.000   test_loss=2.263   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.999   train_acc= 1.000   test_loss=2.245   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.993   train_acc= 1.000   test_loss=2.230   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.973   train_acc= 1.000   test_loss=2.208   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.961   train_acc= 1.000   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.952   train_acc= 1.000   test_loss=2.180   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.940   train_acc= 1.000   test_loss=2.170   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.921   train_acc= 1.000   test_loss=2.144   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.910   train_acc= 1.000   test_loss=2.133   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.899   train_acc= 1.000   test_loss=2.129   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.887   train_acc= 1.000   test_loss=2.124   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.875   train_acc= 1.000   test_loss=2.110   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 32   train_loss= 1.861   train_acc= 1.000   test_loss=2.090   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.847   train_acc= 1.000   test_loss=2.094   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.834   train_acc= 1.000   test_loss=2.085   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.837   train_acc= 1.000   test_loss=2.071   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.814   train_acc= 1.000   test_loss=2.060   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.803   train_acc= 1.000   test_loss=2.049   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.794   train_acc= 1.000   test_loss=2.040   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.785   train_acc= 1.000   test_loss=2.022   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.768   train_acc= 1.000   test_loss=2.011   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.761   train_acc= 1.000   test_loss=1.996   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.753   train_acc= 1.000   test_loss=1.989   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.736   train_acc= 1.000   test_loss=1.979   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.736   train_acc= 1.000   test_loss=1.975   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.720   train_acc= 1.000   test_loss=1.981   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.706   train_acc= 1.000   test_loss=1.972   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.699   train_acc= 1.000   test_loss=1.964   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.690   train_acc= 1.000   test_loss=1.951   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.681   train_acc= 1.000   test_loss=1.943   test_acc= 0.889\n",
      "run time: 0.7105218847592671 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 3\n",
      "epoch= 0   train_loss= 2.885   train_acc= 0.639   test_loss=2.775   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.625   train_acc= 0.904   test_loss=2.628   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.517   train_acc= 0.940   test_loss=2.600   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.481   train_acc= 0.904   test_loss=2.576   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.353   train_acc= 0.988   test_loss=2.474   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.354   train_acc= 0.976   test_loss=2.477   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.301   train_acc= 0.988   test_loss=2.438   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.256   train_acc= 1.000   test_loss=2.430   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.244   train_acc= 0.976   test_loss=2.401   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.216   train_acc= 0.988   test_loss=2.335   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.185   train_acc= 1.000   test_loss=2.350   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.185   train_acc= 0.988   test_loss=2.256   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.166   train_acc= 1.000   test_loss=2.280   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.133   train_acc= 1.000   test_loss=2.228   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.115   train_acc= 1.000   test_loss=2.229   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.096   train_acc= 1.000   test_loss=2.185   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.087   train_acc= 0.988   test_loss=2.170   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.078   train_acc= 1.000   test_loss=2.181   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.051   train_acc= 1.000   test_loss=2.149   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.033   train_acc= 1.000   test_loss=2.153   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.025   train_acc= 1.000   test_loss=2.121   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.004   train_acc= 1.000   test_loss=2.104   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.992   train_acc= 1.000   test_loss=2.089   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.978   train_acc= 1.000   test_loss=2.077   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.968   train_acc= 1.000   test_loss=2.077   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.958   train_acc= 0.988   test_loss=2.061   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.940   train_acc= 1.000   test_loss=2.037   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.930   train_acc= 1.000   test_loss=2.024   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.921   train_acc= 1.000   test_loss=1.992   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.908   train_acc= 1.000   test_loss=2.011   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.889   train_acc= 1.000   test_loss=1.977   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.876   train_acc= 1.000   test_loss=1.971   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.862   train_acc= 1.000   test_loss=1.953   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.854   train_acc= 1.000   test_loss=1.938   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.839   train_acc= 1.000   test_loss=1.923   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.831   train_acc= 1.000   test_loss=1.916   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.818   train_acc= 1.000   test_loss=1.910   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.802   train_acc= 1.000   test_loss=1.902   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.793   train_acc= 1.000   test_loss=1.881   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.781   train_acc= 1.000   test_loss=1.863   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.774   train_acc= 1.000   test_loss=1.864   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.761   train_acc= 1.000   test_loss=1.845   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.750   train_acc= 1.000   test_loss=1.830   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.743   train_acc= 1.000   test_loss=1.823   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.734   train_acc= 1.000   test_loss=1.804   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.721   train_acc= 1.000   test_loss=1.786   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.714   train_acc= 1.000   test_loss=1.782   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.700   train_acc= 1.000   test_loss=1.783   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.690   train_acc= 1.000   test_loss=1.777   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.692   train_acc= 1.000   test_loss=1.745   test_acc= 1.000\n",
      "run time: 0.7037633021672567 min\n",
      "test_acc=1.000\n",
      "run= 3   fold= 4\n",
      "epoch= 0   train_loss= 2.967   train_acc= 0.614   test_loss=2.879   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.646   train_acc= 0.855   test_loss=2.714   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.488   train_acc= 0.940   test_loss=2.735   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.427   train_acc= 0.928   test_loss=2.610   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.427   train_acc= 0.904   test_loss=2.547   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.314   train_acc= 0.976   test_loss=2.635   test_acc= 0.667\n",
      "epoch= 6   train_loss= 2.295   train_acc= 0.988   test_loss=2.529   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.264   train_acc= 0.988   test_loss=2.415   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.234   train_acc= 0.988   test_loss=2.441   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.214   train_acc= 0.976   test_loss=2.591   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.176   train_acc= 1.000   test_loss=2.452   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.176   train_acc= 0.988   test_loss=2.407   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.137   train_acc= 1.000   test_loss=2.377   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.125   train_acc= 1.000   test_loss=2.377   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.110   train_acc= 1.000   test_loss=2.366   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.109   train_acc= 0.976   test_loss=2.332   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.077   train_acc= 1.000   test_loss=2.343   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.068   train_acc= 1.000   test_loss=2.336   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.046   train_acc= 1.000   test_loss=2.303   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.027   train_acc= 1.000   test_loss=2.266   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.023   train_acc= 1.000   test_loss=2.261   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.996   train_acc= 1.000   test_loss=2.256   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.987   train_acc= 1.000   test_loss=2.216   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.971   train_acc= 1.000   test_loss=2.209   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.961   train_acc= 1.000   test_loss=2.199   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.958   train_acc= 1.000   test_loss=2.156   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.940   train_acc= 1.000   test_loss=2.175   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.930   train_acc= 1.000   test_loss=2.206   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 28   train_loss= 1.906   train_acc= 1.000   test_loss=2.179   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.898   train_acc= 1.000   test_loss=2.146   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.884   train_acc= 1.000   test_loss=2.135   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.868   train_acc= 1.000   test_loss=2.121   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.865   train_acc= 1.000   test_loss=2.086   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.847   train_acc= 1.000   test_loss=2.103   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.835   train_acc= 1.000   test_loss=2.105   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.825   train_acc= 1.000   test_loss=2.125   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.812   train_acc= 1.000   test_loss=2.087   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.804   train_acc= 1.000   test_loss=2.075   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.792   train_acc= 1.000   test_loss=2.038   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.780   train_acc= 1.000   test_loss=2.024   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.773   train_acc= 1.000   test_loss=2.017   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.763   train_acc= 1.000   test_loss=2.040   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.746   train_acc= 1.000   test_loss=2.020   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.737   train_acc= 1.000   test_loss=2.012   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.728   train_acc= 1.000   test_loss=1.973   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.727   train_acc= 1.000   test_loss=2.013   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.708   train_acc= 1.000   test_loss=1.977   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.698   train_acc= 1.000   test_loss=1.954   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.700   train_acc= 1.000   test_loss=1.971   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.683   train_acc= 1.000   test_loss=1.933   test_acc= 0.889\n",
      "run time: 0.7080424149831136 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 5\n",
      "epoch= 0   train_loss= 2.942   train_acc= 0.614   test_loss=2.766   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.640   train_acc= 0.831   test_loss=2.624   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.460   train_acc= 0.952   test_loss=2.615   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.391   train_acc= 0.976   test_loss=2.587   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.378   train_acc= 0.964   test_loss=2.550   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.370   train_acc= 0.952   test_loss=2.478   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.341   train_acc= 0.964   test_loss=2.463   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.247   train_acc= 1.000   test_loss=2.417   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.221   train_acc= 0.988   test_loss=2.389   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.196   train_acc= 1.000   test_loss=2.348   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.180   train_acc= 1.000   test_loss=2.317   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.161   train_acc= 1.000   test_loss=2.331   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.137   train_acc= 1.000   test_loss=2.276   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.114   train_acc= 1.000   test_loss=2.277   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.102   train_acc= 1.000   test_loss=2.283   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.082   train_acc= 1.000   test_loss=2.247   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.069   train_acc= 1.000   test_loss=2.219   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.051   train_acc= 1.000   test_loss=2.207   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.038   train_acc= 1.000   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.024   train_acc= 1.000   test_loss=2.169   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.008   train_acc= 1.000   test_loss=2.137   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.990   train_acc= 1.000   test_loss=2.135   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.976   train_acc= 1.000   test_loss=2.121   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.962   train_acc= 1.000   test_loss=2.108   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.951   train_acc= 1.000   test_loss=2.075   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.944   train_acc= 1.000   test_loss=2.111   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.928   train_acc= 1.000   test_loss=2.080   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.910   train_acc= 1.000   test_loss=2.058   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.901   train_acc= 1.000   test_loss=2.077   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.890   train_acc= 1.000   test_loss=2.026   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.879   train_acc= 1.000   test_loss=2.044   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.870   train_acc= 1.000   test_loss=1.984   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.852   train_acc= 1.000   test_loss=1.986   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.842   train_acc= 1.000   test_loss=1.976   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.829   train_acc= 1.000   test_loss=1.967   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.816   train_acc= 1.000   test_loss=1.964   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.805   train_acc= 1.000   test_loss=1.948   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.798   train_acc= 1.000   test_loss=1.936   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.785   train_acc= 1.000   test_loss=1.929   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.773   train_acc= 1.000   test_loss=1.921   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.764   train_acc= 1.000   test_loss=1.912   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.751   train_acc= 1.000   test_loss=1.904   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.742   train_acc= 1.000   test_loss=1.886   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.730   train_acc= 1.000   test_loss=1.877   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.721   train_acc= 1.000   test_loss=1.860   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.708   train_acc= 1.000   test_loss=1.849   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.703   train_acc= 1.000   test_loss=1.845   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.695   train_acc= 1.000   test_loss=1.842   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.685   train_acc= 1.000   test_loss=1.829   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.672   train_acc= 1.000   test_loss=1.813   test_acc= 0.889\n",
      "run time: 0.7316919644673665 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 6\n",
      "epoch= 0   train_loss= 2.885   train_acc= 0.627   test_loss=2.721   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.588   train_acc= 0.916   test_loss=2.659   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.508   train_acc= 0.904   test_loss=2.611   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.422   train_acc= 0.964   test_loss=2.537   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.356   train_acc= 0.964   test_loss=2.484   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.343   train_acc= 0.940   test_loss=2.446   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.276   train_acc= 1.000   test_loss=2.440   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.250   train_acc= 0.988   test_loss=2.395   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.213   train_acc= 1.000   test_loss=2.378   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.188   train_acc= 1.000   test_loss=2.348   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.179   train_acc= 0.988   test_loss=2.421   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.163   train_acc= 1.000   test_loss=2.309   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.142   train_acc= 1.000   test_loss=2.298   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.117   train_acc= 1.000   test_loss=2.260   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.102   train_acc= 0.988   test_loss=2.255   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.082   train_acc= 1.000   test_loss=2.252   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.066   train_acc= 1.000   test_loss=2.219   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.053   train_acc= 1.000   test_loss=2.205   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.044   train_acc= 1.000   test_loss=2.194   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.040   train_acc= 0.988   test_loss=2.195   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.004   train_acc= 1.000   test_loss=2.154   test_acc= 1.000\n",
      "epoch= 21   train_loss= 1.992   train_acc= 1.000   test_loss=2.140   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.982   train_acc= 1.000   test_loss=2.137   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.963   train_acc= 1.000   test_loss=2.119   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 24   train_loss= 1.953   train_acc= 1.000   test_loss=2.099   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.940   train_acc= 1.000   test_loss=2.072   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.931   train_acc= 1.000   test_loss=2.052   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.916   train_acc= 1.000   test_loss=2.049   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.898   train_acc= 1.000   test_loss=2.038   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.891   train_acc= 1.000   test_loss=2.019   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.878   train_acc= 1.000   test_loss=1.999   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.868   train_acc= 1.000   test_loss=1.994   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.852   train_acc= 1.000   test_loss=1.983   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.844   train_acc= 1.000   test_loss=1.984   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.829   train_acc= 1.000   test_loss=1.972   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.819   train_acc= 1.000   test_loss=1.958   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.806   train_acc= 1.000   test_loss=1.943   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.795   train_acc= 1.000   test_loss=1.933   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.785   train_acc= 1.000   test_loss=1.920   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.772   train_acc= 1.000   test_loss=1.905   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.762   train_acc= 1.000   test_loss=1.889   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.753   train_acc= 1.000   test_loss=1.877   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.744   train_acc= 1.000   test_loss=1.873   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.733   train_acc= 1.000   test_loss=1.871   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.729   train_acc= 1.000   test_loss=1.867   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.712   train_acc= 1.000   test_loss=1.843   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.701   train_acc= 1.000   test_loss=1.838   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.693   train_acc= 1.000   test_loss=1.824   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.682   train_acc= 1.000   test_loss=1.810   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.675   train_acc= 1.000   test_loss=1.798   test_acc= 1.000\n",
      "run time: 0.7117776354153951 min\n",
      "test_acc=1.000\n",
      "run= 3   fold= 7\n",
      "epoch= 0   train_loss= 2.955   train_acc= 0.590   test_loss=2.824   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.656   train_acc= 0.807   test_loss=2.738   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.562   train_acc= 0.867   test_loss=2.619   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.458   train_acc= 0.904   test_loss=2.592   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.415   train_acc= 0.928   test_loss=2.535   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.337   train_acc= 0.964   test_loss=2.519   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.306   train_acc= 0.976   test_loss=2.434   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.268   train_acc= 0.988   test_loss=2.445   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.242   train_acc= 0.988   test_loss=2.372   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.197   train_acc= 1.000   test_loss=2.351   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.179   train_acc= 1.000   test_loss=2.315   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.151   train_acc= 1.000   test_loss=2.291   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.140   train_acc= 1.000   test_loss=2.268   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.132   train_acc= 0.988   test_loss=2.245   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.106   train_acc= 1.000   test_loss=2.224   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.085   train_acc= 1.000   test_loss=2.221   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.082   train_acc= 1.000   test_loss=2.184   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.054   train_acc= 1.000   test_loss=2.174   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.038   train_acc= 1.000   test_loss=2.169   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.021   train_acc= 1.000   test_loss=2.150   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.007   train_acc= 1.000   test_loss=2.137   test_acc= 1.000\n",
      "epoch= 21   train_loss= 1.994   train_acc= 1.000   test_loss=2.117   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.982   train_acc= 1.000   test_loss=2.104   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.977   train_acc= 1.000   test_loss=2.111   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.955   train_acc= 1.000   test_loss=2.078   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.939   train_acc= 1.000   test_loss=2.056   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.929   train_acc= 1.000   test_loss=2.047   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.913   train_acc= 1.000   test_loss=2.029   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.901   train_acc= 1.000   test_loss=2.015   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.887   train_acc= 1.000   test_loss=2.001   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.875   train_acc= 1.000   test_loss=1.988   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.863   train_acc= 1.000   test_loss=1.979   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.854   train_acc= 1.000   test_loss=1.961   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.841   train_acc= 1.000   test_loss=1.949   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.828   train_acc= 1.000   test_loss=1.937   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.819   train_acc= 1.000   test_loss=1.925   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.808   train_acc= 1.000   test_loss=1.913   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.794   train_acc= 1.000   test_loss=1.900   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.785   train_acc= 1.000   test_loss=1.884   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.773   train_acc= 1.000   test_loss=1.873   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.762   train_acc= 1.000   test_loss=1.864   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.752   train_acc= 1.000   test_loss=1.853   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.743   train_acc= 1.000   test_loss=1.845   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.740   train_acc= 1.000   test_loss=1.837   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.719   train_acc= 1.000   test_loss=1.822   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.711   train_acc= 1.000   test_loss=1.810   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.702   train_acc= 1.000   test_loss=1.806   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.690   train_acc= 1.000   test_loss=1.795   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.682   train_acc= 1.000   test_loss=1.783   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.673   train_acc= 1.000   test_loss=1.772   test_acc= 1.000\n",
      "run time: 0.7213616291681926 min\n",
      "test_acc=1.000\n",
      "run= 3   fold= 8\n",
      "epoch= 0   train_loss= 2.931   train_acc= 0.627   test_loss=2.890   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.591   train_acc= 0.880   test_loss=2.796   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.494   train_acc= 0.892   test_loss=2.748   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.436   train_acc= 0.940   test_loss=2.759   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.372   train_acc= 0.976   test_loss=2.681   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.308   train_acc= 0.976   test_loss=2.650   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.277   train_acc= 0.988   test_loss=2.640   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.269   train_acc= 0.988   test_loss=2.605   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.242   train_acc= 0.976   test_loss=2.580   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.196   train_acc= 1.000   test_loss=2.589   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.187   train_acc= 0.988   test_loss=2.547   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.153   train_acc= 1.000   test_loss=2.541   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.133   train_acc= 0.988   test_loss=2.571   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.124   train_acc= 1.000   test_loss=2.562   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.097   train_acc= 1.000   test_loss=2.526   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.082   train_acc= 1.000   test_loss=2.517   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.078   train_acc= 0.988   test_loss=2.526   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.048   train_acc= 1.000   test_loss=2.497   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.045   train_acc= 1.000   test_loss=2.484   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.018   train_acc= 1.000   test_loss=2.474   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 20   train_loss= 2.009   train_acc= 1.000   test_loss=2.484   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.000   train_acc= 1.000   test_loss=2.433   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.989   train_acc= 1.000   test_loss=2.415   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.965   train_acc= 1.000   test_loss=2.424   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.958   train_acc= 1.000   test_loss=2.391   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.938   train_acc= 1.000   test_loss=2.405   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.926   train_acc= 1.000   test_loss=2.388   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.913   train_acc= 1.000   test_loss=2.389   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.898   train_acc= 1.000   test_loss=2.374   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.886   train_acc= 1.000   test_loss=2.364   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.874   train_acc= 1.000   test_loss=2.355   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.864   train_acc= 1.000   test_loss=2.329   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.854   train_acc= 1.000   test_loss=2.330   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.838   train_acc= 1.000   test_loss=2.333   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.836   train_acc= 1.000   test_loss=2.301   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.817   train_acc= 1.000   test_loss=2.292   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.805   train_acc= 1.000   test_loss=2.279   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.793   train_acc= 1.000   test_loss=2.279   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.783   train_acc= 1.000   test_loss=2.268   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.772   train_acc= 1.000   test_loss=2.266   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.767   train_acc= 1.000   test_loss=2.250   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.750   train_acc= 1.000   test_loss=2.234   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.739   train_acc= 1.000   test_loss=2.222   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.731   train_acc= 1.000   test_loss=2.215   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.719   train_acc= 1.000   test_loss=2.205   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.711   train_acc= 1.000   test_loss=2.209   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.700   train_acc= 1.000   test_loss=2.190   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.690   train_acc= 1.000   test_loss=2.188   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.680   train_acc= 1.000   test_loss=2.169   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.673   train_acc= 1.000   test_loss=2.166   test_acc= 0.889\n",
      "run time: 0.7284290830294291 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 9\n",
      "epoch= 0   train_loss= 2.922   train_acc= 0.699   test_loss=2.885   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.573   train_acc= 0.880   test_loss=2.955   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.473   train_acc= 0.916   test_loss=2.771   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.409   train_acc= 0.952   test_loss=2.749   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.344   train_acc= 0.976   test_loss=2.787   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.323   train_acc= 0.964   test_loss=2.756   test_acc= 0.556\n",
      "epoch= 6   train_loss= 2.287   train_acc= 0.988   test_loss=2.683   test_acc= 0.556\n",
      "epoch= 7   train_loss= 2.248   train_acc= 0.988   test_loss=2.701   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.227   train_acc= 1.000   test_loss=2.666   test_acc= 0.667\n",
      "epoch= 9   train_loss= 2.200   train_acc= 1.000   test_loss=2.629   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.179   train_acc= 1.000   test_loss=2.661   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.175   train_acc= 0.988   test_loss=2.648   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.142   train_acc= 1.000   test_loss=2.608   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.121   train_acc= 1.000   test_loss=2.604   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.102   train_acc= 1.000   test_loss=2.589   test_acc= 0.667\n",
      "epoch= 15   train_loss= 2.096   train_acc= 1.000   test_loss=2.641   test_acc= 0.556\n",
      "epoch= 16   train_loss= 2.080   train_acc= 1.000   test_loss=2.608   test_acc= 0.556\n",
      "epoch= 17   train_loss= 2.057   train_acc= 1.000   test_loss=2.591   test_acc= 0.556\n",
      "epoch= 18   train_loss= 2.041   train_acc= 1.000   test_loss=2.569   test_acc= 0.556\n",
      "epoch= 19   train_loss= 2.029   train_acc= 1.000   test_loss=2.588   test_acc= 0.556\n",
      "epoch= 20   train_loss= 2.016   train_acc= 1.000   test_loss=2.552   test_acc= 0.556\n",
      "epoch= 21   train_loss= 2.004   train_acc= 1.000   test_loss=2.549   test_acc= 0.556\n",
      "epoch= 22   train_loss= 1.989   train_acc= 1.000   test_loss=2.529   test_acc= 0.556\n",
      "epoch= 23   train_loss= 1.985   train_acc= 1.000   test_loss=2.538   test_acc= 0.556\n",
      "epoch= 24   train_loss= 1.959   train_acc= 1.000   test_loss=2.500   test_acc= 0.556\n",
      "epoch= 25   train_loss= 1.946   train_acc= 1.000   test_loss=2.506   test_acc= 0.556\n",
      "epoch= 26   train_loss= 1.935   train_acc= 1.000   test_loss=2.492   test_acc= 0.556\n",
      "epoch= 27   train_loss= 1.927   train_acc= 1.000   test_loss=2.495   test_acc= 0.556\n",
      "epoch= 28   train_loss= 1.912   train_acc= 1.000   test_loss=2.500   test_acc= 0.556\n",
      "epoch= 29   train_loss= 1.897   train_acc= 1.000   test_loss=2.461   test_acc= 0.556\n",
      "epoch= 30   train_loss= 1.886   train_acc= 1.000   test_loss=2.455   test_acc= 0.556\n",
      "epoch= 31   train_loss= 1.872   train_acc= 1.000   test_loss=2.436   test_acc= 0.556\n",
      "epoch= 32   train_loss= 1.867   train_acc= 1.000   test_loss=2.414   test_acc= 0.556\n",
      "epoch= 33   train_loss= 1.849   train_acc= 1.000   test_loss=2.405   test_acc= 0.556\n",
      "epoch= 34   train_loss= 1.837   train_acc= 1.000   test_loss=2.398   test_acc= 0.556\n",
      "epoch= 35   train_loss= 1.831   train_acc= 1.000   test_loss=2.395   test_acc= 0.556\n",
      "epoch= 36   train_loss= 1.814   train_acc= 1.000   test_loss=2.385   test_acc= 0.556\n",
      "epoch= 37   train_loss= 1.807   train_acc= 1.000   test_loss=2.366   test_acc= 0.556\n",
      "epoch= 38   train_loss= 1.794   train_acc= 1.000   test_loss=2.360   test_acc= 0.556\n",
      "epoch= 39   train_loss= 1.779   train_acc= 1.000   test_loss=2.340   test_acc= 0.556\n",
      "epoch= 40   train_loss= 1.774   train_acc= 1.000   test_loss=2.338   test_acc= 0.556\n",
      "epoch= 41   train_loss= 1.760   train_acc= 1.000   test_loss=2.337   test_acc= 0.556\n",
      "epoch= 42   train_loss= 1.749   train_acc= 1.000   test_loss=2.323   test_acc= 0.556\n",
      "epoch= 43   train_loss= 1.739   train_acc= 1.000   test_loss=2.310   test_acc= 0.556\n",
      "epoch= 44   train_loss= 1.725   train_acc= 1.000   test_loss=2.295   test_acc= 0.556\n",
      "epoch= 45   train_loss= 1.718   train_acc= 1.000   test_loss=2.278   test_acc= 0.556\n",
      "epoch= 46   train_loss= 1.709   train_acc= 1.000   test_loss=2.270   test_acc= 0.556\n",
      "epoch= 47   train_loss= 1.698   train_acc= 1.000   test_loss=2.253   test_acc= 0.556\n",
      "epoch= 48   train_loss= 1.686   train_acc= 1.000   test_loss=2.257   test_acc= 0.556\n",
      "epoch= 49   train_loss= 1.677   train_acc= 1.000   test_loss=2.232   test_acc= 0.556\n",
      "run time: 0.7106786648432414 min\n",
      "test_acc=0.556\n",
      "run= 4   fold= 0\n",
      "epoch= 0   train_loss= 2.929   train_acc= 0.646   test_loss=2.666   test_acc= 1.000\n",
      "epoch= 1   train_loss= 2.611   train_acc= 0.854   test_loss=2.552   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.494   train_acc= 0.939   test_loss=2.461   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.439   train_acc= 0.939   test_loss=2.419   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.363   train_acc= 0.963   test_loss=2.327   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.310   train_acc= 0.988   test_loss=2.389   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.306   train_acc= 0.963   test_loss=2.377   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.256   train_acc= 0.988   test_loss=2.249   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.232   train_acc= 0.988   test_loss=2.240   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.189   train_acc= 1.000   test_loss=2.201   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.168   train_acc= 1.000   test_loss=2.198   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.150   train_acc= 1.000   test_loss=2.163   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.143   train_acc= 0.988   test_loss=2.163   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.123   train_acc= 1.000   test_loss=2.137   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.095   train_acc= 1.000   test_loss=2.122   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.083   train_acc= 1.000   test_loss=2.099   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 16   train_loss= 2.069   train_acc= 1.000   test_loss=2.088   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.056   train_acc= 1.000   test_loss=2.069   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.036   train_acc= 1.000   test_loss=2.061   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.029   train_acc= 1.000   test_loss=2.042   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.010   train_acc= 1.000   test_loss=2.020   test_acc= 1.000\n",
      "epoch= 21   train_loss= 1.997   train_acc= 1.000   test_loss=2.019   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.981   train_acc= 1.000   test_loss=2.003   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.966   train_acc= 1.000   test_loss=1.989   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.956   train_acc= 1.000   test_loss=1.973   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.942   train_acc= 1.000   test_loss=1.962   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.929   train_acc= 1.000   test_loss=1.952   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.920   train_acc= 1.000   test_loss=1.928   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.901   train_acc= 1.000   test_loss=1.917   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.887   train_acc= 1.000   test_loss=1.904   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.877   train_acc= 1.000   test_loss=1.895   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.868   train_acc= 1.000   test_loss=1.878   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.855   train_acc= 1.000   test_loss=1.868   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.842   train_acc= 1.000   test_loss=1.861   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.830   train_acc= 1.000   test_loss=1.851   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.819   train_acc= 1.000   test_loss=1.836   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.810   train_acc= 1.000   test_loss=1.828   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.797   train_acc= 1.000   test_loss=1.816   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.787   train_acc= 1.000   test_loss=1.805   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.776   train_acc= 1.000   test_loss=1.794   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.765   train_acc= 1.000   test_loss=1.783   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.752   train_acc= 1.000   test_loss=1.774   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.743   train_acc= 1.000   test_loss=1.765   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.737   train_acc= 1.000   test_loss=1.754   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.725   train_acc= 1.000   test_loss=1.739   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.716   train_acc= 1.000   test_loss=1.730   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.705   train_acc= 1.000   test_loss=1.724   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.696   train_acc= 1.000   test_loss=1.711   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.684   train_acc= 1.000   test_loss=1.705   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.677   train_acc= 1.000   test_loss=1.695   test_acc= 1.000\n",
      "run time: 0.7071350534756978 min\n",
      "test_acc=1.000\n",
      "run= 4   fold= 1\n",
      "epoch= 0   train_loss= 3.012   train_acc= 0.610   test_loss=2.802   test_acc= 0.500\n",
      "epoch= 1   train_loss= 2.640   train_acc= 0.841   test_loss=2.644   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.506   train_acc= 0.963   test_loss=2.623   test_acc= 0.700\n",
      "epoch= 3   train_loss= 2.430   train_acc= 0.939   test_loss=2.517   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.418   train_acc= 0.939   test_loss=2.485   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.325   train_acc= 0.988   test_loss=2.543   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.278   train_acc= 0.988   test_loss=2.494   test_acc= 0.800\n",
      "epoch= 7   train_loss= 2.256   train_acc= 0.988   test_loss=2.450   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.227   train_acc= 1.000   test_loss=2.511   test_acc= 0.600\n",
      "epoch= 9   train_loss= 2.200   train_acc= 1.000   test_loss=2.420   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.197   train_acc= 1.000   test_loss=2.431   test_acc= 0.800\n",
      "epoch= 11   train_loss= 2.166   train_acc= 1.000   test_loss=2.401   test_acc= 0.800\n",
      "epoch= 12   train_loss= 2.144   train_acc= 1.000   test_loss=2.384   test_acc= 0.800\n",
      "epoch= 13   train_loss= 2.117   train_acc= 1.000   test_loss=2.376   test_acc= 0.800\n",
      "epoch= 14   train_loss= 2.112   train_acc= 1.000   test_loss=2.325   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.090   train_acc= 1.000   test_loss=2.308   test_acc= 0.800\n",
      "epoch= 16   train_loss= 2.084   train_acc= 0.988   test_loss=2.275   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.062   train_acc= 1.000   test_loss=2.301   test_acc= 0.800\n",
      "epoch= 18   train_loss= 2.040   train_acc= 1.000   test_loss=2.266   test_acc= 0.800\n",
      "epoch= 19   train_loss= 2.026   train_acc= 1.000   test_loss=2.268   test_acc= 0.800\n",
      "epoch= 20   train_loss= 2.017   train_acc= 1.000   test_loss=2.266   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.000   train_acc= 1.000   test_loss=2.208   test_acc= 0.900\n",
      "epoch= 22   train_loss= 1.987   train_acc= 1.000   test_loss=2.211   test_acc= 0.800\n",
      "epoch= 23   train_loss= 1.974   train_acc= 1.000   test_loss=2.225   test_acc= 0.800\n",
      "epoch= 24   train_loss= 1.956   train_acc= 1.000   test_loss=2.200   test_acc= 0.800\n",
      "epoch= 25   train_loss= 1.944   train_acc= 1.000   test_loss=2.183   test_acc= 0.800\n",
      "epoch= 26   train_loss= 1.934   train_acc= 1.000   test_loss=2.169   test_acc= 0.800\n",
      "epoch= 27   train_loss= 1.918   train_acc= 1.000   test_loss=2.150   test_acc= 0.800\n",
      "epoch= 28   train_loss= 1.908   train_acc= 1.000   test_loss=2.134   test_acc= 0.800\n",
      "epoch= 29   train_loss= 1.905   train_acc= 1.000   test_loss=2.126   test_acc= 0.800\n",
      "epoch= 30   train_loss= 1.884   train_acc= 1.000   test_loss=2.119   test_acc= 0.800\n",
      "epoch= 31   train_loss= 1.871   train_acc= 1.000   test_loss=2.114   test_acc= 0.800\n",
      "epoch= 32   train_loss= 1.858   train_acc= 1.000   test_loss=2.090   test_acc= 0.800\n",
      "epoch= 33   train_loss= 1.853   train_acc= 1.000   test_loss=2.090   test_acc= 0.800\n",
      "epoch= 34   train_loss= 1.834   train_acc= 1.000   test_loss=2.085   test_acc= 0.800\n",
      "epoch= 35   train_loss= 1.825   train_acc= 1.000   test_loss=2.078   test_acc= 0.800\n",
      "epoch= 36   train_loss= 1.813   train_acc= 1.000   test_loss=2.048   test_acc= 0.800\n",
      "epoch= 37   train_loss= 1.801   train_acc= 1.000   test_loss=2.041   test_acc= 0.800\n",
      "epoch= 38   train_loss= 1.792   train_acc= 1.000   test_loss=2.037   test_acc= 0.800\n",
      "epoch= 39   train_loss= 1.782   train_acc= 1.000   test_loss=2.032   test_acc= 0.800\n",
      "epoch= 40   train_loss= 1.778   train_acc= 1.000   test_loss=2.009   test_acc= 0.800\n",
      "epoch= 41   train_loss= 1.758   train_acc= 1.000   test_loss=2.015   test_acc= 0.800\n",
      "epoch= 42   train_loss= 1.752   train_acc= 1.000   test_loss=1.968   test_acc= 0.800\n",
      "epoch= 43   train_loss= 1.737   train_acc= 1.000   test_loss=1.977   test_acc= 0.800\n",
      "epoch= 44   train_loss= 1.730   train_acc= 1.000   test_loss=1.964   test_acc= 0.800\n",
      "epoch= 45   train_loss= 1.721   train_acc= 1.000   test_loss=1.954   test_acc= 0.800\n",
      "epoch= 46   train_loss= 1.711   train_acc= 1.000   test_loss=1.941   test_acc= 0.800\n",
      "epoch= 47   train_loss= 1.698   train_acc= 1.000   test_loss=1.940   test_acc= 0.800\n",
      "epoch= 48   train_loss= 1.689   train_acc= 1.000   test_loss=1.928   test_acc= 0.800\n",
      "epoch= 49   train_loss= 1.681   train_acc= 1.000   test_loss=1.917   test_acc= 0.800\n",
      "run time: 0.7102924823760987 min\n",
      "test_acc=0.800\n",
      "run= 4   fold= 2\n",
      "epoch= 0   train_loss= 3.008   train_acc= 0.578   test_loss=2.695   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.602   train_acc= 0.855   test_loss=2.626   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.513   train_acc= 0.867   test_loss=2.559   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.412   train_acc= 0.964   test_loss=2.489   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.355   train_acc= 0.964   test_loss=2.486   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.315   train_acc= 0.988   test_loss=2.448   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.333   train_acc= 0.952   test_loss=2.408   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.278   train_acc= 0.976   test_loss=2.369   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.226   train_acc= 1.000   test_loss=2.379   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.204   train_acc= 1.000   test_loss=2.328   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.191   train_acc= 1.000   test_loss=2.315   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.160   train_acc= 1.000   test_loss=2.298   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 12   train_loss= 2.144   train_acc= 1.000   test_loss=2.273   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.118   train_acc= 1.000   test_loss=2.265   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.105   train_acc= 1.000   test_loss=2.249   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.097   train_acc= 0.988   test_loss=2.211   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.073   train_acc= 1.000   test_loss=2.210   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.066   train_acc= 1.000   test_loss=2.198   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.042   train_acc= 1.000   test_loss=2.167   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.021   train_acc= 1.000   test_loss=2.153   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.018   train_acc= 1.000   test_loss=2.129   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.001   train_acc= 1.000   test_loss=2.106   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.983   train_acc= 1.000   test_loss=2.094   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.967   train_acc= 1.000   test_loss=2.083   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.953   train_acc= 1.000   test_loss=2.067   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.941   train_acc= 1.000   test_loss=2.054   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.933   train_acc= 1.000   test_loss=2.034   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.920   train_acc= 1.000   test_loss=2.027   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.905   train_acc= 1.000   test_loss=2.011   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.893   train_acc= 1.000   test_loss=2.000   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.879   train_acc= 1.000   test_loss=1.985   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.868   train_acc= 1.000   test_loss=1.981   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.856   train_acc= 1.000   test_loss=1.959   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.845   train_acc= 1.000   test_loss=1.974   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.836   train_acc= 1.000   test_loss=1.948   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.823   train_acc= 1.000   test_loss=1.933   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.809   train_acc= 1.000   test_loss=1.926   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.806   train_acc= 1.000   test_loss=1.919   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.790   train_acc= 1.000   test_loss=1.897   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.780   train_acc= 1.000   test_loss=1.888   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.768   train_acc= 1.000   test_loss=1.885   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.755   train_acc= 1.000   test_loss=1.883   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.749   train_acc= 1.000   test_loss=1.853   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.735   train_acc= 1.000   test_loss=1.861   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.724   train_acc= 1.000   test_loss=1.848   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.718   train_acc= 1.000   test_loss=1.827   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.708   train_acc= 1.000   test_loss=1.816   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.695   train_acc= 1.000   test_loss=1.815   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.685   train_acc= 1.000   test_loss=1.806   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.679   train_acc= 1.000   test_loss=1.788   test_acc= 1.000\n",
      "run time: 0.7098656972249349 min\n",
      "test_acc=1.000\n",
      "run= 4   fold= 3\n",
      "epoch= 0   train_loss= 2.927   train_acc= 0.663   test_loss=2.898   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.642   train_acc= 0.831   test_loss=2.726   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.467   train_acc= 0.928   test_loss=2.721   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.419   train_acc= 0.940   test_loss=2.673   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.361   train_acc= 0.964   test_loss=2.601   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.326   train_acc= 0.976   test_loss=2.589   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.281   train_acc= 1.000   test_loss=2.491   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.251   train_acc= 1.000   test_loss=2.495   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.224   train_acc= 1.000   test_loss=2.492   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.187   train_acc= 1.000   test_loss=2.470   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.177   train_acc= 1.000   test_loss=2.455   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.148   train_acc= 1.000   test_loss=2.439   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.160   train_acc= 1.000   test_loss=2.463   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.143   train_acc= 0.976   test_loss=2.446   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.103   train_acc= 1.000   test_loss=2.370   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.091   train_acc= 1.000   test_loss=2.409   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.070   train_acc= 1.000   test_loss=2.369   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.052   train_acc= 1.000   test_loss=2.340   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.036   train_acc= 1.000   test_loss=2.357   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.026   train_acc= 1.000   test_loss=2.323   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.013   train_acc= 1.000   test_loss=2.310   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.997   train_acc= 1.000   test_loss=2.301   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.982   train_acc= 1.000   test_loss=2.256   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.968   train_acc= 1.000   test_loss=2.277   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.952   train_acc= 1.000   test_loss=2.258   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.943   train_acc= 1.000   test_loss=2.281   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.929   train_acc= 1.000   test_loss=2.244   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.915   train_acc= 1.000   test_loss=2.220   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.902   train_acc= 1.000   test_loss=2.212   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.891   train_acc= 1.000   test_loss=2.229   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.878   train_acc= 1.000   test_loss=2.212   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.866   train_acc= 1.000   test_loss=2.195   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.854   train_acc= 1.000   test_loss=2.150   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.842   train_acc= 1.000   test_loss=2.151   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.831   train_acc= 1.000   test_loss=2.144   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.821   train_acc= 1.000   test_loss=2.149   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.811   train_acc= 1.000   test_loss=2.121   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.796   train_acc= 1.000   test_loss=2.139   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.785   train_acc= 1.000   test_loss=2.109   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.777   train_acc= 1.000   test_loss=2.110   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.762   train_acc= 1.000   test_loss=2.097   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.752   train_acc= 1.000   test_loss=2.088   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.744   train_acc= 1.000   test_loss=2.069   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.734   train_acc= 1.000   test_loss=2.059   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.722   train_acc= 1.000   test_loss=2.065   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.712   train_acc= 1.000   test_loss=2.035   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.702   train_acc= 1.000   test_loss=2.032   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.693   train_acc= 1.000   test_loss=2.027   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.685   train_acc= 1.000   test_loss=2.026   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.673   train_acc= 1.000   test_loss=2.016   test_acc= 0.889\n",
      "run time: 0.7318971157073975 min\n",
      "test_acc=0.889\n",
      "run= 4   fold= 4\n",
      "epoch= 0   train_loss= 2.938   train_acc= 0.578   test_loss=2.772   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.659   train_acc= 0.892   test_loss=2.861   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.536   train_acc= 0.892   test_loss=2.783   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.490   train_acc= 0.904   test_loss=2.679   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.412   train_acc= 0.952   test_loss=2.669   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.338   train_acc= 0.964   test_loss=2.695   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.310   train_acc= 0.964   test_loss=2.544   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.263   train_acc= 0.988   test_loss=2.612   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 8   train_loss= 2.228   train_acc= 1.000   test_loss=2.613   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.211   train_acc= 1.000   test_loss=2.586   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.183   train_acc= 1.000   test_loss=2.526   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.173   train_acc= 0.988   test_loss=2.518   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.152   train_acc= 0.988   test_loss=2.496   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.132   train_acc= 0.988   test_loss=2.523   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.113   train_acc= 1.000   test_loss=2.439   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.094   train_acc= 1.000   test_loss=2.420   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.079   train_acc= 1.000   test_loss=2.397   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.068   train_acc= 1.000   test_loss=2.421   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.048   train_acc= 1.000   test_loss=2.431   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.026   train_acc= 1.000   test_loss=2.395   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.007   train_acc= 1.000   test_loss=2.351   test_acc= 0.778\n",
      "epoch= 21   train_loss= 1.998   train_acc= 1.000   test_loss=2.319   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.987   train_acc= 1.000   test_loss=2.299   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.964   train_acc= 1.000   test_loss=2.299   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.958   train_acc= 1.000   test_loss=2.270   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.940   train_acc= 1.000   test_loss=2.256   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.924   train_acc= 1.000   test_loss=2.241   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.916   train_acc= 1.000   test_loss=2.212   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.900   train_acc= 1.000   test_loss=2.213   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.889   train_acc= 1.000   test_loss=2.219   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.879   train_acc= 1.000   test_loss=2.201   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.869   train_acc= 1.000   test_loss=2.225   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.859   train_acc= 1.000   test_loss=2.217   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.850   train_acc= 1.000   test_loss=2.151   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.833   train_acc= 1.000   test_loss=2.130   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.820   train_acc= 1.000   test_loss=2.142   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.811   train_acc= 1.000   test_loss=2.129   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.799   train_acc= 1.000   test_loss=2.122   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.786   train_acc= 1.000   test_loss=2.121   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.772   train_acc= 1.000   test_loss=2.104   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.762   train_acc= 1.000   test_loss=2.086   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.751   train_acc= 1.000   test_loss=2.074   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.742   train_acc= 1.000   test_loss=2.079   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.735   train_acc= 1.000   test_loss=2.077   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.726   train_acc= 1.000   test_loss=2.060   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.718   train_acc= 1.000   test_loss=2.038   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.704   train_acc= 1.000   test_loss=2.030   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.694   train_acc= 1.000   test_loss=1.999   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.684   train_acc= 1.000   test_loss=1.987   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.675   train_acc= 1.000   test_loss=1.991   test_acc= 0.778\n",
      "run time: 0.7289894660313924 min\n",
      "test_acc=0.778\n",
      "run= 4   fold= 5\n",
      "epoch= 0   train_loss= 2.981   train_acc= 0.602   test_loss=2.751   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.691   train_acc= 0.819   test_loss=2.565   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.547   train_acc= 0.880   test_loss=2.476   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.492   train_acc= 0.916   test_loss=2.493   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.403   train_acc= 0.952   test_loss=2.423   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.350   train_acc= 0.964   test_loss=2.394   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.323   train_acc= 0.976   test_loss=2.367   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.274   train_acc= 0.988   test_loss=2.375   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.253   train_acc= 0.976   test_loss=2.347   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.227   train_acc= 0.976   test_loss=2.306   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.216   train_acc= 1.000   test_loss=2.314   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.185   train_acc= 1.000   test_loss=2.301   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.158   train_acc= 1.000   test_loss=2.273   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.132   train_acc= 1.000   test_loss=2.237   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.111   train_acc= 1.000   test_loss=2.213   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.090   train_acc= 1.000   test_loss=2.204   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.080   train_acc= 1.000   test_loss=2.204   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.064   train_acc= 1.000   test_loss=2.189   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.058   train_acc= 0.988   test_loss=2.180   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.036   train_acc= 1.000   test_loss=2.181   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.026   train_acc= 0.988   test_loss=2.156   test_acc= 1.000\n",
      "epoch= 21   train_loss= 1.999   train_acc= 1.000   test_loss=2.128   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.998   train_acc= 1.000   test_loss=2.122   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.979   train_acc= 1.000   test_loss=2.103   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.961   train_acc= 1.000   test_loss=2.106   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.955   train_acc= 1.000   test_loss=2.091   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.933   train_acc= 1.000   test_loss=2.068   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.919   train_acc= 1.000   test_loss=2.055   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.908   train_acc= 1.000   test_loss=2.032   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.904   train_acc= 1.000   test_loss=2.034   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.880   train_acc= 1.000   test_loss=2.018   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.874   train_acc= 1.000   test_loss=2.001   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.861   train_acc= 1.000   test_loss=2.002   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.850   train_acc= 1.000   test_loss=2.005   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.841   train_acc= 1.000   test_loss=1.994   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.828   train_acc= 1.000   test_loss=1.999   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.811   train_acc= 1.000   test_loss=1.983   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.803   train_acc= 1.000   test_loss=1.958   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.791   train_acc= 1.000   test_loss=1.938   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.780   train_acc= 1.000   test_loss=1.923   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.768   train_acc= 1.000   test_loss=1.916   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.759   train_acc= 1.000   test_loss=1.902   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.750   train_acc= 1.000   test_loss=1.899   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.740   train_acc= 1.000   test_loss=1.882   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.725   train_acc= 1.000   test_loss=1.871   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.718   train_acc= 1.000   test_loss=1.859   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.708   train_acc= 1.000   test_loss=1.872   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.698   train_acc= 1.000   test_loss=1.858   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.688   train_acc= 1.000   test_loss=1.846   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.678   train_acc= 1.000   test_loss=1.840   test_acc= 0.889\n",
      "run time: 0.7762822349866231 min\n",
      "test_acc=0.889\n",
      "run= 4   fold= 6\n",
      "epoch= 0   train_loss= 2.898   train_acc= 0.663   test_loss=2.781   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.666   train_acc= 0.795   test_loss=2.676   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.505   train_acc= 0.940   test_loss=2.626   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.440   train_acc= 0.916   test_loss=2.574   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 4   train_loss= 2.384   train_acc= 0.952   test_loss=2.545   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.337   train_acc= 0.952   test_loss=2.534   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.308   train_acc= 0.964   test_loss=2.513   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.265   train_acc= 0.976   test_loss=2.529   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.235   train_acc= 1.000   test_loss=2.491   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.218   train_acc= 0.988   test_loss=2.456   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.200   train_acc= 0.988   test_loss=2.447   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.177   train_acc= 1.000   test_loss=2.441   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.139   train_acc= 1.000   test_loss=2.419   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.120   train_acc= 1.000   test_loss=2.411   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.110   train_acc= 1.000   test_loss=2.407   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.096   train_acc= 1.000   test_loss=2.415   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.077   train_acc= 1.000   test_loss=2.373   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.070   train_acc= 1.000   test_loss=2.367   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.040   train_acc= 1.000   test_loss=2.338   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.033   train_acc= 1.000   test_loss=2.325   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.009   train_acc= 1.000   test_loss=2.315   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.000   train_acc= 1.000   test_loss=2.298   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.986   train_acc= 1.000   test_loss=2.290   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.972   train_acc= 1.000   test_loss=2.272   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.952   train_acc= 1.000   test_loss=2.261   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.944   train_acc= 1.000   test_loss=2.252   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.928   train_acc= 1.000   test_loss=2.239   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.916   train_acc= 1.000   test_loss=2.223   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.907   train_acc= 1.000   test_loss=2.221   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.891   train_acc= 1.000   test_loss=2.203   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.880   train_acc= 1.000   test_loss=2.187   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.868   train_acc= 1.000   test_loss=2.174   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.861   train_acc= 1.000   test_loss=2.183   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.841   train_acc= 1.000   test_loss=2.167   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.837   train_acc= 1.000   test_loss=2.146   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.825   train_acc= 1.000   test_loss=2.136   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.813   train_acc= 1.000   test_loss=2.123   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.800   train_acc= 1.000   test_loss=2.113   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.791   train_acc= 1.000   test_loss=2.113   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.776   train_acc= 1.000   test_loss=2.093   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.766   train_acc= 1.000   test_loss=2.077   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.756   train_acc= 1.000   test_loss=2.069   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.745   train_acc= 1.000   test_loss=2.056   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.738   train_acc= 1.000   test_loss=2.038   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.724   train_acc= 1.000   test_loss=2.031   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.717   train_acc= 1.000   test_loss=2.022   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.705   train_acc= 1.000   test_loss=2.015   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.695   train_acc= 1.000   test_loss=2.006   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.684   train_acc= 1.000   test_loss=1.996   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.674   train_acc= 1.000   test_loss=1.985   test_acc= 0.778\n",
      "run time: 1.1564171195030213 min\n",
      "test_acc=0.778\n",
      "run= 4   fold= 7\n",
      "epoch= 0   train_loss= 2.946   train_acc= 0.663   test_loss=2.796   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.630   train_acc= 0.843   test_loss=2.746   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.503   train_acc= 0.976   test_loss=2.639   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.500   train_acc= 0.940   test_loss=2.574   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.370   train_acc= 0.988   test_loss=2.572   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.346   train_acc= 0.976   test_loss=2.655   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.319   train_acc= 0.976   test_loss=2.503   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.282   train_acc= 0.988   test_loss=2.418   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.253   train_acc= 0.988   test_loss=2.386   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.207   train_acc= 1.000   test_loss=2.371   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.197   train_acc= 0.976   test_loss=2.348   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.172   train_acc= 1.000   test_loss=2.337   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.147   train_acc= 1.000   test_loss=2.328   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.127   train_acc= 1.000   test_loss=2.281   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.127   train_acc= 1.000   test_loss=2.263   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.093   train_acc= 1.000   test_loss=2.283   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.079   train_acc= 1.000   test_loss=2.212   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.069   train_acc= 1.000   test_loss=2.191   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.052   train_acc= 1.000   test_loss=2.208   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.030   train_acc= 1.000   test_loss=2.166   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.025   train_acc= 1.000   test_loss=2.155   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.006   train_acc= 1.000   test_loss=2.141   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.998   train_acc= 1.000   test_loss=2.150   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.978   train_acc= 1.000   test_loss=2.108   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.967   train_acc= 1.000   test_loss=2.104   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.951   train_acc= 1.000   test_loss=2.101   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.937   train_acc= 1.000   test_loss=2.058   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.923   train_acc= 1.000   test_loss=2.042   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.914   train_acc= 1.000   test_loss=2.060   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.900   train_acc= 1.000   test_loss=2.033   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.890   train_acc= 1.000   test_loss=2.015   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.875   train_acc= 1.000   test_loss=2.001   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.864   train_acc= 1.000   test_loss=1.974   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.851   train_acc= 1.000   test_loss=1.983   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.847   train_acc= 1.000   test_loss=1.967   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.830   train_acc= 1.000   test_loss=1.960   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.819   train_acc= 1.000   test_loss=1.942   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.806   train_acc= 1.000   test_loss=1.933   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.794   train_acc= 1.000   test_loss=1.920   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.785   train_acc= 1.000   test_loss=1.906   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.775   train_acc= 1.000   test_loss=1.898   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.764   train_acc= 1.000   test_loss=1.881   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.751   train_acc= 1.000   test_loss=1.873   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.743   train_acc= 1.000   test_loss=1.857   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.732   train_acc= 1.000   test_loss=1.850   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.722   train_acc= 1.000   test_loss=1.840   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.712   train_acc= 1.000   test_loss=1.835   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.701   train_acc= 1.000   test_loss=1.818   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.692   train_acc= 1.000   test_loss=1.805   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.683   train_acc= 1.000   test_loss=1.794   test_acc= 1.000\n",
      "run time: 0.8543475866317749 min\n",
      "test_acc=1.000\n",
      "run= 4   fold= 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0   train_loss= 2.891   train_acc= 0.639   test_loss=2.814   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.579   train_acc= 0.880   test_loss=2.723   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.501   train_acc= 0.904   test_loss=2.789   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.429   train_acc= 0.940   test_loss=2.664   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.367   train_acc= 0.976   test_loss=2.751   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.304   train_acc= 0.988   test_loss=2.647   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.271   train_acc= 0.988   test_loss=2.616   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.261   train_acc= 1.000   test_loss=2.677   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.219   train_acc= 1.000   test_loss=2.674   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.197   train_acc= 1.000   test_loss=2.661   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.183   train_acc= 0.988   test_loss=2.676   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.148   train_acc= 1.000   test_loss=2.655   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.128   train_acc= 1.000   test_loss=2.594   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.111   train_acc= 1.000   test_loss=2.660   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.111   train_acc= 1.000   test_loss=2.612   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.085   train_acc= 1.000   test_loss=2.534   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.073   train_acc= 1.000   test_loss=2.506   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.049   train_acc= 1.000   test_loss=2.523   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.039   train_acc= 1.000   test_loss=2.554   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.021   train_acc= 1.000   test_loss=2.538   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.004   train_acc= 1.000   test_loss=2.516   test_acc= 0.778\n",
      "epoch= 21   train_loss= 1.993   train_acc= 1.000   test_loss=2.511   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.973   train_acc= 1.000   test_loss=2.476   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.965   train_acc= 1.000   test_loss=2.498   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.947   train_acc= 1.000   test_loss=2.460   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.937   train_acc= 1.000   test_loss=2.439   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.932   train_acc= 1.000   test_loss=2.425   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.910   train_acc= 1.000   test_loss=2.435   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.897   train_acc= 1.000   test_loss=2.420   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.885   train_acc= 1.000   test_loss=2.422   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.873   train_acc= 1.000   test_loss=2.421   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.862   train_acc= 1.000   test_loss=2.389   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.850   train_acc= 1.000   test_loss=2.389   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.839   train_acc= 1.000   test_loss=2.401   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.826   train_acc= 1.000   test_loss=2.413   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.813   train_acc= 1.000   test_loss=2.411   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.805   train_acc= 1.000   test_loss=2.385   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.795   train_acc= 1.000   test_loss=2.393   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.782   train_acc= 1.000   test_loss=2.350   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.769   train_acc= 1.000   test_loss=2.338   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.761   train_acc= 1.000   test_loss=2.335   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.748   train_acc= 1.000   test_loss=2.367   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.738   train_acc= 1.000   test_loss=2.304   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.730   train_acc= 1.000   test_loss=2.288   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.720   train_acc= 1.000   test_loss=2.281   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.711   train_acc= 1.000   test_loss=2.254   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.708   train_acc= 1.000   test_loss=2.185   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.689   train_acc= 1.000   test_loss=2.224   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.678   train_acc= 1.000   test_loss=2.213   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.671   train_acc= 1.000   test_loss=2.207   test_acc= 0.778\n",
      "run time: 0.840147598584493 min\n",
      "test_acc=0.778\n",
      "run= 4   fold= 9\n",
      "epoch= 0   train_loss= 2.981   train_acc= 0.651   test_loss=2.789   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.628   train_acc= 0.831   test_loss=2.646   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.517   train_acc= 0.855   test_loss=2.627   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.416   train_acc= 0.952   test_loss=2.572   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.386   train_acc= 0.952   test_loss=2.489   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.301   train_acc= 0.988   test_loss=2.465   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.286   train_acc= 0.988   test_loss=2.393   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.251   train_acc= 0.988   test_loss=2.350   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.238   train_acc= 0.964   test_loss=2.285   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.196   train_acc= 1.000   test_loss=2.300   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.168   train_acc= 0.988   test_loss=2.289   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.144   train_acc= 0.988   test_loss=2.270   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.115   train_acc= 1.000   test_loss=2.242   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.104   train_acc= 1.000   test_loss=2.236   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.086   train_acc= 1.000   test_loss=2.206   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.074   train_acc= 1.000   test_loss=2.205   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.058   train_acc= 1.000   test_loss=2.174   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.043   train_acc= 1.000   test_loss=2.166   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.026   train_acc= 1.000   test_loss=2.131   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.010   train_acc= 1.000   test_loss=2.110   test_acc= 1.000\n",
      "epoch= 20   train_loss= 1.995   train_acc= 1.000   test_loss=2.094   test_acc= 1.000\n",
      "epoch= 21   train_loss= 1.975   train_acc= 1.000   test_loss=2.072   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.970   train_acc= 1.000   test_loss=2.054   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.956   train_acc= 1.000   test_loss=2.041   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.941   train_acc= 1.000   test_loss=2.028   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.931   train_acc= 1.000   test_loss=2.007   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.915   train_acc= 1.000   test_loss=1.996   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.901   train_acc= 1.000   test_loss=1.984   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.891   train_acc= 1.000   test_loss=1.987   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.877   train_acc= 1.000   test_loss=1.969   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.866   train_acc= 1.000   test_loss=1.949   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.854   train_acc= 1.000   test_loss=1.935   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.841   train_acc= 1.000   test_loss=1.920   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.829   train_acc= 1.000   test_loss=1.914   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.818   train_acc= 1.000   test_loss=1.907   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.804   train_acc= 1.000   test_loss=1.889   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.796   train_acc= 1.000   test_loss=1.895   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.789   train_acc= 1.000   test_loss=1.890   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.772   train_acc= 1.000   test_loss=1.870   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.764   train_acc= 1.000   test_loss=1.850   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.751   train_acc= 1.000   test_loss=1.838   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.744   train_acc= 1.000   test_loss=1.823   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.730   train_acc= 1.000   test_loss=1.819   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.724   train_acc= 1.000   test_loss=1.805   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.710   train_acc= 1.000   test_loss=1.790   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.704   train_acc= 1.000   test_loss=1.781   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 46   train_loss= 1.693   train_acc= 1.000   test_loss=1.764   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.682   train_acc= 1.000   test_loss=1.753   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.671   train_acc= 1.000   test_loss=1.750   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.662   train_acc= 1.000   test_loss=1.743   test_acc= 1.000\n",
      "run time: 0.7546258330345154 min\n",
      "test_acc=1.000\n",
      "MI-Net mean accuracy =  0.89066666\n",
      "std =  0.10099162\n"
     ]
    }
   ],
   "source": [
    "# perform five times 10-fold cross-validation experiments\n",
    "run = 5\n",
    "n_folds = 10\n",
    "acc = np.zeros((run, n_folds), dtype=np.float32)\n",
    "for irun in range(run):\n",
    "    dataset = load_dataset('musk1', n_folds)\n",
    "    for ifold in range(n_folds):\n",
    "        print('run=', irun, '  fold=', ifold)\n",
    "        acc[irun][ifold] = MI_Net(dataset[ifold])\n",
    "print('MI-Net mean accuracy = ', np.mean(acc))\n",
    "print('std = ', np.std(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI-net pooling deep supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from random import shuffle\n",
    "import argparse\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Layer, Dropout, average\n",
    "\n",
    "from mil_nets.dataset import load_dataset\n",
    "from mil_nets.layer import Feature_pooling\n",
    "from mil_nets.metrics import bag_accuracy\n",
    "from mil_nets.objectives import bag_loss\n",
    "from mil_nets.utils import convertToBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval(model, test_set):\n",
    "    \"\"\"Evaluate on testing set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training MI-Net with deep supervision model.\n",
    "    test_set : list\n",
    "        A list of testing set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on testing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_test_batch = len(test_set)\n",
    "    test_loss = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    test_acc = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    for ibatch, batch in enumerate(test_set):\n",
    "        result = model.test_on_batch({'input':batch[0].astype(np.float32)}, {'fp1':batch[1].astype(np.float32), 'fp2':batch[1].astype(np.float32), 'fp3':batch[1].astype(np.float32), 'ave':batch[1].astype(np.float32)})\n",
    "        test_loss[ibatch] = result[0]\n",
    "        test_acc[ibatch] = result[-1]\n",
    "    return np.mean(test_loss), np.mean(test_acc)\n",
    "\n",
    "def train_eval(model, train_set):\n",
    "    \"\"\"Evaluate on training set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training MI-Net with deep supervision model.\n",
    "    train_set : list\n",
    "        A list of training set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on traing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_train_batch = len(train_set)\n",
    "    train_loss = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    train_acc = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    shuffle(train_set)\n",
    "    for ibatch, batch in enumerate(train_set):\n",
    "        result = model.train_on_batch({'input':batch[0].astype(np.float32)}, {'fp1':batch[1].astype(np.float32), 'fp2':batch[1].astype(np.float32), 'fp3':batch[1].astype(np.float32), 'ave':batch[1].astype(np.float32)})\n",
    "        train_loss[ibatch] = result[0]\n",
    "        train_acc[ibatch] = result[-1]\n",
    "    return np.mean(train_loss), np.mean(train_acc)\n",
    "\n",
    "def MI_Net_with_DS(dataset):\n",
    "    \"\"\"Train and evaluate on MI-Net with deep supervision.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    dataset : dict\n",
    "        A dictionary contains all dataset information. We split train/test by keys.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_acc : float\n",
    "        Testing accuracy of MI-Net with deep supervision.\n",
    "    \"\"\"\n",
    "    weight_decay=0.005\n",
    "    init_lr=5e-4\n",
    "    pooling_mode='max'\n",
    "    momentum=0.9\n",
    "    max_epoch=50\n",
    "    # load data and convert type\n",
    "    train_bags = dataset['train']\n",
    "    test_bags = dataset['test']\n",
    "\n",
    "    # convert bag to batch\n",
    "    train_set = convertToBatch(train_bags)\n",
    "    test_set = convertToBatch(test_bags)\n",
    "    dimension = train_set[0][0].shape[1]\n",
    "    weight = [1.0, 1.0, 1.0, 0.0]\n",
    "\n",
    "    # data: instance feature, n*d, n = number of training instance\n",
    "    data_input = Input(shape=(dimension,), dtype='float32', name='input')\n",
    "\n",
    "    # fully-connected\n",
    "    fc1 = Dense(256, activation='relu', kernel_regularizer=l2(weight_decay))(data_input)\n",
    "    fc2 = Dense(128, activation='relu', kernel_regularizer=l2(weight_decay))(fc1)\n",
    "    fc3 = Dense(64, activation='relu', kernel_regularizer=l2(weight_decay))(fc2)\n",
    "\n",
    "    # dropout\n",
    "    dropout1 = Dropout(rate=0.5)(fc1)\n",
    "    dropout2 = Dropout(rate=0.5)(fc2)\n",
    "    dropout3 = Dropout(rate=0.5)(fc3)\n",
    "\n",
    "    # features pooling\n",
    "    fp1 = Feature_pooling(output_dim=1, kernel_regularizer=l2(weight_decay), pooling_mode=pooling_mode, name='fp1')(dropout1)\n",
    "    fp2 = Feature_pooling(output_dim=1, kernel_regularizer=l2(weight_decay), pooling_mode=pooling_mode, name='fp2')(dropout2)\n",
    "    fp3 = Feature_pooling(output_dim=1, kernel_regularizer=l2(weight_decay), pooling_mode=pooling_mode, name='fp3')(dropout3)\n",
    "\n",
    "    # score average\n",
    "    mg_ave =average([fp1,fp2,fp3], name='ave')\n",
    "\n",
    "    model = Model(inputs=[data_input], outputs=[fp1, fp2, fp3, mg_ave])\n",
    "    sgd = SGD(lr=init_lr, decay=1e-4, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss={'fp1':bag_loss, 'fp2':bag_loss, 'fp3':bag_loss, 'ave':bag_loss}, loss_weights={'fp1':weight[0], 'fp2':weight[1], 'fp3':weight[2], 'ave':weight[3]}, optimizer=sgd, metrics=[bag_accuracy])\n",
    "\n",
    "    # train model\n",
    "    t1 = time.time()\n",
    "    num_batch = len(train_set)\n",
    "    for epoch in range(max_epoch):\n",
    "        train_loss, train_acc = train_eval(model, train_set)\n",
    "        test_loss, test_acc = test_eval(model, test_set)\n",
    "        print('epoch=', epoch, '  train_loss= {:.3f}'.format(train_loss), '  train_acc= {:.3f}'.format(train_acc), '  test_loss={:.3f}'.format(test_loss), '  test_acc= {:.3f}'.format(test_acc))\n",
    "    t2 = time.time()\n",
    "    print('run time:', (t2-t1) / 60, 'min')\n",
    "    print('test_acc={:.3f}'.format(test_acc))\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run= 0   fold= 0\n",
      "epoch= 0   train_loss= 4.697   train_acc= 0.646   test_loss=3.608   test_acc= 0.700\n",
      "epoch= 1   train_loss= 3.576   train_acc= 0.817   test_loss=3.062   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.949   train_acc= 0.927   test_loss=3.044   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.781   train_acc= 0.939   test_loss=2.858   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.590   train_acc= 0.976   test_loss=2.990   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.591   train_acc= 0.951   test_loss=2.754   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.410   train_acc= 0.988   test_loss=2.781   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.364   train_acc= 1.000   test_loss=2.655   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.309   train_acc= 1.000   test_loss=2.594   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.276   train_acc= 1.000   test_loss=2.758   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.267   train_acc= 1.000   test_loss=2.562   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.248   train_acc= 1.000   test_loss=2.510   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.221   train_acc= 1.000   test_loss=2.505   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.202   train_acc= 1.000   test_loss=2.545   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.175   train_acc= 1.000   test_loss=2.475   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.174   train_acc= 1.000   test_loss=2.530   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.150   train_acc= 1.000   test_loss=2.473   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.133   train_acc= 1.000   test_loss=2.455   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.097   train_acc= 1.000   test_loss=2.461   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.096   train_acc= 1.000   test_loss=2.422   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.085   train_acc= 1.000   test_loss=2.373   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.051   train_acc= 1.000   test_loss=2.401   test_acc= 0.900\n",
      "epoch= 22   train_loss= 2.044   train_acc= 1.000   test_loss=2.372   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.029   train_acc= 1.000   test_loss=2.361   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.017   train_acc= 1.000   test_loss=2.323   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.004   train_acc= 1.000   test_loss=2.323   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.988   train_acc= 1.000   test_loss=2.319   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.977   train_acc= 1.000   test_loss=2.354   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.968   train_acc= 1.000   test_loss=2.282   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.957   train_acc= 1.000   test_loss=2.280   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.939   train_acc= 1.000   test_loss=2.336   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.928   train_acc= 1.000   test_loss=2.261   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.915   train_acc= 1.000   test_loss=2.261   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.904   train_acc= 1.000   test_loss=2.225   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.899   train_acc= 1.000   test_loss=2.192   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.878   train_acc= 1.000   test_loss=2.193   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.870   train_acc= 1.000   test_loss=2.179   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.859   train_acc= 1.000   test_loss=2.164   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.854   train_acc= 1.000   test_loss=2.149   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.833   train_acc= 1.000   test_loss=2.146   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.822   train_acc= 1.000   test_loss=2.166   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.815   train_acc= 1.000   test_loss=2.119   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.805   train_acc= 1.000   test_loss=2.124   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.797   train_acc= 1.000   test_loss=2.129   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.781   train_acc= 1.000   test_loss=2.130   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.770   train_acc= 1.000   test_loss=2.093   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.759   train_acc= 1.000   test_loss=2.073   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.753   train_acc= 1.000   test_loss=2.059   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.750   train_acc= 1.000   test_loss=2.066   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.736   train_acc= 1.000   test_loss=2.020   test_acc= 1.000\n",
      "run time: 1.2767133315404255 min\n",
      "test_acc=1.000\n",
      "run= 0   fold= 1\n",
      "epoch= 0   train_loss= 4.539   train_acc= 0.683   test_loss=3.967   test_acc= 0.600\n",
      "epoch= 1   train_loss= 3.522   train_acc= 0.841   test_loss=3.717   test_acc= 0.800\n",
      "epoch= 2   train_loss= 3.103   train_acc= 0.890   test_loss=3.101   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.799   train_acc= 0.976   test_loss=3.164   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.688   train_acc= 0.976   test_loss=3.300   test_acc= 0.800\n",
      "epoch= 5   train_loss= 2.511   train_acc= 0.988   test_loss=3.111   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.435   train_acc= 0.988   test_loss=2.956   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.364   train_acc= 1.000   test_loss=3.173   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.366   train_acc= 1.000   test_loss=3.005   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.328   train_acc= 1.000   test_loss=3.245   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.288   train_acc= 1.000   test_loss=3.080   test_acc= 0.800\n",
      "epoch= 11   train_loss= 2.245   train_acc= 1.000   test_loss=3.030   test_acc= 0.800\n",
      "epoch= 12   train_loss= 2.251   train_acc= 1.000   test_loss=2.978   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.218   train_acc= 1.000   test_loss=3.008   test_acc= 0.800\n",
      "epoch= 14   train_loss= 2.187   train_acc= 1.000   test_loss=2.881   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.170   train_acc= 1.000   test_loss=2.977   test_acc= 0.800\n",
      "epoch= 16   train_loss= 2.148   train_acc= 1.000   test_loss=2.977   test_acc= 0.800\n",
      "epoch= 17   train_loss= 2.144   train_acc= 1.000   test_loss=2.928   test_acc= 0.800\n",
      "epoch= 18   train_loss= 2.124   train_acc= 1.000   test_loss=2.956   test_acc= 0.800\n",
      "epoch= 19   train_loss= 2.101   train_acc= 1.000   test_loss=2.924   test_acc= 0.800\n",
      "epoch= 20   train_loss= 2.088   train_acc= 1.000   test_loss=2.873   test_acc= 0.800\n",
      "epoch= 21   train_loss= 2.087   train_acc= 1.000   test_loss=2.861   test_acc= 0.800\n",
      "epoch= 22   train_loss= 2.058   train_acc= 1.000   test_loss=2.896   test_acc= 0.800\n",
      "epoch= 23   train_loss= 2.052   train_acc= 1.000   test_loss=3.131   test_acc= 0.800\n",
      "epoch= 24   train_loss= 2.033   train_acc= 1.000   test_loss=2.868   test_acc= 0.800\n",
      "epoch= 25   train_loss= 2.020   train_acc= 1.000   test_loss=2.871   test_acc= 0.800\n",
      "epoch= 26   train_loss= 1.997   train_acc= 1.000   test_loss=2.895   test_acc= 0.800\n",
      "epoch= 27   train_loss= 2.000   train_acc= 1.000   test_loss=2.857   test_acc= 0.800\n",
      "epoch= 28   train_loss= 1.972   train_acc= 1.000   test_loss=2.904   test_acc= 0.800\n",
      "epoch= 29   train_loss= 1.967   train_acc= 1.000   test_loss=2.853   test_acc= 0.800\n",
      "epoch= 30   train_loss= 1.944   train_acc= 1.000   test_loss=2.813   test_acc= 0.800\n",
      "epoch= 31   train_loss= 1.940   train_acc= 1.000   test_loss=2.799   test_acc= 0.800\n",
      "epoch= 32   train_loss= 1.928   train_acc= 1.000   test_loss=2.723   test_acc= 0.800\n",
      "epoch= 33   train_loss= 1.918   train_acc= 1.000   test_loss=2.720   test_acc= 0.800\n",
      "epoch= 34   train_loss= 1.905   train_acc= 1.000   test_loss=2.790   test_acc= 0.800\n",
      "epoch= 35   train_loss= 1.888   train_acc= 1.000   test_loss=2.756   test_acc= 0.800\n",
      "epoch= 36   train_loss= 1.877   train_acc= 1.000   test_loss=2.739   test_acc= 0.800\n",
      "epoch= 37   train_loss= 1.866   train_acc= 1.000   test_loss=2.788   test_acc= 0.800\n",
      "epoch= 38   train_loss= 1.858   train_acc= 1.000   test_loss=2.741   test_acc= 0.800\n",
      "epoch= 39   train_loss= 1.838   train_acc= 1.000   test_loss=2.797   test_acc= 0.800\n",
      "epoch= 40   train_loss= 1.833   train_acc= 1.000   test_loss=2.745   test_acc= 0.800\n",
      "epoch= 41   train_loss= 1.816   train_acc= 1.000   test_loss=2.750   test_acc= 0.800\n",
      "epoch= 42   train_loss= 1.811   train_acc= 1.000   test_loss=2.781   test_acc= 0.800\n",
      "epoch= 43   train_loss= 1.798   train_acc= 1.000   test_loss=2.740   test_acc= 0.800\n",
      "epoch= 44   train_loss= 1.789   train_acc= 1.000   test_loss=2.703   test_acc= 0.800\n",
      "epoch= 45   train_loss= 1.782   train_acc= 1.000   test_loss=2.725   test_acc= 0.800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 46   train_loss= 1.776   train_acc= 1.000   test_loss=2.678   test_acc= 0.800\n",
      "epoch= 47   train_loss= 1.760   train_acc= 1.000   test_loss=2.742   test_acc= 0.800\n",
      "epoch= 48   train_loss= 1.755   train_acc= 1.000   test_loss=2.752   test_acc= 0.800\n",
      "epoch= 49   train_loss= 1.741   train_acc= 1.000   test_loss=2.751   test_acc= 0.800\n",
      "run time: 1.3564526200294496 min\n",
      "test_acc=0.800\n",
      "run= 0   fold= 2\n",
      "epoch= 0   train_loss= 4.495   train_acc= 0.699   test_loss=3.243   test_acc= 0.889\n",
      "epoch= 1   train_loss= 3.333   train_acc= 0.867   test_loss=3.100   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.935   train_acc= 0.952   test_loss=2.785   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.819   train_acc= 0.964   test_loss=2.667   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.698   train_acc= 0.964   test_loss=2.637   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.569   train_acc= 0.976   test_loss=2.508   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.531   train_acc= 0.988   test_loss=2.448   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.500   train_acc= 0.964   test_loss=2.521   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.370   train_acc= 0.988   test_loss=2.393   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.331   train_acc= 1.000   test_loss=2.368   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.273   train_acc= 1.000   test_loss=2.343   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.249   train_acc= 1.000   test_loss=2.305   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.240   train_acc= 1.000   test_loss=2.290   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.216   train_acc= 1.000   test_loss=2.245   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.184   train_acc= 1.000   test_loss=2.259   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.161   train_acc= 1.000   test_loss=2.217   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.137   train_acc= 1.000   test_loss=2.209   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.132   train_acc= 1.000   test_loss=2.176   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.143   train_acc= 1.000   test_loss=2.173   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.099   train_acc= 1.000   test_loss=2.160   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.078   train_acc= 1.000   test_loss=2.133   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.059   train_acc= 1.000   test_loss=2.120   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.041   train_acc= 1.000   test_loss=2.102   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.033   train_acc= 1.000   test_loss=2.095   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.020   train_acc= 1.000   test_loss=2.080   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.000   train_acc= 1.000   test_loss=2.059   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.985   train_acc= 1.000   test_loss=2.048   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.968   train_acc= 1.000   test_loss=2.031   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.964   train_acc= 1.000   test_loss=2.021   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.954   train_acc= 1.000   test_loss=2.009   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.932   train_acc= 1.000   test_loss=1.998   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.920   train_acc= 1.000   test_loss=1.982   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.910   train_acc= 1.000   test_loss=1.967   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.901   train_acc= 1.000   test_loss=1.962   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.891   train_acc= 1.000   test_loss=1.951   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.876   train_acc= 1.000   test_loss=1.937   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.865   train_acc= 1.000   test_loss=1.927   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.850   train_acc= 1.000   test_loss=1.916   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.850   train_acc= 1.000   test_loss=1.904   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.833   train_acc= 1.000   test_loss=1.891   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.817   train_acc= 1.000   test_loss=1.885   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.809   train_acc= 1.000   test_loss=1.875   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.801   train_acc= 1.000   test_loss=1.861   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.786   train_acc= 1.000   test_loss=1.847   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.776   train_acc= 1.000   test_loss=1.835   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.766   train_acc= 1.000   test_loss=1.822   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.756   train_acc= 1.000   test_loss=1.814   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.754   train_acc= 1.000   test_loss=1.807   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.737   train_acc= 1.000   test_loss=1.799   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.725   train_acc= 1.000   test_loss=1.789   test_acc= 1.000\n",
      "run time: 1.2650454004605611 min\n",
      "test_acc=1.000\n",
      "run= 0   fold= 3\n",
      "epoch= 0   train_loss= 4.485   train_acc= 0.675   test_loss=3.813   test_acc= 0.889\n",
      "epoch= 1   train_loss= 3.412   train_acc= 0.831   test_loss=3.373   test_acc= 0.889\n",
      "epoch= 2   train_loss= 3.049   train_acc= 0.928   test_loss=3.109   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.748   train_acc= 0.976   test_loss=2.854   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.635   train_acc= 0.964   test_loss=2.645   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.536   train_acc= 1.000   test_loss=2.711   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.418   train_acc= 1.000   test_loss=2.696   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.436   train_acc= 0.988   test_loss=2.709   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.355   train_acc= 1.000   test_loss=2.696   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.314   train_acc= 1.000   test_loss=2.784   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.312   train_acc= 1.000   test_loss=2.548   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.242   train_acc= 1.000   test_loss=2.524   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.212   train_acc= 1.000   test_loss=2.529   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.217   train_acc= 1.000   test_loss=2.538   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.184   train_acc= 1.000   test_loss=2.547   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.179   train_acc= 1.000   test_loss=2.603   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.176   train_acc= 1.000   test_loss=2.574   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.138   train_acc= 1.000   test_loss=2.490   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.118   train_acc= 1.000   test_loss=2.393   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.094   train_acc= 1.000   test_loss=2.400   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.084   train_acc= 1.000   test_loss=2.380   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.066   train_acc= 1.000   test_loss=2.372   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.051   train_acc= 1.000   test_loss=2.342   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.051   train_acc= 1.000   test_loss=2.286   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.015   train_acc= 1.000   test_loss=2.281   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.017   train_acc= 1.000   test_loss=2.252   test_acc= 1.000\n",
      "epoch= 26   train_loss= 2.009   train_acc= 1.000   test_loss=2.307   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.985   train_acc= 1.000   test_loss=2.254   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.965   train_acc= 1.000   test_loss=2.210   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.950   train_acc= 1.000   test_loss=2.217   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.944   train_acc= 1.000   test_loss=2.208   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.926   train_acc= 1.000   test_loss=2.178   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.910   train_acc= 1.000   test_loss=2.169   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.905   train_acc= 1.000   test_loss=2.158   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.895   train_acc= 1.000   test_loss=2.134   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.895   train_acc= 1.000   test_loss=2.132   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.867   train_acc= 1.000   test_loss=2.121   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.853   train_acc= 1.000   test_loss=2.106   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.840   train_acc= 1.000   test_loss=2.098   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.839   train_acc= 1.000   test_loss=2.091   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.820   train_acc= 1.000   test_loss=2.073   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.812   train_acc= 1.000   test_loss=2.070   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 42   train_loss= 1.805   train_acc= 1.000   test_loss=2.073   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.794   train_acc= 1.000   test_loss=2.052   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.781   train_acc= 1.000   test_loss=2.047   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.776   train_acc= 1.000   test_loss=2.029   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.760   train_acc= 1.000   test_loss=2.007   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.754   train_acc= 1.000   test_loss=2.004   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.743   train_acc= 1.000   test_loss=1.992   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.732   train_acc= 1.000   test_loss=1.968   test_acc= 1.000\n",
      "run time: 1.2091976523399353 min\n",
      "test_acc=1.000\n",
      "run= 0   fold= 4\n",
      "epoch= 0   train_loss= 4.329   train_acc= 0.747   test_loss=4.795   test_acc= 0.333\n",
      "epoch= 1   train_loss= 3.264   train_acc= 0.904   test_loss=4.177   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.806   train_acc= 0.976   test_loss=4.303   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.683   train_acc= 0.964   test_loss=4.477   test_acc= 0.556\n",
      "epoch= 4   train_loss= 2.554   train_acc= 0.964   test_loss=4.412   test_acc= 0.556\n",
      "epoch= 5   train_loss= 2.488   train_acc= 0.988   test_loss=4.886   test_acc= 0.333\n",
      "epoch= 6   train_loss= 2.399   train_acc= 0.988   test_loss=4.696   test_acc= 0.333\n",
      "epoch= 7   train_loss= 2.381   train_acc= 1.000   test_loss=4.455   test_acc= 0.556\n",
      "epoch= 8   train_loss= 2.349   train_acc= 0.988   test_loss=4.519   test_acc= 0.444\n",
      "epoch= 9   train_loss= 2.289   train_acc= 0.988   test_loss=4.840   test_acc= 0.333\n",
      "epoch= 10   train_loss= 2.241   train_acc= 1.000   test_loss=4.802   test_acc= 0.444\n",
      "epoch= 11   train_loss= 2.225   train_acc= 1.000   test_loss=5.216   test_acc= 0.444\n",
      "epoch= 12   train_loss= 2.212   train_acc= 1.000   test_loss=4.756   test_acc= 0.333\n",
      "epoch= 13   train_loss= 2.172   train_acc= 1.000   test_loss=4.822   test_acc= 0.333\n",
      "epoch= 14   train_loss= 2.170   train_acc= 1.000   test_loss=5.047   test_acc= 0.444\n",
      "epoch= 15   train_loss= 2.147   train_acc= 1.000   test_loss=4.832   test_acc= 0.333\n",
      "epoch= 16   train_loss= 2.122   train_acc= 1.000   test_loss=4.963   test_acc= 0.333\n",
      "epoch= 17   train_loss= 2.126   train_acc= 1.000   test_loss=4.749   test_acc= 0.333\n",
      "epoch= 18   train_loss= 2.083   train_acc= 1.000   test_loss=4.771   test_acc= 0.333\n",
      "epoch= 19   train_loss= 2.074   train_acc= 1.000   test_loss=4.686   test_acc= 0.333\n",
      "epoch= 20   train_loss= 2.065   train_acc= 1.000   test_loss=4.790   test_acc= 0.444\n",
      "epoch= 21   train_loss= 2.070   train_acc= 1.000   test_loss=4.822   test_acc= 0.333\n",
      "epoch= 22   train_loss= 2.039   train_acc= 1.000   test_loss=4.833   test_acc= 0.333\n",
      "epoch= 23   train_loss= 2.018   train_acc= 1.000   test_loss=4.779   test_acc= 0.333\n",
      "epoch= 24   train_loss= 2.011   train_acc= 1.000   test_loss=4.740   test_acc= 0.444\n",
      "epoch= 25   train_loss= 1.991   train_acc= 1.000   test_loss=4.626   test_acc= 0.333\n",
      "epoch= 26   train_loss= 1.980   train_acc= 1.000   test_loss=4.899   test_acc= 0.444\n",
      "epoch= 27   train_loss= 1.961   train_acc= 1.000   test_loss=4.791   test_acc= 0.444\n",
      "epoch= 28   train_loss= 1.958   train_acc= 1.000   test_loss=4.901   test_acc= 0.444\n",
      "epoch= 29   train_loss= 1.945   train_acc= 1.000   test_loss=4.868   test_acc= 0.333\n",
      "epoch= 30   train_loss= 1.920   train_acc= 1.000   test_loss=4.844   test_acc= 0.444\n",
      "epoch= 31   train_loss= 1.923   train_acc= 1.000   test_loss=4.750   test_acc= 0.333\n",
      "epoch= 32   train_loss= 1.906   train_acc= 1.000   test_loss=4.757   test_acc= 0.444\n",
      "epoch= 33   train_loss= 1.896   train_acc= 1.000   test_loss=4.540   test_acc= 0.444\n",
      "epoch= 34   train_loss= 1.879   train_acc= 1.000   test_loss=4.621   test_acc= 0.444\n",
      "epoch= 35   train_loss= 1.890   train_acc= 1.000   test_loss=4.482   test_acc= 0.444\n",
      "epoch= 36   train_loss= 1.859   train_acc= 1.000   test_loss=4.626   test_acc= 0.444\n",
      "epoch= 37   train_loss= 1.849   train_acc= 1.000   test_loss=4.684   test_acc= 0.444\n",
      "epoch= 38   train_loss= 1.840   train_acc= 1.000   test_loss=4.843   test_acc= 0.444\n",
      "epoch= 39   train_loss= 1.824   train_acc= 1.000   test_loss=4.757   test_acc= 0.444\n",
      "epoch= 40   train_loss= 1.816   train_acc= 1.000   test_loss=4.552   test_acc= 0.444\n",
      "epoch= 41   train_loss= 1.800   train_acc= 1.000   test_loss=4.532   test_acc= 0.444\n",
      "epoch= 42   train_loss= 1.790   train_acc= 1.000   test_loss=4.542   test_acc= 0.444\n",
      "epoch= 43   train_loss= 1.777   train_acc= 1.000   test_loss=4.623   test_acc= 0.444\n",
      "epoch= 44   train_loss= 1.763   train_acc= 1.000   test_loss=4.533   test_acc= 0.444\n",
      "epoch= 45   train_loss= 1.764   train_acc= 1.000   test_loss=4.547   test_acc= 0.444\n",
      "epoch= 46   train_loss= 1.749   train_acc= 1.000   test_loss=4.574   test_acc= 0.444\n",
      "epoch= 47   train_loss= 1.742   train_acc= 1.000   test_loss=4.560   test_acc= 0.444\n",
      "epoch= 48   train_loss= 1.729   train_acc= 1.000   test_loss=4.539   test_acc= 0.444\n",
      "epoch= 49   train_loss= 1.721   train_acc= 1.000   test_loss=4.518   test_acc= 0.444\n",
      "run time: 1.2331732551256815 min\n",
      "test_acc=0.444\n",
      "run= 0   fold= 5\n",
      "epoch= 0   train_loss= 4.395   train_acc= 0.675   test_loss=3.325   test_acc= 0.889\n",
      "epoch= 1   train_loss= 3.432   train_acc= 0.855   test_loss=3.145   test_acc= 0.889\n",
      "epoch= 2   train_loss= 3.111   train_acc= 0.916   test_loss=3.384   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.783   train_acc= 0.952   test_loss=3.417   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.628   train_acc= 0.976   test_loss=3.295   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.545   train_acc= 0.964   test_loss=3.384   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.504   train_acc= 0.976   test_loss=3.266   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.406   train_acc= 1.000   test_loss=3.204   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.383   train_acc= 0.988   test_loss=3.237   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.339   train_acc= 1.000   test_loss=3.273   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.283   train_acc= 1.000   test_loss=3.374   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.252   train_acc= 1.000   test_loss=3.323   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.222   train_acc= 1.000   test_loss=3.254   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.189   train_acc= 1.000   test_loss=3.232   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.173   train_acc= 1.000   test_loss=3.189   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.167   train_acc= 1.000   test_loss=3.282   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.150   train_acc= 1.000   test_loss=3.246   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.116   train_acc= 1.000   test_loss=3.221   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.104   train_acc= 1.000   test_loss=3.189   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.090   train_acc= 1.000   test_loss=3.165   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.081   train_acc= 1.000   test_loss=3.195   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.053   train_acc= 1.000   test_loss=3.141   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.051   train_acc= 1.000   test_loss=3.158   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.026   train_acc= 1.000   test_loss=3.134   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.025   train_acc= 1.000   test_loss=3.146   test_acc= 0.889\n",
      "epoch= 25   train_loss= 2.001   train_acc= 1.000   test_loss=3.116   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.991   train_acc= 1.000   test_loss=3.106   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.979   train_acc= 1.000   test_loss=3.106   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.964   train_acc= 1.000   test_loss=3.093   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.959   train_acc= 1.000   test_loss=3.111   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.937   train_acc= 1.000   test_loss=3.117   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.925   train_acc= 1.000   test_loss=3.096   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.929   train_acc= 1.000   test_loss=3.060   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.908   train_acc= 1.000   test_loss=3.033   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.904   train_acc= 1.000   test_loss=3.015   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.884   train_acc= 1.000   test_loss=3.025   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.868   train_acc= 1.000   test_loss=3.005   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.856   train_acc= 1.000   test_loss=2.990   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 38   train_loss= 1.842   train_acc= 1.000   test_loss=2.998   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.833   train_acc= 1.000   test_loss=3.003   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.828   train_acc= 1.000   test_loss=2.969   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.818   train_acc= 1.000   test_loss=2.956   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.806   train_acc= 1.000   test_loss=2.971   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.791   train_acc= 1.000   test_loss=2.968   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.780   train_acc= 1.000   test_loss=2.941   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.771   train_acc= 1.000   test_loss=2.940   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.762   train_acc= 1.000   test_loss=2.942   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.750   train_acc= 1.000   test_loss=2.935   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.739   train_acc= 1.000   test_loss=2.934   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.739   train_acc= 1.000   test_loss=2.923   test_acc= 0.889\n",
      "run time: 1.2839973330497743 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 6\n",
      "epoch= 0   train_loss= 4.548   train_acc= 0.663   test_loss=3.607   test_acc= 0.889\n",
      "epoch= 1   train_loss= 3.330   train_acc= 0.843   test_loss=3.221   test_acc= 1.000\n",
      "epoch= 2   train_loss= 3.038   train_acc= 0.964   test_loss=3.117   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.826   train_acc= 0.964   test_loss=2.827   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.727   train_acc= 0.976   test_loss=2.672   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.539   train_acc= 1.000   test_loss=2.574   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.459   train_acc= 0.988   test_loss=2.493   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.367   train_acc= 1.000   test_loss=2.544   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.392   train_acc= 0.988   test_loss=2.501   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.364   train_acc= 1.000   test_loss=2.383   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.313   train_acc= 1.000   test_loss=2.359   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.290   train_acc= 1.000   test_loss=2.332   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.232   train_acc= 1.000   test_loss=2.324   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.248   train_acc= 1.000   test_loss=2.300   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.197   train_acc= 1.000   test_loss=2.258   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.164   train_acc= 1.000   test_loss=2.246   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.151   train_acc= 1.000   test_loss=2.223   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.127   train_acc= 1.000   test_loss=2.191   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.109   train_acc= 1.000   test_loss=2.178   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.109   train_acc= 1.000   test_loss=2.166   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.101   train_acc= 1.000   test_loss=2.176   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.065   train_acc= 1.000   test_loss=2.147   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.061   train_acc= 1.000   test_loss=2.125   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.028   train_acc= 1.000   test_loss=2.099   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.025   train_acc= 1.000   test_loss=2.097   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.015   train_acc= 1.000   test_loss=2.073   test_acc= 1.000\n",
      "epoch= 26   train_loss= 2.003   train_acc= 1.000   test_loss=2.069   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.979   train_acc= 1.000   test_loss=2.059   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.979   train_acc= 1.000   test_loss=2.060   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.975   train_acc= 1.000   test_loss=2.034   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.954   train_acc= 1.000   test_loss=2.015   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.926   train_acc= 1.000   test_loss=2.000   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.914   train_acc= 1.000   test_loss=1.985   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.904   train_acc= 1.000   test_loss=1.976   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.898   train_acc= 1.000   test_loss=1.965   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.890   train_acc= 1.000   test_loss=1.954   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.872   train_acc= 1.000   test_loss=1.928   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.864   train_acc= 1.000   test_loss=1.917   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.851   train_acc= 1.000   test_loss=1.906   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.833   train_acc= 1.000   test_loss=1.900   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.827   train_acc= 1.000   test_loss=1.885   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.825   train_acc= 1.000   test_loss=1.873   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.803   train_acc= 1.000   test_loss=1.866   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.797   train_acc= 1.000   test_loss=1.852   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.789   train_acc= 1.000   test_loss=1.839   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.771   train_acc= 1.000   test_loss=1.828   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.767   train_acc= 1.000   test_loss=1.823   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.747   train_acc= 1.000   test_loss=1.815   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.745   train_acc= 1.000   test_loss=1.800   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.737   train_acc= 1.000   test_loss=1.794   test_acc= 1.000\n",
      "run time: 1.2979853987693786 min\n",
      "test_acc=1.000\n",
      "run= 0   fold= 7\n",
      "epoch= 0   train_loss= 4.370   train_acc= 0.687   test_loss=4.385   test_acc= 0.667\n",
      "epoch= 1   train_loss= 3.079   train_acc= 0.928   test_loss=4.699   test_acc= 0.444\n",
      "epoch= 2   train_loss= 2.855   train_acc= 0.940   test_loss=4.624   test_acc= 0.444\n",
      "epoch= 3   train_loss= 2.635   train_acc= 0.976   test_loss=4.608   test_acc= 0.556\n",
      "epoch= 4   train_loss= 2.554   train_acc= 0.976   test_loss=4.463   test_acc= 0.556\n",
      "epoch= 5   train_loss= 2.450   train_acc= 1.000   test_loss=4.816   test_acc= 0.667\n",
      "epoch= 6   train_loss= 2.424   train_acc= 0.988   test_loss=4.751   test_acc= 0.556\n",
      "epoch= 7   train_loss= 2.333   train_acc= 1.000   test_loss=4.821   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.364   train_acc= 1.000   test_loss=4.659   test_acc= 0.667\n",
      "epoch= 9   train_loss= 2.300   train_acc= 1.000   test_loss=4.700   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.228   train_acc= 1.000   test_loss=4.635   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.205   train_acc= 1.000   test_loss=4.695   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.181   train_acc= 1.000   test_loss=4.622   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.176   train_acc= 1.000   test_loss=4.534   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.165   train_acc= 1.000   test_loss=4.627   test_acc= 0.667\n",
      "epoch= 15   train_loss= 2.129   train_acc= 1.000   test_loss=4.618   test_acc= 0.667\n",
      "epoch= 16   train_loss= 2.136   train_acc= 1.000   test_loss=4.698   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.120   train_acc= 1.000   test_loss=4.664   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.092   train_acc= 1.000   test_loss=4.568   test_acc= 0.667\n",
      "epoch= 19   train_loss= 2.081   train_acc= 1.000   test_loss=4.576   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.059   train_acc= 1.000   test_loss=4.562   test_acc= 0.667\n",
      "epoch= 21   train_loss= 2.043   train_acc= 1.000   test_loss=4.625   test_acc= 0.667\n",
      "epoch= 22   train_loss= 2.028   train_acc= 1.000   test_loss=4.621   test_acc= 0.667\n",
      "epoch= 23   train_loss= 2.016   train_acc= 1.000   test_loss=4.648   test_acc= 0.667\n",
      "epoch= 24   train_loss= 2.005   train_acc= 1.000   test_loss=4.643   test_acc= 0.667\n",
      "epoch= 25   train_loss= 1.993   train_acc= 1.000   test_loss=4.559   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.980   train_acc= 1.000   test_loss=4.508   test_acc= 0.667\n",
      "epoch= 27   train_loss= 1.956   train_acc= 1.000   test_loss=4.584   test_acc= 0.667\n",
      "epoch= 28   train_loss= 1.944   train_acc= 1.000   test_loss=4.460   test_acc= 0.667\n",
      "epoch= 29   train_loss= 1.934   train_acc= 1.000   test_loss=4.462   test_acc= 0.667\n",
      "epoch= 30   train_loss= 1.921   train_acc= 1.000   test_loss=4.485   test_acc= 0.667\n",
      "epoch= 31   train_loss= 1.910   train_acc= 1.000   test_loss=4.505   test_acc= 0.667\n",
      "epoch= 32   train_loss= 1.903   train_acc= 1.000   test_loss=4.426   test_acc= 0.667\n",
      "epoch= 33   train_loss= 1.888   train_acc= 1.000   test_loss=4.366   test_acc= 0.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 34   train_loss= 1.880   train_acc= 1.000   test_loss=4.390   test_acc= 0.667\n",
      "epoch= 35   train_loss= 1.866   train_acc= 1.000   test_loss=4.461   test_acc= 0.667\n",
      "epoch= 36   train_loss= 1.855   train_acc= 1.000   test_loss=4.388   test_acc= 0.667\n",
      "epoch= 37   train_loss= 1.841   train_acc= 1.000   test_loss=4.349   test_acc= 0.667\n",
      "epoch= 38   train_loss= 1.827   train_acc= 1.000   test_loss=4.373   test_acc= 0.667\n",
      "epoch= 39   train_loss= 1.817   train_acc= 1.000   test_loss=4.385   test_acc= 0.667\n",
      "epoch= 40   train_loss= 1.804   train_acc= 1.000   test_loss=4.400   test_acc= 0.667\n",
      "epoch= 41   train_loss= 1.799   train_acc= 1.000   test_loss=4.332   test_acc= 0.667\n",
      "epoch= 42   train_loss= 1.788   train_acc= 1.000   test_loss=4.353   test_acc= 0.667\n",
      "epoch= 43   train_loss= 1.776   train_acc= 1.000   test_loss=4.289   test_acc= 0.667\n",
      "epoch= 44   train_loss= 1.762   train_acc= 1.000   test_loss=4.243   test_acc= 0.667\n",
      "epoch= 45   train_loss= 1.753   train_acc= 1.000   test_loss=4.325   test_acc= 0.667\n",
      "epoch= 46   train_loss= 1.754   train_acc= 1.000   test_loss=4.316   test_acc= 0.667\n",
      "epoch= 47   train_loss= 1.730   train_acc= 1.000   test_loss=4.277   test_acc= 0.667\n",
      "epoch= 48   train_loss= 1.725   train_acc= 1.000   test_loss=4.243   test_acc= 0.667\n",
      "epoch= 49   train_loss= 1.722   train_acc= 1.000   test_loss=4.238   test_acc= 0.667\n",
      "run time: 1.245642650127411 min\n",
      "test_acc=0.667\n",
      "run= 0   fold= 8\n",
      "epoch= 0   train_loss= 4.725   train_acc= 0.687   test_loss=3.669   test_acc= 0.889\n",
      "epoch= 1   train_loss= 3.403   train_acc= 0.892   test_loss=3.058   test_acc= 1.000\n",
      "epoch= 2   train_loss= 3.062   train_acc= 0.940   test_loss=2.906   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.772   train_acc= 0.964   test_loss=2.865   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.675   train_acc= 0.976   test_loss=2.615   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.534   train_acc= 1.000   test_loss=2.638   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.493   train_acc= 0.988   test_loss=2.510   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.464   train_acc= 0.976   test_loss=2.504   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.355   train_acc= 1.000   test_loss=2.499   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.337   train_acc= 1.000   test_loss=2.481   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.288   train_acc= 1.000   test_loss=2.521   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.274   train_acc= 1.000   test_loss=2.438   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.213   train_acc= 1.000   test_loss=2.401   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.212   train_acc= 1.000   test_loss=2.353   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.172   train_acc= 1.000   test_loss=2.340   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.149   train_acc= 1.000   test_loss=2.305   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.150   train_acc= 1.000   test_loss=2.391   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.133   train_acc= 1.000   test_loss=2.293   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.102   train_acc= 1.000   test_loss=2.269   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.113   train_acc= 1.000   test_loss=2.223   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.086   train_acc= 1.000   test_loss=2.239   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.058   train_acc= 1.000   test_loss=2.205   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.050   train_acc= 1.000   test_loss=2.179   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.032   train_acc= 1.000   test_loss=2.195   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.026   train_acc= 1.000   test_loss=2.184   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.011   train_acc= 1.000   test_loss=2.149   test_acc= 1.000\n",
      "epoch= 26   train_loss= 2.009   train_acc= 1.000   test_loss=2.222   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.977   train_acc= 1.000   test_loss=2.138   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.971   train_acc= 1.000   test_loss=2.135   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.949   train_acc= 1.000   test_loss=2.120   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.941   train_acc= 1.000   test_loss=2.090   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.930   train_acc= 1.000   test_loss=2.097   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.915   train_acc= 1.000   test_loss=2.074   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.905   train_acc= 1.000   test_loss=2.063   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.889   train_acc= 1.000   test_loss=2.025   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.882   train_acc= 1.000   test_loss=2.034   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.872   train_acc= 1.000   test_loss=2.051   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.857   train_acc= 1.000   test_loss=2.019   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.838   train_acc= 1.000   test_loss=2.006   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.830   train_acc= 1.000   test_loss=1.982   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.829   train_acc= 1.000   test_loss=1.969   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.817   train_acc= 1.000   test_loss=1.956   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.800   train_acc= 1.000   test_loss=1.964   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.786   train_acc= 1.000   test_loss=1.940   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.784   train_acc= 1.000   test_loss=1.924   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.773   train_acc= 1.000   test_loss=1.947   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.759   train_acc= 1.000   test_loss=1.921   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.753   train_acc= 1.000   test_loss=1.924   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.742   train_acc= 1.000   test_loss=1.895   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.728   train_acc= 1.000   test_loss=1.875   test_acc= 1.000\n",
      "run time: 1.2292876482009887 min\n",
      "test_acc=1.000\n",
      "run= 0   fold= 9\n",
      "epoch= 0   train_loss= 4.157   train_acc= 0.699   test_loss=4.004   test_acc= 0.778\n",
      "epoch= 1   train_loss= 3.381   train_acc= 0.880   test_loss=3.302   test_acc= 1.000\n",
      "epoch= 2   train_loss= 3.015   train_acc= 0.940   test_loss=3.444   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.770   train_acc= 0.952   test_loss=3.433   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.675   train_acc= 0.976   test_loss=3.283   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.617   train_acc= 0.976   test_loss=3.024   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.513   train_acc= 0.964   test_loss=2.823   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.422   train_acc= 1.000   test_loss=2.940   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.391   train_acc= 0.988   test_loss=2.604   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.355   train_acc= 1.000   test_loss=2.541   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.331   train_acc= 1.000   test_loss=2.518   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.244   train_acc= 1.000   test_loss=2.583   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.242   train_acc= 1.000   test_loss=2.468   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.251   train_acc= 1.000   test_loss=2.459   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.210   train_acc= 1.000   test_loss=2.416   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.201   train_acc= 1.000   test_loss=2.347   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.147   train_acc= 1.000   test_loss=2.338   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.147   train_acc= 1.000   test_loss=2.302   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.108   train_acc= 1.000   test_loss=2.298   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.098   train_acc= 1.000   test_loss=2.256   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.073   train_acc= 1.000   test_loss=2.231   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.079   train_acc= 1.000   test_loss=2.249   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.046   train_acc= 1.000   test_loss=2.214   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.047   train_acc= 1.000   test_loss=2.185   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.025   train_acc= 1.000   test_loss=2.169   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.013   train_acc= 1.000   test_loss=2.158   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.991   train_acc= 1.000   test_loss=2.126   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.985   train_acc= 1.000   test_loss=2.140   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.963   train_acc= 1.000   test_loss=2.102   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.951   train_acc= 1.000   test_loss=2.091   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 30   train_loss= 1.942   train_acc= 1.000   test_loss=2.096   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.923   train_acc= 1.000   test_loss=2.061   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.914   train_acc= 1.000   test_loss=2.039   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.899   train_acc= 1.000   test_loss=2.025   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.915   train_acc= 1.000   test_loss=2.025   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.886   train_acc= 1.000   test_loss=2.022   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.867   train_acc= 1.000   test_loss=1.997   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.853   train_acc= 1.000   test_loss=1.992   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.846   train_acc= 1.000   test_loss=1.977   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.829   train_acc= 1.000   test_loss=1.963   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.819   train_acc= 1.000   test_loss=1.950   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.827   train_acc= 1.000   test_loss=1.944   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.804   train_acc= 1.000   test_loss=1.917   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.797   train_acc= 1.000   test_loss=1.913   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.782   train_acc= 1.000   test_loss=1.911   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.771   train_acc= 1.000   test_loss=1.886   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.772   train_acc= 1.000   test_loss=1.892   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.751   train_acc= 1.000   test_loss=1.877   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.758   train_acc= 1.000   test_loss=1.847   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.731   train_acc= 1.000   test_loss=1.846   test_acc= 1.000\n",
      "run time: 1.1974070469538372 min\n",
      "test_acc=1.000\n",
      "run= 1   fold= 0\n",
      "epoch= 0   train_loss= 4.529   train_acc= 0.695   test_loss=3.908   test_acc= 0.700\n",
      "epoch= 1   train_loss= 3.364   train_acc= 0.854   test_loss=3.656   test_acc= 0.900\n",
      "epoch= 2   train_loss= 3.051   train_acc= 0.927   test_loss=3.360   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.793   train_acc= 0.963   test_loss=3.222   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.637   train_acc= 0.976   test_loss=3.102   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.486   train_acc= 0.988   test_loss=2.972   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.415   train_acc= 1.000   test_loss=2.950   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.407   train_acc= 0.988   test_loss=2.899   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.349   train_acc= 1.000   test_loss=2.842   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.332   train_acc= 0.988   test_loss=2.788   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.270   train_acc= 1.000   test_loss=2.795   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.249   train_acc= 1.000   test_loss=2.739   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.215   train_acc= 1.000   test_loss=2.732   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.195   train_acc= 1.000   test_loss=2.688   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.196   train_acc= 1.000   test_loss=2.694   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.165   train_acc= 1.000   test_loss=2.661   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.137   train_acc= 1.000   test_loss=2.665   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.125   train_acc= 1.000   test_loss=2.620   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.110   train_acc= 1.000   test_loss=2.637   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.100   train_acc= 1.000   test_loss=2.583   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.103   train_acc= 1.000   test_loss=2.610   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.081   train_acc= 1.000   test_loss=2.592   test_acc= 0.900\n",
      "epoch= 22   train_loss= 2.051   train_acc= 1.000   test_loss=2.569   test_acc= 0.900\n",
      "epoch= 23   train_loss= 2.036   train_acc= 1.000   test_loss=2.553   test_acc= 0.900\n",
      "epoch= 24   train_loss= 2.020   train_acc= 1.000   test_loss=2.521   test_acc= 0.900\n",
      "epoch= 25   train_loss= 2.014   train_acc= 1.000   test_loss=2.498   test_acc= 0.900\n",
      "epoch= 26   train_loss= 1.991   train_acc= 1.000   test_loss=2.469   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.976   train_acc= 1.000   test_loss=2.457   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.975   train_acc= 1.000   test_loss=2.442   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.953   train_acc= 1.000   test_loss=2.438   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.941   train_acc= 1.000   test_loss=2.431   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.940   train_acc= 1.000   test_loss=2.417   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.923   train_acc= 1.000   test_loss=2.407   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.906   train_acc= 1.000   test_loss=2.399   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.914   train_acc= 1.000   test_loss=2.440   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.888   train_acc= 1.000   test_loss=2.399   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.877   train_acc= 1.000   test_loss=2.383   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.860   train_acc= 1.000   test_loss=2.361   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.846   train_acc= 1.000   test_loss=2.351   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.840   train_acc= 1.000   test_loss=2.354   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.822   train_acc= 1.000   test_loss=2.337   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.815   train_acc= 1.000   test_loss=2.321   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.805   train_acc= 1.000   test_loss=2.314   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.795   train_acc= 1.000   test_loss=2.292   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.781   train_acc= 1.000   test_loss=2.278   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.772   train_acc= 1.000   test_loss=2.265   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.765   train_acc= 1.000   test_loss=2.256   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.753   train_acc= 1.000   test_loss=2.249   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.741   train_acc= 1.000   test_loss=2.249   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.732   train_acc= 1.000   test_loss=2.238   test_acc= 0.900\n",
      "run time: 1.3383696635564168 min\n",
      "test_acc=0.900\n",
      "run= 1   fold= 1\n",
      "epoch= 0   train_loss= 4.329   train_acc= 0.707   test_loss=3.636   test_acc= 0.900\n",
      "epoch= 1   train_loss= 3.236   train_acc= 0.902   test_loss=3.723   test_acc= 0.800\n",
      "epoch= 2   train_loss= 2.888   train_acc= 0.939   test_loss=3.634   test_acc= 0.700\n",
      "epoch= 3   train_loss= 2.796   train_acc= 0.951   test_loss=3.114   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.612   train_acc= 0.976   test_loss=3.459   test_acc= 0.700\n",
      "epoch= 5   train_loss= 2.476   train_acc= 0.988   test_loss=3.668   test_acc= 0.700\n",
      "epoch= 6   train_loss= 2.465   train_acc= 1.000   test_loss=3.355   test_acc= 0.800\n",
      "epoch= 7   train_loss= 2.372   train_acc= 1.000   test_loss=3.312   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.327   train_acc= 0.988   test_loss=3.422   test_acc= 0.700\n",
      "epoch= 9   train_loss= 2.287   train_acc= 1.000   test_loss=3.524   test_acc= 0.700\n",
      "epoch= 10   train_loss= 2.292   train_acc= 1.000   test_loss=3.164   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.314   train_acc= 0.976   test_loss=2.964   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.229   train_acc= 1.000   test_loss=3.237   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.213   train_acc= 1.000   test_loss=3.176   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.179   train_acc= 1.000   test_loss=3.148   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.173   train_acc= 1.000   test_loss=3.186   test_acc= 0.800\n",
      "epoch= 16   train_loss= 2.145   train_acc= 1.000   test_loss=3.002   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.126   train_acc= 1.000   test_loss=3.113   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.118   train_acc= 1.000   test_loss=3.152   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.106   train_acc= 1.000   test_loss=3.030   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.079   train_acc= 1.000   test_loss=3.171   test_acc= 0.800\n",
      "epoch= 21   train_loss= 2.066   train_acc= 1.000   test_loss=3.041   test_acc= 0.900\n",
      "epoch= 22   train_loss= 2.043   train_acc= 1.000   test_loss=2.970   test_acc= 0.900\n",
      "epoch= 23   train_loss= 2.048   train_acc= 1.000   test_loss=2.957   test_acc= 0.900\n",
      "epoch= 24   train_loss= 2.024   train_acc= 1.000   test_loss=2.954   test_acc= 0.900\n",
      "epoch= 25   train_loss= 2.013   train_acc= 1.000   test_loss=2.936   test_acc= 0.900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 26   train_loss= 2.003   train_acc= 1.000   test_loss=2.980   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.988   train_acc= 1.000   test_loss=2.952   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.981   train_acc= 1.000   test_loss=2.922   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.966   train_acc= 1.000   test_loss=2.920   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.953   train_acc= 1.000   test_loss=2.930   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.932   train_acc= 1.000   test_loss=2.893   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.921   train_acc= 1.000   test_loss=2.801   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.918   train_acc= 1.000   test_loss=2.892   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.910   train_acc= 1.000   test_loss=2.730   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.883   train_acc= 1.000   test_loss=2.799   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.869   train_acc= 1.000   test_loss=2.788   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.871   train_acc= 1.000   test_loss=2.811   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.853   train_acc= 1.000   test_loss=2.769   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.839   train_acc= 1.000   test_loss=2.737   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.840   train_acc= 1.000   test_loss=2.744   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.823   train_acc= 1.000   test_loss=2.712   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.808   train_acc= 1.000   test_loss=2.718   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.796   train_acc= 1.000   test_loss=2.723   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.782   train_acc= 1.000   test_loss=2.723   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.780   train_acc= 1.000   test_loss=2.666   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.764   train_acc= 1.000   test_loss=2.662   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.756   train_acc= 1.000   test_loss=2.698   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.746   train_acc= 1.000   test_loss=2.648   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.737   train_acc= 1.000   test_loss=2.635   test_acc= 0.900\n",
      "run time: 1.2372050364812215 min\n",
      "test_acc=0.900\n",
      "run= 1   fold= 2\n",
      "epoch= 0   train_loss= 4.438   train_acc= 0.651   test_loss=3.838   test_acc= 0.778\n",
      "epoch= 1   train_loss= 3.139   train_acc= 0.940   test_loss=3.447   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.878   train_acc= 0.952   test_loss=3.232   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.688   train_acc= 0.988   test_loss=3.296   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.540   train_acc= 0.988   test_loss=3.476   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.526   train_acc= 0.988   test_loss=3.112   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.457   train_acc= 0.988   test_loss=3.201   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.365   train_acc= 1.000   test_loss=3.399   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.328   train_acc= 1.000   test_loss=3.269   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.289   train_acc= 1.000   test_loss=3.221   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.271   train_acc= 1.000   test_loss=3.120   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.232   train_acc= 1.000   test_loss=3.189   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.215   train_acc= 1.000   test_loss=3.046   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.182   train_acc= 1.000   test_loss=3.064   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.203   train_acc= 1.000   test_loss=3.087   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.233   train_acc= 1.000   test_loss=3.026   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.140   train_acc= 1.000   test_loss=2.954   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.131   train_acc= 1.000   test_loss=3.011   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.120   train_acc= 1.000   test_loss=3.078   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.094   train_acc= 1.000   test_loss=2.965   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.111   train_acc= 1.000   test_loss=2.981   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.061   train_acc= 1.000   test_loss=2.999   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.051   train_acc= 1.000   test_loss=2.967   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.030   train_acc= 1.000   test_loss=2.946   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.025   train_acc= 1.000   test_loss=2.955   test_acc= 0.889\n",
      "epoch= 25   train_loss= 2.002   train_acc= 1.000   test_loss=2.937   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.991   train_acc= 1.000   test_loss=2.935   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.980   train_acc= 1.000   test_loss=2.955   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.966   train_acc= 1.000   test_loss=2.925   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.956   train_acc= 1.000   test_loss=2.880   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.938   train_acc= 1.000   test_loss=2.904   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.927   train_acc= 1.000   test_loss=2.890   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.922   train_acc= 1.000   test_loss=2.833   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.910   train_acc= 1.000   test_loss=2.849   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.889   train_acc= 1.000   test_loss=2.826   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.879   train_acc= 1.000   test_loss=2.826   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.863   train_acc= 1.000   test_loss=2.794   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.863   train_acc= 1.000   test_loss=2.872   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.843   train_acc= 1.000   test_loss=2.820   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.835   train_acc= 1.000   test_loss=2.828   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.825   train_acc= 1.000   test_loss=2.806   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.817   train_acc= 1.000   test_loss=2.796   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.801   train_acc= 1.000   test_loss=2.756   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.796   train_acc= 1.000   test_loss=2.767   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.784   train_acc= 1.000   test_loss=2.778   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.782   train_acc= 1.000   test_loss=2.733   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.766   train_acc= 1.000   test_loss=2.709   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.760   train_acc= 1.000   test_loss=2.723   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.763   train_acc= 1.000   test_loss=2.731   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.732   train_acc= 1.000   test_loss=2.740   test_acc= 0.889\n",
      "run time: 1.2067118485768635 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 3\n",
      "epoch= 0   train_loss= 4.299   train_acc= 0.711   test_loss=4.118   test_acc= 0.667\n",
      "epoch= 1   train_loss= 3.267   train_acc= 0.928   test_loss=3.807   test_acc= 0.667\n",
      "epoch= 2   train_loss= 3.011   train_acc= 0.928   test_loss=4.100   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.808   train_acc= 0.940   test_loss=4.028   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.685   train_acc= 0.976   test_loss=3.972   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.559   train_acc= 0.964   test_loss=3.386   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.455   train_acc= 0.988   test_loss=3.555   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.406   train_acc= 1.000   test_loss=3.798   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.317   train_acc= 1.000   test_loss=3.629   test_acc= 0.667\n",
      "epoch= 9   train_loss= 2.285   train_acc= 1.000   test_loss=3.720   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.244   train_acc= 1.000   test_loss=3.677   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.260   train_acc= 1.000   test_loss=4.006   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.214   train_acc= 1.000   test_loss=3.478   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.191   train_acc= 1.000   test_loss=3.663   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.176   train_acc= 1.000   test_loss=3.672   test_acc= 0.667\n",
      "epoch= 15   train_loss= 2.138   train_acc= 1.000   test_loss=3.463   test_acc= 0.667\n",
      "epoch= 16   train_loss= 2.145   train_acc= 1.000   test_loss=3.592   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.130   train_acc= 1.000   test_loss=3.515   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.102   train_acc= 1.000   test_loss=3.567   test_acc= 0.667\n",
      "epoch= 19   train_loss= 2.090   train_acc= 1.000   test_loss=3.688   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.068   train_acc= 1.000   test_loss=3.477   test_acc= 0.667\n",
      "epoch= 21   train_loss= 2.045   train_acc= 1.000   test_loss=3.507   test_acc= 0.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 22   train_loss= 2.037   train_acc= 1.000   test_loss=3.485   test_acc= 0.667\n",
      "epoch= 23   train_loss= 2.024   train_acc= 1.000   test_loss=3.518   test_acc= 0.667\n",
      "epoch= 24   train_loss= 2.014   train_acc= 1.000   test_loss=3.348   test_acc= 0.667\n",
      "epoch= 25   train_loss= 2.002   train_acc= 1.000   test_loss=3.536   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.985   train_acc= 1.000   test_loss=3.549   test_acc= 0.667\n",
      "epoch= 27   train_loss= 1.972   train_acc= 1.000   test_loss=3.391   test_acc= 0.667\n",
      "epoch= 28   train_loss= 1.958   train_acc= 1.000   test_loss=3.362   test_acc= 0.667\n",
      "epoch= 29   train_loss= 1.938   train_acc= 1.000   test_loss=3.400   test_acc= 0.667\n",
      "epoch= 30   train_loss= 1.932   train_acc= 1.000   test_loss=3.605   test_acc= 0.667\n",
      "epoch= 31   train_loss= 1.927   train_acc= 1.000   test_loss=3.416   test_acc= 0.667\n",
      "epoch= 32   train_loss= 1.908   train_acc= 1.000   test_loss=3.376   test_acc= 0.667\n",
      "epoch= 33   train_loss= 1.896   train_acc= 1.000   test_loss=3.352   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.880   train_acc= 1.000   test_loss=3.244   test_acc= 0.667\n",
      "epoch= 35   train_loss= 1.869   train_acc= 1.000   test_loss=3.234   test_acc= 0.667\n",
      "epoch= 36   train_loss= 1.858   train_acc= 1.000   test_loss=3.249   test_acc= 0.667\n",
      "epoch= 37   train_loss= 1.844   train_acc= 1.000   test_loss=3.272   test_acc= 0.667\n",
      "epoch= 38   train_loss= 1.839   train_acc= 1.000   test_loss=3.279   test_acc= 0.667\n",
      "epoch= 39   train_loss= 1.832   train_acc= 1.000   test_loss=3.254   test_acc= 0.667\n",
      "epoch= 40   train_loss= 1.816   train_acc= 1.000   test_loss=3.317   test_acc= 0.667\n",
      "epoch= 41   train_loss= 1.822   train_acc= 1.000   test_loss=2.977   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.799   train_acc= 1.000   test_loss=3.189   test_acc= 0.667\n",
      "epoch= 43   train_loss= 1.784   train_acc= 1.000   test_loss=3.180   test_acc= 0.667\n",
      "epoch= 44   train_loss= 1.777   train_acc= 1.000   test_loss=3.093   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.765   train_acc= 1.000   test_loss=3.083   test_acc= 0.667\n",
      "epoch= 46   train_loss= 1.750   train_acc= 1.000   test_loss=3.025   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.744   train_acc= 1.000   test_loss=3.127   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.730   train_acc= 1.000   test_loss=3.082   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.728   train_acc= 1.000   test_loss=3.064   test_acc= 0.667\n",
      "run time: 1.2307886481285095 min\n",
      "test_acc=0.667\n",
      "run= 1   fold= 4\n",
      "epoch= 0   train_loss= 4.297   train_acc= 0.687   test_loss=4.934   test_acc= 0.444\n",
      "epoch= 1   train_loss= 3.259   train_acc= 0.940   test_loss=4.200   test_acc= 0.556\n",
      "epoch= 2   train_loss= 2.853   train_acc= 0.952   test_loss=4.000   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.815   train_acc= 0.940   test_loss=4.168   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.711   train_acc= 0.976   test_loss=3.763   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.459   train_acc= 0.988   test_loss=3.530   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.462   train_acc= 0.976   test_loss=3.604   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.356   train_acc= 0.988   test_loss=3.502   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.337   train_acc= 1.000   test_loss=3.425   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.334   train_acc= 0.988   test_loss=3.239   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.274   train_acc= 1.000   test_loss=3.287   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.250   train_acc= 1.000   test_loss=3.225   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.229   train_acc= 1.000   test_loss=3.114   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.221   train_acc= 1.000   test_loss=3.146   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.184   train_acc= 1.000   test_loss=3.218   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.167   train_acc= 1.000   test_loss=3.105   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.140   train_acc= 1.000   test_loss=3.128   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.118   train_acc= 1.000   test_loss=3.101   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.098   train_acc= 1.000   test_loss=3.141   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.104   train_acc= 1.000   test_loss=2.997   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.073   train_acc= 1.000   test_loss=2.997   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.073   train_acc= 1.000   test_loss=2.985   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.048   train_acc= 1.000   test_loss=3.016   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.024   train_acc= 1.000   test_loss=3.005   test_acc= 0.778\n",
      "epoch= 24   train_loss= 2.012   train_acc= 1.000   test_loss=3.010   test_acc= 0.778\n",
      "epoch= 25   train_loss= 2.003   train_acc= 1.000   test_loss=3.031   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.981   train_acc= 1.000   test_loss=2.975   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.987   train_acc= 1.000   test_loss=2.933   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.960   train_acc= 1.000   test_loss=2.926   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.945   train_acc= 1.000   test_loss=2.887   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.932   train_acc= 1.000   test_loss=2.875   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.924   train_acc= 1.000   test_loss=2.861   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.908   train_acc= 1.000   test_loss=2.853   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.895   train_acc= 1.000   test_loss=2.801   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.891   train_acc= 1.000   test_loss=2.808   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.880   train_acc= 1.000   test_loss=2.747   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.868   train_acc= 1.000   test_loss=2.782   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.855   train_acc= 1.000   test_loss=2.785   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.835   train_acc= 1.000   test_loss=2.764   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.826   train_acc= 1.000   test_loss=2.775   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.824   train_acc= 1.000   test_loss=2.716   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.808   train_acc= 1.000   test_loss=2.701   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.803   train_acc= 1.000   test_loss=2.655   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.794   train_acc= 1.000   test_loss=2.632   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.778   train_acc= 1.000   test_loss=2.637   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.767   train_acc= 1.000   test_loss=2.655   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.749   train_acc= 1.000   test_loss=2.650   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.747   train_acc= 1.000   test_loss=2.620   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.733   train_acc= 1.000   test_loss=2.608   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.727   train_acc= 1.000   test_loss=2.607   test_acc= 0.778\n",
      "run time: 1.2197558482487996 min\n",
      "test_acc=0.778\n",
      "run= 1   fold= 5\n",
      "epoch= 0   train_loss= 4.747   train_acc= 0.602   test_loss=3.468   test_acc= 1.000\n",
      "epoch= 1   train_loss= 3.165   train_acc= 0.904   test_loss=3.359   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.948   train_acc= 0.940   test_loss=3.229   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.829   train_acc= 0.940   test_loss=2.979   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.560   train_acc= 0.976   test_loss=2.868   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.528   train_acc= 0.988   test_loss=2.827   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.457   train_acc= 0.976   test_loss=2.814   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.415   train_acc= 1.000   test_loss=2.845   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.416   train_acc= 0.988   test_loss=2.797   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.308   train_acc= 1.000   test_loss=2.785   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.274   train_acc= 1.000   test_loss=2.690   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.245   train_acc= 1.000   test_loss=2.679   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.233   train_acc= 1.000   test_loss=2.655   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.224   train_acc= 1.000   test_loss=2.626   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.179   train_acc= 1.000   test_loss=2.602   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.161   train_acc= 1.000   test_loss=2.581   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.135   train_acc= 1.000   test_loss=2.558   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.137   train_acc= 1.000   test_loss=2.524   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 18   train_loss= 2.107   train_acc= 1.000   test_loss=2.522   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.101   train_acc= 1.000   test_loss=2.547   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.079   train_acc= 1.000   test_loss=2.503   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.057   train_acc= 1.000   test_loss=2.498   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.043   train_acc= 1.000   test_loss=2.478   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.038   train_acc= 1.000   test_loss=2.445   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.019   train_acc= 1.000   test_loss=2.420   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.999   train_acc= 1.000   test_loss=2.406   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.993   train_acc= 1.000   test_loss=2.379   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.978   train_acc= 1.000   test_loss=2.367   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.965   train_acc= 1.000   test_loss=2.357   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.961   train_acc= 1.000   test_loss=2.349   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.934   train_acc= 1.000   test_loss=2.340   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.925   train_acc= 1.000   test_loss=2.323   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.929   train_acc= 1.000   test_loss=2.339   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.902   train_acc= 1.000   test_loss=2.323   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.891   train_acc= 1.000   test_loss=2.291   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.885   train_acc= 1.000   test_loss=2.287   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.867   train_acc= 1.000   test_loss=2.279   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.857   train_acc= 1.000   test_loss=2.261   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.842   train_acc= 1.000   test_loss=2.263   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.831   train_acc= 1.000   test_loss=2.244   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.825   train_acc= 1.000   test_loss=2.239   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.812   train_acc= 1.000   test_loss=2.219   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.803   train_acc= 1.000   test_loss=2.206   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.795   train_acc= 1.000   test_loss=2.197   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.784   train_acc= 1.000   test_loss=2.190   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.771   train_acc= 1.000   test_loss=2.192   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.758   train_acc= 1.000   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.748   train_acc= 1.000   test_loss=2.166   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.741   train_acc= 1.000   test_loss=2.160   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.727   train_acc= 1.000   test_loss=2.153   test_acc= 0.889\n",
      "run time: 1.2188777963320414 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 6\n",
      "epoch= 0   train_loss= 4.500   train_acc= 0.723   test_loss=3.849   test_acc= 0.778\n",
      "epoch= 1   train_loss= 3.378   train_acc= 0.855   test_loss=3.698   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.867   train_acc= 0.952   test_loss=3.425   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.756   train_acc= 0.976   test_loss=3.160   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.629   train_acc= 0.976   test_loss=2.948   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.577   train_acc= 0.964   test_loss=2.857   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.447   train_acc= 0.988   test_loss=3.090   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.442   train_acc= 0.988   test_loss=2.899   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.343   train_acc= 1.000   test_loss=2.801   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.311   train_acc= 1.000   test_loss=2.710   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.296   train_acc= 1.000   test_loss=2.727   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.224   train_acc= 1.000   test_loss=2.714   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.201   train_acc= 1.000   test_loss=2.734   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.203   train_acc= 1.000   test_loss=2.681   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.175   train_acc= 1.000   test_loss=2.671   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.147   train_acc= 1.000   test_loss=2.612   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.135   train_acc= 1.000   test_loss=2.628   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.129   train_acc= 1.000   test_loss=2.619   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.101   train_acc= 1.000   test_loss=2.505   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.105   train_acc= 1.000   test_loss=2.543   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.057   train_acc= 1.000   test_loss=2.471   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.061   train_acc= 1.000   test_loss=2.497   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.048   train_acc= 1.000   test_loss=2.441   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.029   train_acc= 1.000   test_loss=2.415   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.028   train_acc= 1.000   test_loss=2.337   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.001   train_acc= 1.000   test_loss=2.304   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.988   train_acc= 1.000   test_loss=2.332   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.975   train_acc= 1.000   test_loss=2.384   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.953   train_acc= 1.000   test_loss=2.365   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.942   train_acc= 1.000   test_loss=2.329   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.937   train_acc= 1.000   test_loss=2.327   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.918   train_acc= 1.000   test_loss=2.321   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.904   train_acc= 1.000   test_loss=2.316   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.895   train_acc= 1.000   test_loss=2.256   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.879   train_acc= 1.000   test_loss=2.251   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.866   train_acc= 1.000   test_loss=2.254   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.867   train_acc= 1.000   test_loss=2.271   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.850   train_acc= 1.000   test_loss=2.192   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.834   train_acc= 1.000   test_loss=2.184   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.827   train_acc= 1.000   test_loss=2.159   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.826   train_acc= 1.000   test_loss=2.209   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.802   train_acc= 1.000   test_loss=2.207   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.791   train_acc= 1.000   test_loss=2.214   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.806   train_acc= 1.000   test_loss=2.194   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.769   train_acc= 1.000   test_loss=2.163   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.770   train_acc= 1.000   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.750   train_acc= 1.000   test_loss=2.150   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.739   train_acc= 1.000   test_loss=2.135   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.727   train_acc= 1.000   test_loss=2.127   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.723   train_acc= 1.000   test_loss=2.145   test_acc= 0.889\n",
      "run time: 1.2237583200136821 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 7\n",
      "epoch= 0   train_loss= 4.448   train_acc= 0.663   test_loss=3.545   test_acc= 1.000\n",
      "epoch= 1   train_loss= 3.317   train_acc= 0.867   test_loss=3.096   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.988   train_acc= 0.952   test_loss=2.971   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.788   train_acc= 0.928   test_loss=2.759   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.606   train_acc= 0.964   test_loss=2.913   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.555   train_acc= 0.988   test_loss=2.608   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.511   train_acc= 0.964   test_loss=2.659   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.471   train_acc= 0.976   test_loss=2.559   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.412   train_acc= 0.976   test_loss=2.549   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.304   train_acc= 1.000   test_loss=2.483   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.275   train_acc= 1.000   test_loss=2.425   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.265   train_acc= 1.000   test_loss=2.479   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.214   train_acc= 1.000   test_loss=2.382   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.216   train_acc= 1.000   test_loss=2.382   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 14   train_loss= 2.164   train_acc= 1.000   test_loss=2.341   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.200   train_acc= 1.000   test_loss=2.374   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.149   train_acc= 1.000   test_loss=2.352   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.136   train_acc= 1.000   test_loss=2.307   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.109   train_acc= 1.000   test_loss=2.289   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.082   train_acc= 1.000   test_loss=2.255   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.075   train_acc= 1.000   test_loss=2.212   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.051   train_acc= 1.000   test_loss=2.196   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.044   train_acc= 1.000   test_loss=2.195   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.051   train_acc= 1.000   test_loss=2.186   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.014   train_acc= 1.000   test_loss=2.176   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.012   train_acc= 1.000   test_loss=2.152   test_acc= 1.000\n",
      "epoch= 26   train_loss= 2.001   train_acc= 1.000   test_loss=2.143   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.977   train_acc= 1.000   test_loss=2.098   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.961   train_acc= 1.000   test_loss=2.078   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.974   train_acc= 1.000   test_loss=2.066   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.938   train_acc= 1.000   test_loss=2.055   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.924   train_acc= 1.000   test_loss=2.043   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.912   train_acc= 1.000   test_loss=2.032   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.899   train_acc= 1.000   test_loss=2.028   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.896   train_acc= 1.000   test_loss=2.036   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.877   train_acc= 1.000   test_loss=2.003   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.868   train_acc= 1.000   test_loss=1.983   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.851   train_acc= 1.000   test_loss=1.969   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.840   train_acc= 1.000   test_loss=1.952   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.828   train_acc= 1.000   test_loss=1.954   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.817   train_acc= 1.000   test_loss=1.933   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.807   train_acc= 1.000   test_loss=1.918   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.797   train_acc= 1.000   test_loss=1.899   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.788   train_acc= 1.000   test_loss=1.903   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.782   train_acc= 1.000   test_loss=1.890   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.772   train_acc= 1.000   test_loss=1.878   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.758   train_acc= 1.000   test_loss=1.872   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.741   train_acc= 1.000   test_loss=1.861   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.741   train_acc= 1.000   test_loss=1.854   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.724   train_acc= 1.000   test_loss=1.848   test_acc= 1.000\n",
      "run time: 1.226418097813924 min\n",
      "test_acc=1.000\n",
      "run= 1   fold= 8\n",
      "epoch= 0   train_loss= 4.467   train_acc= 0.675   test_loss=3.300   test_acc= 1.000\n",
      "epoch= 1   train_loss= 3.317   train_acc= 0.880   test_loss=2.931   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.888   train_acc= 0.976   test_loss=2.671   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.747   train_acc= 0.964   test_loss=2.619   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.671   train_acc= 0.976   test_loss=2.599   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.563   train_acc= 0.988   test_loss=2.581   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.504   train_acc= 0.964   test_loss=2.453   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.372   train_acc= 1.000   test_loss=2.410   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.380   train_acc= 0.988   test_loss=2.379   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.348   train_acc= 1.000   test_loss=2.365   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.296   train_acc= 1.000   test_loss=2.319   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.292   train_acc= 1.000   test_loss=2.333   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.244   train_acc= 1.000   test_loss=2.272   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.199   train_acc= 1.000   test_loss=2.238   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.182   train_acc= 1.000   test_loss=2.225   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.164   train_acc= 1.000   test_loss=2.199   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.162   train_acc= 1.000   test_loss=2.210   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.150   train_acc= 1.000   test_loss=2.189   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.120   train_acc= 1.000   test_loss=2.162   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.093   train_acc= 1.000   test_loss=2.139   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.089   train_acc= 1.000   test_loss=2.118   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.074   train_acc= 1.000   test_loss=2.112   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.072   train_acc= 1.000   test_loss=2.095   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.032   train_acc= 1.000   test_loss=2.078   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.032   train_acc= 1.000   test_loss=2.064   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.069   train_acc= 1.000   test_loss=2.050   test_acc= 1.000\n",
      "epoch= 26   train_loss= 2.004   train_acc= 1.000   test_loss=2.043   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.993   train_acc= 1.000   test_loss=2.033   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.966   train_acc= 1.000   test_loss=2.020   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.959   train_acc= 1.000   test_loss=1.997   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.953   train_acc= 1.000   test_loss=1.977   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.933   train_acc= 1.000   test_loss=1.965   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.927   train_acc= 1.000   test_loss=1.953   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.907   train_acc= 1.000   test_loss=1.942   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.895   train_acc= 1.000   test_loss=1.936   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.888   train_acc= 1.000   test_loss=1.931   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.867   train_acc= 1.000   test_loss=1.920   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.870   train_acc= 1.000   test_loss=1.903   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.847   train_acc= 1.000   test_loss=1.895   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.854   train_acc= 1.000   test_loss=1.884   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.831   train_acc= 1.000   test_loss=1.878   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.829   train_acc= 1.000   test_loss=1.884   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.816   train_acc= 1.000   test_loss=1.869   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.803   train_acc= 1.000   test_loss=1.849   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.789   train_acc= 1.000   test_loss=1.839   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.782   train_acc= 1.000   test_loss=1.822   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.772   train_acc= 1.000   test_loss=1.815   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.765   train_acc= 1.000   test_loss=1.801   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.750   train_acc= 1.000   test_loss=1.788   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.733   train_acc= 1.000   test_loss=1.780   test_acc= 1.000\n",
      "run time: 1.2409356474876403 min\n",
      "test_acc=1.000\n",
      "run= 1   fold= 9\n",
      "epoch= 0   train_loss= 4.513   train_acc= 0.639   test_loss=3.217   test_acc= 1.000\n",
      "epoch= 1   train_loss= 3.385   train_acc= 0.855   test_loss=3.664   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.913   train_acc= 0.952   test_loss=2.850   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.965   train_acc= 0.952   test_loss=2.997   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.693   train_acc= 0.964   test_loss=2.849   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.670   train_acc= 0.940   test_loss=3.079   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.547   train_acc= 0.976   test_loss=2.801   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.412   train_acc= 1.000   test_loss=2.768   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.358   train_acc= 1.000   test_loss=2.695   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.332   train_acc= 1.000   test_loss=2.722   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 10   train_loss= 2.301   train_acc= 1.000   test_loss=2.674   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.280   train_acc= 1.000   test_loss=2.621   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.259   train_acc= 1.000   test_loss=2.710   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.215   train_acc= 1.000   test_loss=2.629   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.210   train_acc= 1.000   test_loss=2.648   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.192   train_acc= 1.000   test_loss=2.651   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.182   train_acc= 1.000   test_loss=2.555   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.138   train_acc= 1.000   test_loss=2.567   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.137   train_acc= 1.000   test_loss=2.520   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.120   train_acc= 1.000   test_loss=2.501   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.108   train_acc= 1.000   test_loss=2.550   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.087   train_acc= 1.000   test_loss=2.506   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.063   train_acc= 1.000   test_loss=2.491   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.070   train_acc= 1.000   test_loss=2.526   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.036   train_acc= 1.000   test_loss=2.511   test_acc= 0.889\n",
      "epoch= 25   train_loss= 2.021   train_acc= 1.000   test_loss=2.499   test_acc= 0.889\n",
      "epoch= 26   train_loss= 2.015   train_acc= 1.000   test_loss=2.478   test_acc= 1.000\n",
      "epoch= 27   train_loss= 2.004   train_acc= 1.000   test_loss=2.488   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.983   train_acc= 1.000   test_loss=2.449   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.967   train_acc= 1.000   test_loss=2.426   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.958   train_acc= 1.000   test_loss=2.423   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.965   train_acc= 1.000   test_loss=2.426   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.939   train_acc= 1.000   test_loss=2.437   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.929   train_acc= 1.000   test_loss=2.434   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.912   train_acc= 1.000   test_loss=2.387   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.906   train_acc= 1.000   test_loss=2.385   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.886   train_acc= 1.000   test_loss=2.358   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.877   train_acc= 1.000   test_loss=2.345   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.863   train_acc= 1.000   test_loss=2.332   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.861   train_acc= 1.000   test_loss=2.336   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.846   train_acc= 1.000   test_loss=2.317   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.830   train_acc= 1.000   test_loss=2.310   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.828   train_acc= 1.000   test_loss=2.334   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.810   train_acc= 1.000   test_loss=2.307   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.809   train_acc= 1.000   test_loss=2.291   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.794   train_acc= 1.000   test_loss=2.276   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.786   train_acc= 1.000   test_loss=2.271   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.766   train_acc= 1.000   test_loss=2.270   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.761   train_acc= 1.000   test_loss=2.243   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.751   train_acc= 1.000   test_loss=2.234   test_acc= 0.889\n",
      "run time: 1.4936880469322205 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 0\n",
      "epoch= 0   train_loss= 4.399   train_acc= 0.659   test_loss=4.114   test_acc= 0.600\n",
      "epoch= 1   train_loss= 3.383   train_acc= 0.866   test_loss=3.147   test_acc= 0.900\n",
      "epoch= 2   train_loss= 3.089   train_acc= 0.915   test_loss=3.418   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.827   train_acc= 0.976   test_loss=3.023   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.708   train_acc= 0.963   test_loss=2.912   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.607   train_acc= 0.963   test_loss=3.104   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.539   train_acc= 0.988   test_loss=2.765   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.432   train_acc= 0.988   test_loss=2.620   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.386   train_acc= 1.000   test_loss=2.693   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.325   train_acc= 0.988   test_loss=2.453   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.274   train_acc= 1.000   test_loss=2.548   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.260   train_acc= 1.000   test_loss=2.512   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.249   train_acc= 1.000   test_loss=2.450   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.236   train_acc= 0.988   test_loss=2.365   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.206   train_acc= 1.000   test_loss=2.499   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.175   train_acc= 1.000   test_loss=2.342   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.141   train_acc= 1.000   test_loss=2.332   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.153   train_acc= 1.000   test_loss=2.327   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.113   train_acc= 1.000   test_loss=2.313   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.102   train_acc= 1.000   test_loss=2.281   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.074   train_acc= 1.000   test_loss=2.254   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.076   train_acc= 1.000   test_loss=2.241   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.048   train_acc= 1.000   test_loss=2.226   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.034   train_acc= 1.000   test_loss=2.194   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.039   train_acc= 1.000   test_loss=2.192   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.002   train_acc= 1.000   test_loss=2.199   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.995   train_acc= 1.000   test_loss=2.161   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.975   train_acc= 1.000   test_loss=2.140   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.970   train_acc= 1.000   test_loss=2.146   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.965   train_acc= 1.000   test_loss=2.107   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.953   train_acc= 1.000   test_loss=2.117   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.935   train_acc= 1.000   test_loss=2.077   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.914   train_acc= 1.000   test_loss=2.054   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.911   train_acc= 1.000   test_loss=2.056   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.899   train_acc= 1.000   test_loss=2.049   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.879   train_acc= 1.000   test_loss=2.031   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.877   train_acc= 1.000   test_loss=2.011   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.857   train_acc= 1.000   test_loss=2.009   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.850   train_acc= 1.000   test_loss=1.995   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.834   train_acc= 1.000   test_loss=1.981   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.824   train_acc= 1.000   test_loss=1.974   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.815   train_acc= 1.000   test_loss=1.961   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.809   train_acc= 1.000   test_loss=1.943   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.793   train_acc= 1.000   test_loss=1.933   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.783   train_acc= 1.000   test_loss=1.912   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.777   train_acc= 1.000   test_loss=1.907   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.768   train_acc= 1.000   test_loss=1.902   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.750   train_acc= 1.000   test_loss=1.890   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.758   train_acc= 1.000   test_loss=1.877   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.738   train_acc= 1.000   test_loss=1.873   test_acc= 1.000\n",
      "run time: 1.2203980684280396 min\n",
      "test_acc=1.000\n",
      "run= 2   fold= 1\n",
      "epoch= 0   train_loss= 4.497   train_acc= 0.646   test_loss=3.990   test_acc= 0.700\n",
      "epoch= 1   train_loss= 3.250   train_acc= 0.890   test_loss=3.415   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.991   train_acc= 0.927   test_loss=3.194   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.741   train_acc= 0.976   test_loss=2.948   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.637   train_acc= 0.939   test_loss=3.117   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.548   train_acc= 0.988   test_loss=3.029   test_acc= 0.900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 6   train_loss= 2.433   train_acc= 0.988   test_loss=2.832   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.368   train_acc= 1.000   test_loss=2.791   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.341   train_acc= 1.000   test_loss=2.763   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.289   train_acc= 1.000   test_loss=2.599   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.311   train_acc= 1.000   test_loss=2.691   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.266   train_acc= 1.000   test_loss=2.665   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.226   train_acc= 1.000   test_loss=2.608   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.221   train_acc= 1.000   test_loss=2.650   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.180   train_acc= 1.000   test_loss=2.575   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.155   train_acc= 1.000   test_loss=2.589   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.138   train_acc= 1.000   test_loss=2.543   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.133   train_acc= 1.000   test_loss=2.509   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.131   train_acc= 1.000   test_loss=2.537   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.103   train_acc= 1.000   test_loss=2.489   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.087   train_acc= 1.000   test_loss=2.429   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.071   train_acc= 1.000   test_loss=2.463   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.062   train_acc= 1.000   test_loss=2.466   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.053   train_acc= 1.000   test_loss=2.450   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.022   train_acc= 1.000   test_loss=2.397   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.015   train_acc= 1.000   test_loss=2.369   test_acc= 1.000\n",
      "epoch= 26   train_loss= 2.003   train_acc= 1.000   test_loss=2.449   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.984   train_acc= 1.000   test_loss=2.374   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.965   train_acc= 1.000   test_loss=2.349   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.957   train_acc= 1.000   test_loss=2.331   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.948   train_acc= 1.000   test_loss=2.291   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.929   train_acc= 1.000   test_loss=2.285   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.914   train_acc= 1.000   test_loss=2.285   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.905   train_acc= 1.000   test_loss=2.263   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.898   train_acc= 1.000   test_loss=2.242   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.880   train_acc= 1.000   test_loss=2.209   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.878   train_acc= 1.000   test_loss=2.243   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.861   train_acc= 1.000   test_loss=2.225   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.852   train_acc= 1.000   test_loss=2.217   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.843   train_acc= 1.000   test_loss=2.180   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.824   train_acc= 1.000   test_loss=2.190   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.820   train_acc= 1.000   test_loss=2.154   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.807   train_acc= 1.000   test_loss=2.180   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.797   train_acc= 1.000   test_loss=2.144   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.781   train_acc= 1.000   test_loss=2.155   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.774   train_acc= 1.000   test_loss=2.157   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.767   train_acc= 1.000   test_loss=2.135   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.755   train_acc= 1.000   test_loss=2.105   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.761   train_acc= 1.000   test_loss=2.170   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.739   train_acc= 1.000   test_loss=2.132   test_acc= 1.000\n",
      "run time: 1.2468512852986653 min\n",
      "test_acc=1.000\n",
      "run= 2   fold= 2\n",
      "epoch= 0   train_loss= 4.446   train_acc= 0.687   test_loss=3.873   test_acc= 0.778\n",
      "epoch= 1   train_loss= 3.455   train_acc= 0.807   test_loss=3.346   test_acc= 0.778\n",
      "epoch= 2   train_loss= 3.160   train_acc= 0.892   test_loss=3.246   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.732   train_acc= 0.988   test_loss=3.026   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.608   train_acc= 0.976   test_loss=2.915   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.538   train_acc= 0.988   test_loss=2.683   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.455   train_acc= 0.988   test_loss=3.134   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.389   train_acc= 0.988   test_loss=2.706   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.350   train_acc= 1.000   test_loss=2.879   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.304   train_acc= 1.000   test_loss=2.700   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.305   train_acc= 0.988   test_loss=2.573   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.266   train_acc= 0.988   test_loss=2.568   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.224   train_acc= 1.000   test_loss=2.724   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.222   train_acc= 1.000   test_loss=2.667   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.193   train_acc= 1.000   test_loss=2.597   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.157   train_acc= 1.000   test_loss=2.588   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.149   train_acc= 1.000   test_loss=2.563   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.128   train_acc= 1.000   test_loss=2.592   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.115   train_acc= 1.000   test_loss=2.550   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.089   train_acc= 1.000   test_loss=2.560   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.074   train_acc= 1.000   test_loss=2.495   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.061   train_acc= 1.000   test_loss=2.514   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.053   train_acc= 1.000   test_loss=2.534   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.042   train_acc= 1.000   test_loss=2.450   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.040   train_acc= 1.000   test_loss=2.396   test_acc= 0.889\n",
      "epoch= 25   train_loss= 2.006   train_acc= 1.000   test_loss=2.412   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.992   train_acc= 1.000   test_loss=2.445   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.979   train_acc= 1.000   test_loss=2.421   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.966   train_acc= 1.000   test_loss=2.417   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.945   train_acc= 1.000   test_loss=2.416   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.944   train_acc= 1.000   test_loss=2.374   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.922   train_acc= 1.000   test_loss=2.350   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.913   train_acc= 1.000   test_loss=2.352   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.904   train_acc= 1.000   test_loss=2.345   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.890   train_acc= 1.000   test_loss=2.311   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.874   train_acc= 1.000   test_loss=2.308   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.867   train_acc= 1.000   test_loss=2.305   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.854   train_acc= 1.000   test_loss=2.333   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.855   train_acc= 1.000   test_loss=2.300   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.836   train_acc= 1.000   test_loss=2.263   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.822   train_acc= 1.000   test_loss=2.268   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.803   train_acc= 1.000   test_loss=2.224   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.799   train_acc= 1.000   test_loss=2.229   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.795   train_acc= 1.000   test_loss=2.203   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.778   train_acc= 1.000   test_loss=2.176   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.770   train_acc= 1.000   test_loss=2.192   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.762   train_acc= 1.000   test_loss=2.208   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.756   train_acc= 1.000   test_loss=2.286   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.739   train_acc= 1.000   test_loss=2.230   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.733   train_acc= 1.000   test_loss=2.168   test_acc= 0.889\n",
      "run time: 1.2014594674110413 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 3\n",
      "epoch= 0   train_loss= 4.502   train_acc= 0.614   test_loss=3.400   test_acc= 0.889\n",
      "epoch= 1   train_loss= 3.270   train_acc= 0.904   test_loss=3.398   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2   train_loss= 2.923   train_acc= 0.952   test_loss=3.297   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.757   train_acc= 0.940   test_loss=3.509   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.647   train_acc= 0.964   test_loss=3.357   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.498   train_acc= 1.000   test_loss=3.110   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.411   train_acc= 1.000   test_loss=3.432   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.460   train_acc= 0.976   test_loss=3.492   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.365   train_acc= 1.000   test_loss=3.074   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.314   train_acc= 1.000   test_loss=3.003   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.292   train_acc= 1.000   test_loss=3.329   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.262   train_acc= 1.000   test_loss=3.100   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.238   train_acc= 1.000   test_loss=3.268   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.205   train_acc= 1.000   test_loss=3.207   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.199   train_acc= 1.000   test_loss=3.185   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.164   train_acc= 1.000   test_loss=3.081   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.147   train_acc= 1.000   test_loss=3.142   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.132   train_acc= 1.000   test_loss=3.099   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.148   train_acc= 1.000   test_loss=3.096   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.096   train_acc= 1.000   test_loss=3.101   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.086   train_acc= 1.000   test_loss=3.116   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.073   train_acc= 1.000   test_loss=3.089   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.061   train_acc= 1.000   test_loss=2.939   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.052   train_acc= 1.000   test_loss=2.929   test_acc= 0.778\n",
      "epoch= 24   train_loss= 2.029   train_acc= 1.000   test_loss=3.002   test_acc= 0.778\n",
      "epoch= 25   train_loss= 2.018   train_acc= 1.000   test_loss=2.961   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.992   train_acc= 1.000   test_loss=2.962   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.986   train_acc= 1.000   test_loss=2.905   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.972   train_acc= 1.000   test_loss=2.953   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.955   train_acc= 1.000   test_loss=2.946   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.940   train_acc= 1.000   test_loss=2.963   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.929   train_acc= 1.000   test_loss=2.934   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.925   train_acc= 1.000   test_loss=2.960   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.906   train_acc= 1.000   test_loss=2.882   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.906   train_acc= 1.000   test_loss=2.885   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.885   train_acc= 1.000   test_loss=2.865   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.879   train_acc= 1.000   test_loss=2.964   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.870   train_acc= 1.000   test_loss=2.769   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.855   train_acc= 1.000   test_loss=2.745   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.839   train_acc= 1.000   test_loss=2.853   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.827   train_acc= 1.000   test_loss=2.835   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.827   train_acc= 1.000   test_loss=2.806   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.805   train_acc= 1.000   test_loss=2.844   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.798   train_acc= 1.000   test_loss=2.793   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.788   train_acc= 1.000   test_loss=2.806   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.777   train_acc= 1.000   test_loss=2.755   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.774   train_acc= 1.000   test_loss=2.774   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.766   train_acc= 1.000   test_loss=2.641   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.745   train_acc= 1.000   test_loss=2.678   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.736   train_acc= 1.000   test_loss=2.625   test_acc= 0.778\n",
      "run time: 1.2126451492309571 min\n",
      "test_acc=0.778\n",
      "run= 2   fold= 4\n",
      "epoch= 0   train_loss= 4.658   train_acc= 0.614   test_loss=3.345   test_acc= 1.000\n",
      "epoch= 1   train_loss= 3.463   train_acc= 0.880   test_loss=3.039   test_acc= 1.000\n",
      "epoch= 2   train_loss= 3.181   train_acc= 0.867   test_loss=2.944   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.820   train_acc= 0.940   test_loss=2.966   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.747   train_acc= 0.976   test_loss=2.811   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.623   train_acc= 0.952   test_loss=2.744   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.457   train_acc= 0.988   test_loss=2.705   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.391   train_acc= 1.000   test_loss=2.622   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.363   train_acc= 1.000   test_loss=2.583   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.288   train_acc= 1.000   test_loss=2.545   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.277   train_acc= 1.000   test_loss=2.568   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.230   train_acc= 1.000   test_loss=2.555   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.196   train_acc= 1.000   test_loss=2.520   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.179   train_acc= 1.000   test_loss=2.507   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.155   train_acc= 1.000   test_loss=2.500   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.151   train_acc= 1.000   test_loss=2.473   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.145   train_acc= 1.000   test_loss=2.458   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.112   train_acc= 1.000   test_loss=2.441   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.115   train_acc= 1.000   test_loss=2.475   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.084   train_acc= 1.000   test_loss=2.402   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.078   train_acc= 1.000   test_loss=2.412   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.041   train_acc= 1.000   test_loss=2.396   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.043   train_acc= 1.000   test_loss=2.348   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.019   train_acc= 1.000   test_loss=2.347   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.004   train_acc= 1.000   test_loss=2.335   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.993   train_acc= 1.000   test_loss=2.320   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.974   train_acc= 1.000   test_loss=2.310   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.980   train_acc= 1.000   test_loss=2.288   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.957   train_acc= 1.000   test_loss=2.273   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.936   train_acc= 1.000   test_loss=2.278   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.925   train_acc= 1.000   test_loss=2.259   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.916   train_acc= 1.000   test_loss=2.260   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.908   train_acc= 1.000   test_loss=2.247   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.884   train_acc= 1.000   test_loss=2.239   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.874   train_acc= 1.000   test_loss=2.222   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.862   train_acc= 1.000   test_loss=2.210   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.852   train_acc= 1.000   test_loss=2.209   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.847   train_acc= 1.000   test_loss=2.198   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.832   train_acc= 1.000   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.824   train_acc= 1.000   test_loss=2.154   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.815   train_acc= 1.000   test_loss=2.158   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.806   train_acc= 1.000   test_loss=2.171   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.793   train_acc= 1.000   test_loss=2.137   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.782   train_acc= 1.000   test_loss=2.163   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.768   train_acc= 1.000   test_loss=2.135   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.752   train_acc= 1.000   test_loss=2.116   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.747   train_acc= 1.000   test_loss=2.106   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.739   train_acc= 1.000   test_loss=2.118   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 48   train_loss= 1.730   train_acc= 1.000   test_loss=2.056   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.727   train_acc= 1.000   test_loss=2.095   test_acc= 0.889\n",
      "run time: 1.216727582613627 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 5\n",
      "epoch= 0   train_loss= 4.389   train_acc= 0.699   test_loss=3.468   test_acc= 0.889\n",
      "epoch= 1   train_loss= 3.408   train_acc= 0.819   test_loss=3.008   test_acc= 1.000\n",
      "epoch= 2   train_loss= 3.033   train_acc= 0.916   test_loss=3.256   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.811   train_acc= 0.952   test_loss=2.911   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.716   train_acc= 0.964   test_loss=2.956   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.552   train_acc= 0.988   test_loss=3.257   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.533   train_acc= 0.964   test_loss=2.867   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.391   train_acc= 1.000   test_loss=3.179   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.356   train_acc= 1.000   test_loss=3.295   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.345   train_acc= 0.988   test_loss=3.185   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.290   train_acc= 1.000   test_loss=2.745   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.251   train_acc= 1.000   test_loss=2.892   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.250   train_acc= 1.000   test_loss=2.890   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.224   train_acc= 1.000   test_loss=2.797   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.178   train_acc= 1.000   test_loss=2.907   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.151   train_acc= 1.000   test_loss=2.710   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.152   train_acc= 1.000   test_loss=2.672   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.109   train_acc= 1.000   test_loss=2.747   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.096   train_acc= 1.000   test_loss=2.702   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.108   train_acc= 1.000   test_loss=2.503   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.081   train_acc= 1.000   test_loss=2.636   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.050   train_acc= 1.000   test_loss=2.623   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.037   train_acc= 1.000   test_loss=2.616   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.026   train_acc= 1.000   test_loss=2.536   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.020   train_acc= 1.000   test_loss=2.579   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.992   train_acc= 1.000   test_loss=2.582   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.988   train_acc= 1.000   test_loss=2.465   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.980   train_acc= 1.000   test_loss=2.447   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.958   train_acc= 1.000   test_loss=2.488   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.953   train_acc= 1.000   test_loss=2.462   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.928   train_acc= 1.000   test_loss=2.447   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.926   train_acc= 1.000   test_loss=2.398   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.906   train_acc= 1.000   test_loss=2.404   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.900   train_acc= 1.000   test_loss=2.514   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.881   train_acc= 1.000   test_loss=2.534   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.868   train_acc= 1.000   test_loss=2.471   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.861   train_acc= 1.000   test_loss=2.452   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.851   train_acc= 1.000   test_loss=2.459   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.842   train_acc= 1.000   test_loss=2.353   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.826   train_acc= 1.000   test_loss=2.336   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.822   train_acc= 1.000   test_loss=2.335   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.800   train_acc= 1.000   test_loss=2.347   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.805   train_acc= 1.000   test_loss=2.411   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.789   train_acc= 1.000   test_loss=2.420   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.773   train_acc= 1.000   test_loss=2.443   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.763   train_acc= 1.000   test_loss=2.398   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.758   train_acc= 1.000   test_loss=2.318   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.751   train_acc= 1.000   test_loss=2.333   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.730   train_acc= 1.000   test_loss=2.306   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.725   train_acc= 1.000   test_loss=2.343   test_acc= 0.889\n",
      "run time: 1.2359670519828796 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 6\n",
      "epoch= 0   train_loss= 4.198   train_acc= 0.699   test_loss=3.118   test_acc= 1.000\n",
      "epoch= 1   train_loss= 3.475   train_acc= 0.880   test_loss=3.378   test_acc= 0.889\n",
      "epoch= 2   train_loss= 3.011   train_acc= 0.916   test_loss=3.448   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.792   train_acc= 0.952   test_loss=3.138   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.708   train_acc= 0.964   test_loss=2.942   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.639   train_acc= 0.952   test_loss=2.679   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.507   train_acc= 0.976   test_loss=2.587   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.403   train_acc= 1.000   test_loss=2.532   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.383   train_acc= 0.988   test_loss=2.778   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.322   train_acc= 1.000   test_loss=2.455   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.326   train_acc= 1.000   test_loss=2.528   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.284   train_acc= 0.988   test_loss=2.434   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.258   train_acc= 1.000   test_loss=2.482   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.207   train_acc= 1.000   test_loss=2.400   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.192   train_acc= 1.000   test_loss=2.388   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.160   train_acc= 1.000   test_loss=2.317   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.134   train_acc= 1.000   test_loss=2.325   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.127   train_acc= 1.000   test_loss=2.301   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.105   train_acc= 1.000   test_loss=2.316   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.089   train_acc= 1.000   test_loss=2.239   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.075   train_acc= 1.000   test_loss=2.276   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.076   train_acc= 1.000   test_loss=2.231   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.053   train_acc= 1.000   test_loss=2.245   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.032   train_acc= 1.000   test_loss=2.194   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.029   train_acc= 1.000   test_loss=2.181   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.015   train_acc= 1.000   test_loss=2.225   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.987   train_acc= 1.000   test_loss=2.167   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.976   train_acc= 1.000   test_loss=2.148   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.979   train_acc= 1.000   test_loss=2.167   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.950   train_acc= 1.000   test_loss=2.125   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.936   train_acc= 1.000   test_loss=2.114   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.926   train_acc= 1.000   test_loss=2.124   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.919   train_acc= 1.000   test_loss=2.103   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.908   train_acc= 1.000   test_loss=2.084   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.893   train_acc= 1.000   test_loss=2.072   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.882   train_acc= 1.000   test_loss=2.055   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.868   train_acc= 1.000   test_loss=2.074   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.852   train_acc= 1.000   test_loss=2.042   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.846   train_acc= 1.000   test_loss=2.031   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.834   train_acc= 1.000   test_loss=2.008   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.819   train_acc= 1.000   test_loss=1.994   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.808   train_acc= 1.000   test_loss=1.983   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.806   train_acc= 1.000   test_loss=2.009   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.790   train_acc= 1.000   test_loss=1.988   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 44   train_loss= 1.776   train_acc= 1.000   test_loss=1.981   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.774   train_acc= 1.000   test_loss=1.975   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.786   train_acc= 1.000   test_loss=2.004   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.760   train_acc= 1.000   test_loss=1.949   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.738   train_acc= 1.000   test_loss=1.920   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.728   train_acc= 1.000   test_loss=1.922   test_acc= 1.000\n",
      "run time: 1.2274794141451517 min\n",
      "test_acc=1.000\n",
      "run= 2   fold= 7\n",
      "epoch= 0   train_loss= 4.606   train_acc= 0.627   test_loss=3.453   test_acc= 1.000\n",
      "epoch= 1   train_loss= 3.459   train_acc= 0.831   test_loss=3.101   test_acc= 0.889\n",
      "epoch= 2   train_loss= 3.016   train_acc= 0.952   test_loss=3.034   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.802   train_acc= 0.976   test_loss=2.881   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.633   train_acc= 0.988   test_loss=2.849   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.519   train_acc= 0.976   test_loss=3.001   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.551   train_acc= 0.976   test_loss=3.079   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.400   train_acc= 1.000   test_loss=2.770   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.353   train_acc= 0.988   test_loss=2.755   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.355   train_acc= 1.000   test_loss=2.692   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.266   train_acc= 1.000   test_loss=2.843   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.273   train_acc= 1.000   test_loss=2.712   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.236   train_acc= 1.000   test_loss=2.710   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.209   train_acc= 1.000   test_loss=2.658   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.212   train_acc= 1.000   test_loss=2.721   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.168   train_acc= 1.000   test_loss=2.681   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.149   train_acc= 1.000   test_loss=2.604   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.123   train_acc= 1.000   test_loss=2.574   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.125   train_acc= 1.000   test_loss=2.663   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.106   train_acc= 1.000   test_loss=2.672   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.089   train_acc= 1.000   test_loss=2.496   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.059   train_acc= 1.000   test_loss=2.488   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.062   train_acc= 1.000   test_loss=2.518   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.026   train_acc= 1.000   test_loss=2.535   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.033   train_acc= 1.000   test_loss=2.483   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.007   train_acc= 1.000   test_loss=2.487   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.991   train_acc= 1.000   test_loss=2.488   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.993   train_acc= 1.000   test_loss=2.471   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.963   train_acc= 1.000   test_loss=2.557   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.947   train_acc= 1.000   test_loss=2.464   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.953   train_acc= 1.000   test_loss=2.396   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.923   train_acc= 1.000   test_loss=2.394   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.918   train_acc= 1.000   test_loss=2.346   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.902   train_acc= 1.000   test_loss=2.350   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.890   train_acc= 1.000   test_loss=2.350   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.877   train_acc= 1.000   test_loss=2.343   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.878   train_acc= 1.000   test_loss=2.328   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.853   train_acc= 1.000   test_loss=2.344   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.845   train_acc= 1.000   test_loss=2.324   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.828   train_acc= 1.000   test_loss=2.336   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.818   train_acc= 1.000   test_loss=2.317   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.816   train_acc= 1.000   test_loss=2.340   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.802   train_acc= 1.000   test_loss=2.297   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.788   train_acc= 1.000   test_loss=2.284   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.778   train_acc= 1.000   test_loss=2.262   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.769   train_acc= 1.000   test_loss=2.240   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.766   train_acc= 1.000   test_loss=2.231   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.761   train_acc= 1.000   test_loss=2.187   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.741   train_acc= 1.000   test_loss=2.208   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.730   train_acc= 1.000   test_loss=2.192   test_acc= 0.889\n",
      "run time: 1.213864270846049 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 8\n",
      "epoch= 0   train_loss= 4.455   train_acc= 0.663   test_loss=3.574   test_acc= 0.889\n",
      "epoch= 1   train_loss= 3.211   train_acc= 0.928   test_loss=3.347   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.851   train_acc= 0.964   test_loss=3.382   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.646   train_acc= 0.976   test_loss=3.358   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.620   train_acc= 0.964   test_loss=3.374   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.523   train_acc= 0.976   test_loss=3.167   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.440   train_acc= 0.988   test_loss=3.312   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.361   train_acc= 1.000   test_loss=3.451   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.324   train_acc= 1.000   test_loss=3.360   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.270   train_acc= 1.000   test_loss=3.201   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.311   train_acc= 0.988   test_loss=3.495   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.257   train_acc= 0.988   test_loss=3.405   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.219   train_acc= 1.000   test_loss=3.323   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.173   train_acc= 1.000   test_loss=3.442   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.141   train_acc= 1.000   test_loss=3.463   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.137   train_acc= 1.000   test_loss=3.367   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.134   train_acc= 1.000   test_loss=3.466   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.106   train_acc= 1.000   test_loss=3.311   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.092   train_acc= 1.000   test_loss=3.332   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.092   train_acc= 1.000   test_loss=3.488   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.055   train_acc= 1.000   test_loss=3.543   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.042   train_acc= 1.000   test_loss=3.401   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.028   train_acc= 1.000   test_loss=3.326   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.034   train_acc= 1.000   test_loss=3.336   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.991   train_acc= 1.000   test_loss=3.358   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.981   train_acc= 1.000   test_loss=3.364   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.974   train_acc= 1.000   test_loss=3.425   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.955   train_acc= 1.000   test_loss=3.399   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.948   train_acc= 1.000   test_loss=3.375   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.931   train_acc= 1.000   test_loss=3.404   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.921   train_acc= 1.000   test_loss=3.340   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.909   train_acc= 1.000   test_loss=3.320   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.892   train_acc= 1.000   test_loss=3.329   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.885   train_acc= 1.000   test_loss=3.401   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.869   train_acc= 1.000   test_loss=3.404   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.860   train_acc= 1.000   test_loss=3.340   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.846   train_acc= 1.000   test_loss=3.368   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.847   train_acc= 1.000   test_loss=3.404   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.824   train_acc= 1.000   test_loss=3.381   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.816   train_acc= 1.000   test_loss=3.342   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 40   train_loss= 1.814   train_acc= 1.000   test_loss=3.258   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.791   train_acc= 1.000   test_loss=3.288   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.789   train_acc= 1.000   test_loss=3.282   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.776   train_acc= 1.000   test_loss=3.245   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.763   train_acc= 1.000   test_loss=3.276   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.752   train_acc= 1.000   test_loss=3.225   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.739   train_acc= 1.000   test_loss=3.258   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.737   train_acc= 1.000   test_loss=3.163   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.727   train_acc= 1.000   test_loss=3.244   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.715   train_acc= 1.000   test_loss=3.223   test_acc= 0.778\n",
      "run time: 1.2224454522132873 min\n",
      "test_acc=0.778\n",
      "run= 2   fold= 9\n",
      "epoch= 0   train_loss= 4.240   train_acc= 0.614   test_loss=4.643   test_acc= 0.444\n",
      "epoch= 1   train_loss= 3.054   train_acc= 0.940   test_loss=4.693   test_acc= 0.444\n",
      "epoch= 2   train_loss= 2.827   train_acc= 0.952   test_loss=4.824   test_acc= 0.444\n",
      "epoch= 3   train_loss= 2.745   train_acc= 0.952   test_loss=4.575   test_acc= 0.444\n",
      "epoch= 4   train_loss= 2.446   train_acc= 1.000   test_loss=4.231   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.420   train_acc= 1.000   test_loss=4.741   test_acc= 0.556\n",
      "epoch= 6   train_loss= 2.353   train_acc= 1.000   test_loss=4.398   test_acc= 0.556\n",
      "epoch= 7   train_loss= 2.331   train_acc= 1.000   test_loss=4.366   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.270   train_acc= 1.000   test_loss=4.208   test_acc= 0.667\n",
      "epoch= 9   train_loss= 2.253   train_acc= 1.000   test_loss=4.277   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.241   train_acc= 1.000   test_loss=4.625   test_acc= 0.556\n",
      "epoch= 11   train_loss= 2.217   train_acc= 1.000   test_loss=4.405   test_acc= 0.556\n",
      "epoch= 12   train_loss= 2.180   train_acc= 1.000   test_loss=4.386   test_acc= 0.556\n",
      "epoch= 13   train_loss= 2.159   train_acc= 1.000   test_loss=4.359   test_acc= 0.556\n",
      "epoch= 14   train_loss= 2.145   train_acc= 1.000   test_loss=4.437   test_acc= 0.556\n",
      "epoch= 15   train_loss= 2.126   train_acc= 1.000   test_loss=4.403   test_acc= 0.667\n",
      "epoch= 16   train_loss= 2.126   train_acc= 1.000   test_loss=4.257   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.094   train_acc= 1.000   test_loss=4.289   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.079   train_acc= 1.000   test_loss=4.235   test_acc= 0.667\n",
      "epoch= 19   train_loss= 2.074   train_acc= 1.000   test_loss=4.375   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.053   train_acc= 1.000   test_loss=4.265   test_acc= 0.667\n",
      "epoch= 21   train_loss= 2.036   train_acc= 1.000   test_loss=4.260   test_acc= 0.556\n",
      "epoch= 22   train_loss= 2.025   train_acc= 1.000   test_loss=4.340   test_acc= 0.556\n",
      "epoch= 23   train_loss= 2.005   train_acc= 1.000   test_loss=4.281   test_acc= 0.667\n",
      "epoch= 24   train_loss= 2.003   train_acc= 1.000   test_loss=4.334   test_acc= 0.667\n",
      "epoch= 25   train_loss= 1.981   train_acc= 1.000   test_loss=4.260   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.969   train_acc= 1.000   test_loss=4.206   test_acc= 0.667\n",
      "epoch= 27   train_loss= 1.954   train_acc= 1.000   test_loss=4.181   test_acc= 0.667\n",
      "epoch= 28   train_loss= 1.939   train_acc= 1.000   test_loss=4.122   test_acc= 0.667\n",
      "epoch= 29   train_loss= 1.927   train_acc= 1.000   test_loss=4.204   test_acc= 0.667\n",
      "epoch= 30   train_loss= 1.913   train_acc= 1.000   test_loss=4.158   test_acc= 0.667\n",
      "epoch= 31   train_loss= 1.907   train_acc= 1.000   test_loss=4.113   test_acc= 0.667\n",
      "epoch= 32   train_loss= 1.890   train_acc= 1.000   test_loss=4.099   test_acc= 0.667\n",
      "epoch= 33   train_loss= 1.882   train_acc= 1.000   test_loss=4.118   test_acc= 0.667\n",
      "epoch= 34   train_loss= 1.869   train_acc= 1.000   test_loss=4.150   test_acc= 0.556\n",
      "epoch= 35   train_loss= 1.858   train_acc= 1.000   test_loss=4.149   test_acc= 0.556\n",
      "epoch= 36   train_loss= 1.843   train_acc= 1.000   test_loss=4.129   test_acc= 0.556\n",
      "epoch= 37   train_loss= 1.837   train_acc= 1.000   test_loss=4.094   test_acc= 0.556\n",
      "epoch= 38   train_loss= 1.823   train_acc= 1.000   test_loss=4.093   test_acc= 0.556\n",
      "epoch= 39   train_loss= 1.810   train_acc= 1.000   test_loss=4.120   test_acc= 0.556\n",
      "epoch= 40   train_loss= 1.808   train_acc= 1.000   test_loss=4.120   test_acc= 0.556\n",
      "epoch= 41   train_loss= 1.794   train_acc= 1.000   test_loss=4.119   test_acc= 0.556\n",
      "epoch= 42   train_loss= 1.783   train_acc= 1.000   test_loss=4.089   test_acc= 0.556\n",
      "epoch= 43   train_loss= 1.773   train_acc= 1.000   test_loss=4.106   test_acc= 0.556\n",
      "epoch= 44   train_loss= 1.764   train_acc= 1.000   test_loss=4.102   test_acc= 0.556\n",
      "epoch= 45   train_loss= 1.749   train_acc= 1.000   test_loss=4.090   test_acc= 0.667\n",
      "epoch= 46   train_loss= 1.741   train_acc= 1.000   test_loss=4.136   test_acc= 0.556\n",
      "epoch= 47   train_loss= 1.729   train_acc= 1.000   test_loss=4.063   test_acc= 0.556\n",
      "epoch= 48   train_loss= 1.722   train_acc= 1.000   test_loss=4.043   test_acc= 0.556\n",
      "epoch= 49   train_loss= 1.709   train_acc= 1.000   test_loss=4.056   test_acc= 0.556\n",
      "run time: 1.2091410001118978 min\n",
      "test_acc=0.556\n",
      "run= 3   fold= 0\n",
      "epoch= 0   train_loss= 4.279   train_acc= 0.671   test_loss=4.044   test_acc= 0.700\n",
      "epoch= 1   train_loss= 3.230   train_acc= 0.878   test_loss=3.362   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.896   train_acc= 0.951   test_loss=3.571   test_acc= 0.800\n",
      "epoch= 3   train_loss= 2.736   train_acc= 0.976   test_loss=3.335   test_acc= 0.800\n",
      "epoch= 4   train_loss= 2.555   train_acc= 0.976   test_loss=3.453   test_acc= 0.800\n",
      "epoch= 5   train_loss= 2.548   train_acc= 0.963   test_loss=3.976   test_acc= 0.700\n",
      "epoch= 6   train_loss= 2.457   train_acc= 0.988   test_loss=3.018   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.404   train_acc= 1.000   test_loss=3.192   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.308   train_acc= 1.000   test_loss=3.185   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.306   train_acc= 1.000   test_loss=3.188   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.243   train_acc= 1.000   test_loss=3.153   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.242   train_acc= 1.000   test_loss=2.970   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.202   train_acc= 1.000   test_loss=3.063   test_acc= 0.800\n",
      "epoch= 13   train_loss= 2.176   train_acc= 1.000   test_loss=3.008   test_acc= 0.800\n",
      "epoch= 14   train_loss= 2.158   train_acc= 1.000   test_loss=3.065   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.146   train_acc= 1.000   test_loss=3.011   test_acc= 0.800\n",
      "epoch= 16   train_loss= 2.137   train_acc= 1.000   test_loss=2.962   test_acc= 0.800\n",
      "epoch= 17   train_loss= 2.112   train_acc= 1.000   test_loss=3.098   test_acc= 0.800\n",
      "epoch= 18   train_loss= 2.088   train_acc= 1.000   test_loss=2.973   test_acc= 0.800\n",
      "epoch= 19   train_loss= 2.083   train_acc= 1.000   test_loss=3.151   test_acc= 0.800\n",
      "epoch= 20   train_loss= 2.071   train_acc= 1.000   test_loss=2.949   test_acc= 0.800\n",
      "epoch= 21   train_loss= 2.045   train_acc= 1.000   test_loss=2.984   test_acc= 0.800\n",
      "epoch= 22   train_loss= 2.029   train_acc= 1.000   test_loss=3.055   test_acc= 0.800\n",
      "epoch= 23   train_loss= 2.013   train_acc= 1.000   test_loss=2.973   test_acc= 0.800\n",
      "epoch= 24   train_loss= 2.002   train_acc= 1.000   test_loss=2.949   test_acc= 0.800\n",
      "epoch= 25   train_loss= 1.989   train_acc= 1.000   test_loss=2.950   test_acc= 0.800\n",
      "epoch= 26   train_loss= 1.987   train_acc= 1.000   test_loss=2.954   test_acc= 0.800\n",
      "epoch= 27   train_loss= 1.963   train_acc= 1.000   test_loss=2.968   test_acc= 0.800\n",
      "epoch= 28   train_loss= 1.952   train_acc= 1.000   test_loss=2.907   test_acc= 0.800\n",
      "epoch= 29   train_loss= 1.942   train_acc= 1.000   test_loss=3.000   test_acc= 0.800\n",
      "epoch= 30   train_loss= 1.919   train_acc= 1.000   test_loss=2.918   test_acc= 0.800\n",
      "epoch= 31   train_loss= 1.920   train_acc= 1.000   test_loss=2.886   test_acc= 0.800\n",
      "epoch= 32   train_loss= 1.898   train_acc= 1.000   test_loss=2.926   test_acc= 0.800\n",
      "epoch= 33   train_loss= 1.892   train_acc= 1.000   test_loss=2.907   test_acc= 0.800\n",
      "epoch= 34   train_loss= 1.876   train_acc= 1.000   test_loss=2.923   test_acc= 0.800\n",
      "epoch= 35   train_loss= 1.865   train_acc= 1.000   test_loss=2.897   test_acc= 0.800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 36   train_loss= 1.863   train_acc= 1.000   test_loss=2.955   test_acc= 0.800\n",
      "epoch= 37   train_loss= 1.845   train_acc= 1.000   test_loss=2.884   test_acc= 0.800\n",
      "epoch= 38   train_loss= 1.839   train_acc= 1.000   test_loss=2.879   test_acc= 0.800\n",
      "epoch= 39   train_loss= 1.820   train_acc= 1.000   test_loss=2.888   test_acc= 0.800\n",
      "epoch= 40   train_loss= 1.818   train_acc= 1.000   test_loss=3.008   test_acc= 0.800\n",
      "epoch= 41   train_loss= 1.807   train_acc= 1.000   test_loss=2.859   test_acc= 0.800\n",
      "epoch= 42   train_loss= 1.788   train_acc= 1.000   test_loss=2.867   test_acc= 0.800\n",
      "epoch= 43   train_loss= 1.780   train_acc= 1.000   test_loss=2.811   test_acc= 0.800\n",
      "epoch= 44   train_loss= 1.772   train_acc= 1.000   test_loss=2.871   test_acc= 0.800\n",
      "epoch= 45   train_loss= 1.754   train_acc= 1.000   test_loss=2.833   test_acc= 0.800\n",
      "epoch= 46   train_loss= 1.750   train_acc= 1.000   test_loss=2.795   test_acc= 0.800\n",
      "epoch= 47   train_loss= 1.737   train_acc= 1.000   test_loss=2.821   test_acc= 0.800\n",
      "epoch= 48   train_loss= 1.727   train_acc= 1.000   test_loss=2.812   test_acc= 0.800\n",
      "epoch= 49   train_loss= 1.721   train_acc= 1.000   test_loss=2.824   test_acc= 0.800\n",
      "run time: 1.2448421160380045 min\n",
      "test_acc=0.800\n",
      "run= 3   fold= 1\n",
      "epoch= 0   train_loss= 4.535   train_acc= 0.646   test_loss=3.614   test_acc= 0.800\n",
      "epoch= 1   train_loss= 3.192   train_acc= 0.939   test_loss=3.547   test_acc= 0.800\n",
      "epoch= 2   train_loss= 3.130   train_acc= 0.915   test_loss=3.231   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.718   train_acc= 0.976   test_loss=3.091   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.651   train_acc= 0.976   test_loss=3.180   test_acc= 0.700\n",
      "epoch= 5   train_loss= 2.585   train_acc= 0.976   test_loss=3.059   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.524   train_acc= 0.976   test_loss=2.866   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.475   train_acc= 0.988   test_loss=2.979   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.347   train_acc= 1.000   test_loss=2.897   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.370   train_acc= 0.988   test_loss=2.908   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.279   train_acc= 1.000   test_loss=3.001   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.262   train_acc= 1.000   test_loss=2.793   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.244   train_acc= 1.000   test_loss=2.903   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.217   train_acc= 1.000   test_loss=2.837   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.192   train_acc= 1.000   test_loss=2.860   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.180   train_acc= 1.000   test_loss=2.924   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.200   train_acc= 1.000   test_loss=2.803   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.132   train_acc= 1.000   test_loss=2.825   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.135   train_acc= 1.000   test_loss=2.794   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.126   train_acc= 1.000   test_loss=2.769   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.105   train_acc= 1.000   test_loss=2.848   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.075   train_acc= 1.000   test_loss=2.688   test_acc= 0.900\n",
      "epoch= 22   train_loss= 2.057   train_acc= 1.000   test_loss=2.717   test_acc= 0.900\n",
      "epoch= 23   train_loss= 2.054   train_acc= 1.000   test_loss=2.781   test_acc= 0.900\n",
      "epoch= 24   train_loss= 2.038   train_acc= 1.000   test_loss=2.761   test_acc= 0.900\n",
      "epoch= 25   train_loss= 2.014   train_acc= 1.000   test_loss=2.730   test_acc= 0.900\n",
      "epoch= 26   train_loss= 2.001   train_acc= 1.000   test_loss=2.722   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.995   train_acc= 1.000   test_loss=2.756   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.975   train_acc= 1.000   test_loss=2.649   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.972   train_acc= 1.000   test_loss=2.658   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.949   train_acc= 1.000   test_loss=2.667   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.938   train_acc= 1.000   test_loss=2.650   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.928   train_acc= 1.000   test_loss=2.705   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.912   train_acc= 1.000   test_loss=2.677   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.902   train_acc= 1.000   test_loss=2.651   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.896   train_acc= 1.000   test_loss=2.636   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.878   train_acc= 1.000   test_loss=2.576   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.870   train_acc= 1.000   test_loss=2.560   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.860   train_acc= 1.000   test_loss=2.536   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.851   train_acc= 1.000   test_loss=2.494   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.838   train_acc= 1.000   test_loss=2.512   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.828   train_acc= 1.000   test_loss=2.504   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.813   train_acc= 1.000   test_loss=2.499   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.807   train_acc= 1.000   test_loss=2.499   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.798   train_acc= 1.000   test_loss=2.472   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.787   train_acc= 1.000   test_loss=2.440   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.776   train_acc= 1.000   test_loss=2.406   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.769   train_acc= 1.000   test_loss=2.415   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.761   train_acc= 1.000   test_loss=2.429   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.746   train_acc= 1.000   test_loss=2.392   test_acc= 0.900\n",
      "run time: 1.2225870450337728 min\n",
      "test_acc=0.900\n",
      "run= 3   fold= 2\n",
      "epoch= 0   train_loss= 4.503   train_acc= 0.687   test_loss=3.571   test_acc= 0.778\n",
      "epoch= 1   train_loss= 3.527   train_acc= 0.855   test_loss=3.522   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.918   train_acc= 0.952   test_loss=3.241   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.744   train_acc= 0.952   test_loss=3.091   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.679   train_acc= 0.952   test_loss=3.259   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.525   train_acc= 0.988   test_loss=3.260   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.494   train_acc= 0.988   test_loss=3.169   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.406   train_acc= 1.000   test_loss=3.203   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.344   train_acc= 1.000   test_loss=3.152   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.329   train_acc= 1.000   test_loss=3.076   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.306   train_acc= 1.000   test_loss=3.139   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.254   train_acc= 1.000   test_loss=3.118   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.247   train_acc= 1.000   test_loss=3.086   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.218   train_acc= 1.000   test_loss=3.150   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.197   train_acc= 1.000   test_loss=3.243   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.165   train_acc= 1.000   test_loss=3.129   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.148   train_acc= 1.000   test_loss=3.088   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.144   train_acc= 1.000   test_loss=3.078   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.119   train_acc= 1.000   test_loss=3.033   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.096   train_acc= 1.000   test_loss=3.075   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.092   train_acc= 1.000   test_loss=3.055   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.065   train_acc= 1.000   test_loss=3.061   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.063   train_acc= 1.000   test_loss=3.035   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.037   train_acc= 1.000   test_loss=3.013   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.026   train_acc= 1.000   test_loss=3.008   test_acc= 0.889\n",
      "epoch= 25   train_loss= 2.026   train_acc= 1.000   test_loss=3.080   test_acc= 0.889\n",
      "epoch= 26   train_loss= 2.001   train_acc= 1.000   test_loss=3.010   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.986   train_acc= 1.000   test_loss=2.973   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.979   train_acc= 1.000   test_loss=3.021   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.961   train_acc= 1.000   test_loss=2.988   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.952   train_acc= 1.000   test_loss=2.969   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.937   train_acc= 1.000   test_loss=2.956   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 32   train_loss= 1.917   train_acc= 1.000   test_loss=2.916   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.917   train_acc= 1.000   test_loss=2.939   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.899   train_acc= 1.000   test_loss=2.961   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.889   train_acc= 1.000   test_loss=2.959   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.876   train_acc= 1.000   test_loss=2.961   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.869   train_acc= 1.000   test_loss=2.956   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.854   train_acc= 1.000   test_loss=2.922   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.843   train_acc= 1.000   test_loss=2.896   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.830   train_acc= 1.000   test_loss=2.924   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.825   train_acc= 1.000   test_loss=2.922   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.811   train_acc= 1.000   test_loss=2.797   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.797   train_acc= 1.000   test_loss=2.812   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.789   train_acc= 1.000   test_loss=2.866   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.777   train_acc= 1.000   test_loss=2.862   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.766   train_acc= 1.000   test_loss=2.904   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.764   train_acc= 1.000   test_loss=2.882   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.753   train_acc= 1.000   test_loss=2.854   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.743   train_acc= 1.000   test_loss=2.855   test_acc= 0.889\n",
      "run time: 1.2162968039512634 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 3\n",
      "epoch= 0   train_loss= 4.182   train_acc= 0.663   test_loss=3.295   test_acc= 1.000\n",
      "epoch= 1   train_loss= 3.299   train_acc= 0.904   test_loss=3.247   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.966   train_acc= 0.928   test_loss=2.990   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.759   train_acc= 0.964   test_loss=2.941   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.667   train_acc= 0.952   test_loss=2.932   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.553   train_acc= 0.988   test_loss=2.726   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.517   train_acc= 0.988   test_loss=2.828   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.406   train_acc= 0.988   test_loss=2.810   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.351   train_acc= 1.000   test_loss=2.619   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.314   train_acc= 1.000   test_loss=2.664   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.289   train_acc= 1.000   test_loss=2.728   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.260   train_acc= 1.000   test_loss=2.763   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.239   train_acc= 1.000   test_loss=2.790   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.223   train_acc= 1.000   test_loss=2.605   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.188   train_acc= 1.000   test_loss=2.610   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.175   train_acc= 1.000   test_loss=2.687   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.165   train_acc= 1.000   test_loss=2.732   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.137   train_acc= 1.000   test_loss=2.597   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.155   train_acc= 1.000   test_loss=2.563   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.107   train_acc= 1.000   test_loss=2.637   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.089   train_acc= 1.000   test_loss=2.514   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.074   train_acc= 1.000   test_loss=2.609   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.055   train_acc= 1.000   test_loss=2.574   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.043   train_acc= 1.000   test_loss=2.490   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.030   train_acc= 1.000   test_loss=2.523   test_acc= 0.889\n",
      "epoch= 25   train_loss= 2.015   train_acc= 1.000   test_loss=2.544   test_acc= 0.889\n",
      "epoch= 26   train_loss= 2.001   train_acc= 1.000   test_loss=2.502   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.985   train_acc= 1.000   test_loss=2.518   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.981   train_acc= 1.000   test_loss=2.465   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.987   train_acc= 1.000   test_loss=2.553   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.949   train_acc= 1.000   test_loss=2.478   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.940   train_acc= 1.000   test_loss=2.500   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.940   train_acc= 1.000   test_loss=2.374   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.909   train_acc= 1.000   test_loss=2.407   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.894   train_acc= 1.000   test_loss=2.417   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.886   train_acc= 1.000   test_loss=2.443   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.884   train_acc= 1.000   test_loss=2.433   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.878   train_acc= 1.000   test_loss=2.408   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.855   train_acc= 1.000   test_loss=2.380   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.842   train_acc= 1.000   test_loss=2.399   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.834   train_acc= 1.000   test_loss=2.342   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.817   train_acc= 1.000   test_loss=2.333   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.809   train_acc= 1.000   test_loss=2.357   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.800   train_acc= 1.000   test_loss=2.353   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.794   train_acc= 1.000   test_loss=2.335   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.776   train_acc= 1.000   test_loss=2.335   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.767   train_acc= 1.000   test_loss=2.280   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.757   train_acc= 1.000   test_loss=2.297   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.755   train_acc= 1.000   test_loss=2.296   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.737   train_acc= 1.000   test_loss=2.288   test_acc= 0.889\n",
      "run time: 1.2286797364552815 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 4\n",
      "epoch= 0   train_loss= 4.485   train_acc= 0.639   test_loss=4.199   test_acc= 0.778\n",
      "epoch= 1   train_loss= 3.335   train_acc= 0.867   test_loss=4.170   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.915   train_acc= 0.952   test_loss=4.008   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.695   train_acc= 0.976   test_loss=3.854   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.572   train_acc= 0.988   test_loss=3.883   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.501   train_acc= 0.988   test_loss=3.767   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.431   train_acc= 1.000   test_loss=3.887   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.382   train_acc= 1.000   test_loss=3.808   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.336   train_acc= 1.000   test_loss=3.742   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.329   train_acc= 1.000   test_loss=3.781   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.313   train_acc= 1.000   test_loss=3.884   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.259   train_acc= 1.000   test_loss=3.758   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.243   train_acc= 1.000   test_loss=3.860   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.210   train_acc= 1.000   test_loss=3.876   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.192   train_acc= 1.000   test_loss=3.847   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.160   train_acc= 1.000   test_loss=3.802   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.157   train_acc= 1.000   test_loss=3.726   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.135   train_acc= 1.000   test_loss=3.794   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.123   train_acc= 1.000   test_loss=3.786   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.109   train_acc= 1.000   test_loss=3.859   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.093   train_acc= 1.000   test_loss=3.888   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.064   train_acc= 1.000   test_loss=3.785   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.050   train_acc= 1.000   test_loss=3.884   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.034   train_acc= 1.000   test_loss=3.875   test_acc= 0.778\n",
      "epoch= 24   train_loss= 2.028   train_acc= 1.000   test_loss=3.811   test_acc= 0.778\n",
      "epoch= 25   train_loss= 2.016   train_acc= 1.000   test_loss=3.828   test_acc= 0.778\n",
      "epoch= 26   train_loss= 2.002   train_acc= 1.000   test_loss=3.773   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.983   train_acc= 1.000   test_loss=3.794   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 28   train_loss= 1.970   train_acc= 1.000   test_loss=3.796   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.962   train_acc= 1.000   test_loss=3.741   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.953   train_acc= 1.000   test_loss=3.741   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.942   train_acc= 1.000   test_loss=3.811   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.923   train_acc= 1.000   test_loss=3.721   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.919   train_acc= 1.000   test_loss=4.042   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.901   train_acc= 1.000   test_loss=3.721   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.884   train_acc= 1.000   test_loss=3.660   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.875   train_acc= 1.000   test_loss=3.595   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.861   train_acc= 1.000   test_loss=3.616   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.854   train_acc= 1.000   test_loss=3.675   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.846   train_acc= 1.000   test_loss=3.631   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.829   train_acc= 1.000   test_loss=3.639   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.821   train_acc= 1.000   test_loss=3.598   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.810   train_acc= 1.000   test_loss=3.592   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.802   train_acc= 1.000   test_loss=3.573   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.782   train_acc= 1.000   test_loss=3.569   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.782   train_acc= 1.000   test_loss=3.542   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.776   train_acc= 1.000   test_loss=3.780   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.759   train_acc= 1.000   test_loss=3.709   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.746   train_acc= 1.000   test_loss=3.625   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.737   train_acc= 1.000   test_loss=3.514   test_acc= 0.778\n",
      "run time: 1.2297546505928039 min\n",
      "test_acc=0.778\n",
      "run= 3   fold= 5\n",
      "epoch= 0   train_loss= 4.500   train_acc= 0.711   test_loss=4.294   test_acc= 0.667\n",
      "epoch= 1   train_loss= 3.363   train_acc= 0.843   test_loss=3.394   test_acc= 0.778\n",
      "epoch= 2   train_loss= 3.001   train_acc= 0.940   test_loss=3.336   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.759   train_acc= 0.940   test_loss=3.363   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.600   train_acc= 0.988   test_loss=3.307   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.563   train_acc= 0.988   test_loss=2.981   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.515   train_acc= 0.964   test_loss=3.164   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.431   train_acc= 1.000   test_loss=2.910   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.377   train_acc= 1.000   test_loss=2.796   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.322   train_acc= 0.988   test_loss=2.751   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.292   train_acc= 1.000   test_loss=2.781   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.290   train_acc= 1.000   test_loss=2.660   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.244   train_acc= 1.000   test_loss=2.568   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.204   train_acc= 1.000   test_loss=2.555   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.196   train_acc= 1.000   test_loss=2.583   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.174   train_acc= 1.000   test_loss=2.534   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.158   train_acc= 1.000   test_loss=2.505   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.125   train_acc= 1.000   test_loss=2.498   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.114   train_acc= 1.000   test_loss=2.452   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.104   train_acc= 1.000   test_loss=2.463   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.089   train_acc= 1.000   test_loss=2.468   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.079   train_acc= 1.000   test_loss=2.398   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.056   train_acc= 1.000   test_loss=2.401   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.035   train_acc= 1.000   test_loss=2.391   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.017   train_acc= 1.000   test_loss=2.364   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.009   train_acc= 1.000   test_loss=2.366   test_acc= 1.000\n",
      "epoch= 26   train_loss= 2.022   train_acc= 1.000   test_loss=2.337   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.993   train_acc= 1.000   test_loss=2.293   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.966   train_acc= 1.000   test_loss=2.276   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.965   train_acc= 1.000   test_loss=2.283   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.945   train_acc= 1.000   test_loss=2.290   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.934   train_acc= 1.000   test_loss=2.288   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.919   train_acc= 1.000   test_loss=2.223   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.900   train_acc= 1.000   test_loss=2.212   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.901   train_acc= 1.000   test_loss=2.212   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.886   train_acc= 1.000   test_loss=2.192   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.872   train_acc= 1.000   test_loss=2.170   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.852   train_acc= 1.000   test_loss=2.148   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.864   train_acc= 1.000   test_loss=2.154   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.836   train_acc= 1.000   test_loss=2.125   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.839   train_acc= 1.000   test_loss=2.149   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.821   train_acc= 1.000   test_loss=2.171   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.809   train_acc= 1.000   test_loss=2.116   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.799   train_acc= 1.000   test_loss=2.119   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.784   train_acc= 1.000   test_loss=2.105   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.769   train_acc= 1.000   test_loss=2.083   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.765   train_acc= 1.000   test_loss=2.085   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.757   train_acc= 1.000   test_loss=2.048   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.741   train_acc= 1.000   test_loss=2.030   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.738   train_acc= 1.000   test_loss=2.037   test_acc= 1.000\n",
      "run time: 1.2175410350163778 min\n",
      "test_acc=1.000\n",
      "run= 3   fold= 6\n",
      "epoch= 0   train_loss= 4.262   train_acc= 0.735   test_loss=3.231   test_acc= 1.000\n",
      "epoch= 1   train_loss= 3.299   train_acc= 0.904   test_loss=3.162   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.978   train_acc= 0.940   test_loss=2.729   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.705   train_acc= 0.964   test_loss=2.751   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.575   train_acc= 0.988   test_loss=2.693   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.543   train_acc= 0.976   test_loss=2.691   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.481   train_acc= 0.964   test_loss=2.653   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.385   train_acc= 0.988   test_loss=2.669   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.390   train_acc= 1.000   test_loss=2.512   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.306   train_acc= 1.000   test_loss=2.519   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.264   train_acc= 1.000   test_loss=2.503   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.250   train_acc= 1.000   test_loss=2.458   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.221   train_acc= 1.000   test_loss=2.432   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.220   train_acc= 1.000   test_loss=2.416   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.198   train_acc= 1.000   test_loss=2.405   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.148   train_acc= 1.000   test_loss=2.376   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.142   train_acc= 1.000   test_loss=2.366   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.125   train_acc= 1.000   test_loss=2.356   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.122   train_acc= 1.000   test_loss=2.346   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.088   train_acc= 1.000   test_loss=2.331   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.092   train_acc= 1.000   test_loss=2.325   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.076   train_acc= 1.000   test_loss=2.288   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.050   train_acc= 1.000   test_loss=2.275   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.041   train_acc= 1.000   test_loss=2.270   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 24   train_loss= 2.030   train_acc= 1.000   test_loss=2.245   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.002   train_acc= 1.000   test_loss=2.237   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.987   train_acc= 1.000   test_loss=2.219   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.971   train_acc= 1.000   test_loss=2.201   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.962   train_acc= 1.000   test_loss=2.186   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.948   train_acc= 1.000   test_loss=2.171   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.939   train_acc= 1.000   test_loss=2.156   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.933   train_acc= 1.000   test_loss=2.145   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.910   train_acc= 1.000   test_loss=2.136   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.898   train_acc= 1.000   test_loss=2.128   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.880   train_acc= 1.000   test_loss=2.116   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.875   train_acc= 1.000   test_loss=2.102   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.860   train_acc= 1.000   test_loss=2.089   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.854   train_acc= 1.000   test_loss=2.087   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.844   train_acc= 1.000   test_loss=2.086   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.832   train_acc= 1.000   test_loss=2.050   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.826   train_acc= 1.000   test_loss=2.040   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.807   train_acc= 1.000   test_loss=2.023   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.795   train_acc= 1.000   test_loss=2.008   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.783   train_acc= 1.000   test_loss=1.993   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.773   train_acc= 1.000   test_loss=1.978   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.769   train_acc= 1.000   test_loss=1.977   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.753   train_acc= 1.000   test_loss=1.971   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.746   train_acc= 1.000   test_loss=1.965   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.742   train_acc= 1.000   test_loss=1.954   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.742   train_acc= 1.000   test_loss=1.941   test_acc= 1.000\n",
      "run time: 1.2166478673617045 min\n",
      "test_acc=1.000\n",
      "run= 3   fold= 7\n",
      "epoch= 0   train_loss= 4.452   train_acc= 0.639   test_loss=3.689   test_acc= 0.889\n",
      "epoch= 1   train_loss= 3.554   train_acc= 0.819   test_loss=4.163   test_acc= 0.556\n",
      "epoch= 2   train_loss= 2.934   train_acc= 0.904   test_loss=3.708   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.775   train_acc= 0.964   test_loss=3.482   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.595   train_acc= 0.988   test_loss=3.350   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.610   train_acc= 0.976   test_loss=3.357   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.448   train_acc= 0.988   test_loss=3.282   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.413   train_acc= 0.976   test_loss=3.271   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.348   train_acc= 1.000   test_loss=3.200   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.337   train_acc= 1.000   test_loss=3.204   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.300   train_acc= 1.000   test_loss=3.137   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.242   train_acc= 1.000   test_loss=3.118   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.235   train_acc= 1.000   test_loss=3.049   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.222   train_acc= 1.000   test_loss=3.070   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.201   train_acc= 1.000   test_loss=3.012   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.161   train_acc= 1.000   test_loss=2.971   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.152   train_acc= 1.000   test_loss=2.936   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.144   train_acc= 1.000   test_loss=2.921   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.111   train_acc= 1.000   test_loss=2.889   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.096   train_acc= 1.000   test_loss=2.858   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.089   train_acc= 1.000   test_loss=2.870   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.071   train_acc= 1.000   test_loss=2.816   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.043   train_acc= 1.000   test_loss=2.803   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.039   train_acc= 1.000   test_loss=2.798   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.022   train_acc= 1.000   test_loss=2.780   test_acc= 0.889\n",
      "epoch= 25   train_loss= 2.010   train_acc= 1.000   test_loss=2.733   test_acc= 0.889\n",
      "epoch= 26   train_loss= 2.006   train_acc= 1.000   test_loss=2.762   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.981   train_acc= 1.000   test_loss=2.741   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.983   train_acc= 1.000   test_loss=2.745   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.958   train_acc= 1.000   test_loss=2.710   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.944   train_acc= 1.000   test_loss=2.699   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.927   train_acc= 1.000   test_loss=2.691   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.922   train_acc= 1.000   test_loss=2.666   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.911   train_acc= 1.000   test_loss=2.664   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.910   train_acc= 1.000   test_loss=2.664   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.882   train_acc= 1.000   test_loss=2.678   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.871   train_acc= 1.000   test_loss=2.647   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.855   train_acc= 1.000   test_loss=2.626   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.850   train_acc= 1.000   test_loss=2.615   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.835   train_acc= 1.000   test_loss=2.587   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.824   train_acc= 1.000   test_loss=2.569   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.826   train_acc= 1.000   test_loss=2.558   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.810   train_acc= 1.000   test_loss=2.549   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.793   train_acc= 1.000   test_loss=2.545   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.778   train_acc= 1.000   test_loss=2.530   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.775   train_acc= 1.000   test_loss=2.527   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.762   train_acc= 1.000   test_loss=2.523   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.753   train_acc= 1.000   test_loss=2.489   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.748   train_acc= 1.000   test_loss=2.473   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.747   train_acc= 1.000   test_loss=2.454   test_acc= 0.889\n",
      "run time: 1.2187280972798666 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 8\n",
      "epoch= 0   train_loss= 4.318   train_acc= 0.747   test_loss=3.676   test_acc= 0.778\n",
      "epoch= 1   train_loss= 3.321   train_acc= 0.892   test_loss=3.627   test_acc= 0.667\n",
      "epoch= 2   train_loss= 3.070   train_acc= 0.916   test_loss=3.148   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.895   train_acc= 0.928   test_loss=2.835   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.670   train_acc= 0.964   test_loss=3.061   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.485   train_acc= 0.988   test_loss=2.765   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.423   train_acc= 1.000   test_loss=2.643   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.375   train_acc= 1.000   test_loss=2.554   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.353   train_acc= 1.000   test_loss=2.571   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.305   train_acc= 1.000   test_loss=2.536   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.275   train_acc= 1.000   test_loss=2.527   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.253   train_acc= 1.000   test_loss=2.436   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.244   train_acc= 1.000   test_loss=2.431   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.197   train_acc= 1.000   test_loss=2.397   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.187   train_acc= 1.000   test_loss=2.351   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.165   train_acc= 1.000   test_loss=2.332   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.145   train_acc= 1.000   test_loss=2.278   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.128   train_acc= 1.000   test_loss=2.247   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.111   train_acc= 1.000   test_loss=2.286   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.105   train_acc= 1.000   test_loss=2.277   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 20   train_loss= 2.096   train_acc= 1.000   test_loss=2.213   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.081   train_acc= 1.000   test_loss=2.222   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.062   train_acc= 1.000   test_loss=2.182   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.041   train_acc= 1.000   test_loss=2.154   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.019   train_acc= 1.000   test_loss=2.138   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.018   train_acc= 1.000   test_loss=2.139   test_acc= 1.000\n",
      "epoch= 26   train_loss= 2.004   train_acc= 1.000   test_loss=2.125   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.987   train_acc= 1.000   test_loss=2.097   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.973   train_acc= 1.000   test_loss=2.081   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.959   train_acc= 1.000   test_loss=2.062   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.943   train_acc= 1.000   test_loss=2.039   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.932   train_acc= 1.000   test_loss=2.031   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.920   train_acc= 1.000   test_loss=2.017   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.913   train_acc= 1.000   test_loss=2.034   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.897   train_acc= 1.000   test_loss=1.999   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.886   train_acc= 1.000   test_loss=1.987   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.873   train_acc= 1.000   test_loss=1.984   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.867   train_acc= 1.000   test_loss=1.987   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.850   train_acc= 1.000   test_loss=1.958   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.836   train_acc= 1.000   test_loss=1.932   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.836   train_acc= 1.000   test_loss=1.924   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.816   train_acc= 1.000   test_loss=1.927   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.804   train_acc= 1.000   test_loss=1.907   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.799   train_acc= 1.000   test_loss=1.898   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.784   train_acc= 1.000   test_loss=1.884   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.776   train_acc= 1.000   test_loss=1.867   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.774   train_acc= 1.000   test_loss=1.864   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.752   train_acc= 1.000   test_loss=1.871   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.746   train_acc= 1.000   test_loss=1.847   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.734   train_acc= 1.000   test_loss=1.842   test_acc= 1.000\n",
      "run time: 1.2386851986249288 min\n",
      "test_acc=1.000\n",
      "run= 3   fold= 9\n",
      "epoch= 0   train_loss= 4.512   train_acc= 0.699   test_loss=3.887   test_acc= 0.778\n",
      "epoch= 1   train_loss= 3.359   train_acc= 0.867   test_loss=3.586   test_acc= 0.889\n",
      "epoch= 2   train_loss= 3.097   train_acc= 0.916   test_loss=3.409   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.904   train_acc= 0.940   test_loss=3.222   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.674   train_acc= 0.964   test_loss=3.045   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.603   train_acc= 0.964   test_loss=3.061   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.505   train_acc= 0.988   test_loss=2.877   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.493   train_acc= 0.976   test_loss=2.844   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.353   train_acc= 1.000   test_loss=2.788   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.319   train_acc= 1.000   test_loss=2.766   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.277   train_acc= 1.000   test_loss=2.695   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.276   train_acc= 1.000   test_loss=2.658   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.251   train_acc= 1.000   test_loss=2.693   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.236   train_acc= 0.988   test_loss=2.762   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.181   train_acc= 1.000   test_loss=2.679   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.161   train_acc= 1.000   test_loss=2.689   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.130   train_acc= 1.000   test_loss=2.625   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.124   train_acc= 1.000   test_loss=2.610   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.108   train_acc= 1.000   test_loss=2.531   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.090   train_acc= 1.000   test_loss=2.539   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.075   train_acc= 1.000   test_loss=2.527   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.043   train_acc= 1.000   test_loss=2.496   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.065   train_acc= 1.000   test_loss=2.524   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.037   train_acc= 1.000   test_loss=2.506   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.010   train_acc= 1.000   test_loss=2.488   test_acc= 0.889\n",
      "epoch= 25   train_loss= 2.003   train_acc= 1.000   test_loss=2.494   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.982   train_acc= 1.000   test_loss=2.429   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.963   train_acc= 1.000   test_loss=2.401   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.958   train_acc= 1.000   test_loss=2.398   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.947   train_acc= 1.000   test_loss=2.373   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.939   train_acc= 1.000   test_loss=2.385   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.935   train_acc= 1.000   test_loss=2.409   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.913   train_acc= 1.000   test_loss=2.357   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.896   train_acc= 1.000   test_loss=2.346   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.882   train_acc= 1.000   test_loss=2.343   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.865   train_acc= 1.000   test_loss=2.331   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.871   train_acc= 1.000   test_loss=2.334   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.856   train_acc= 1.000   test_loss=2.336   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.841   train_acc= 1.000   test_loss=2.308   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.827   train_acc= 1.000   test_loss=2.277   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.812   train_acc= 1.000   test_loss=2.273   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.806   train_acc= 1.000   test_loss=2.246   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.794   train_acc= 1.000   test_loss=2.229   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.785   train_acc= 1.000   test_loss=2.199   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.772   train_acc= 1.000   test_loss=2.184   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.763   train_acc= 1.000   test_loss=2.177   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.756   train_acc= 1.000   test_loss=2.190   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.749   train_acc= 1.000   test_loss=2.209   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.731   train_acc= 1.000   test_loss=2.185   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.723   train_acc= 1.000   test_loss=2.169   test_acc= 0.889\n",
      "run time: 1.2171532352765402 min\n",
      "test_acc=0.889\n",
      "run= 4   fold= 0\n",
      "epoch= 0   train_loss= 4.357   train_acc= 0.646   test_loss=3.506   test_acc= 0.900\n",
      "epoch= 1   train_loss= 3.241   train_acc= 0.927   test_loss=3.586   test_acc= 0.800\n",
      "epoch= 2   train_loss= 2.896   train_acc= 0.939   test_loss=3.549   test_acc= 0.800\n",
      "epoch= 3   train_loss= 2.713   train_acc= 0.976   test_loss=3.139   test_acc= 0.800\n",
      "epoch= 4   train_loss= 2.673   train_acc= 0.988   test_loss=3.203   test_acc= 0.800\n",
      "epoch= 5   train_loss= 2.565   train_acc= 0.963   test_loss=3.389   test_acc= 0.800\n",
      "epoch= 6   train_loss= 2.493   train_acc= 0.988   test_loss=3.525   test_acc= 0.800\n",
      "epoch= 7   train_loss= 2.399   train_acc= 0.988   test_loss=3.174   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.356   train_acc= 1.000   test_loss=3.338   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.330   train_acc= 1.000   test_loss=3.195   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.288   train_acc= 1.000   test_loss=3.294   test_acc= 0.800\n",
      "epoch= 11   train_loss= 2.278   train_acc= 1.000   test_loss=3.079   test_acc= 0.800\n",
      "epoch= 12   train_loss= 2.255   train_acc= 0.988   test_loss=3.078   test_acc= 0.800\n",
      "epoch= 13   train_loss= 2.229   train_acc= 1.000   test_loss=3.242   test_acc= 0.800\n",
      "epoch= 14   train_loss= 2.177   train_acc= 1.000   test_loss=3.118   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.159   train_acc= 1.000   test_loss=3.224   test_acc= 0.800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 16   train_loss= 2.148   train_acc= 1.000   test_loss=3.088   test_acc= 0.800\n",
      "epoch= 17   train_loss= 2.133   train_acc= 1.000   test_loss=3.107   test_acc= 0.800\n",
      "epoch= 18   train_loss= 2.107   train_acc= 1.000   test_loss=3.066   test_acc= 0.800\n",
      "epoch= 19   train_loss= 2.100   train_acc= 1.000   test_loss=3.060   test_acc= 0.800\n",
      "epoch= 20   train_loss= 2.076   train_acc= 1.000   test_loss=3.014   test_acc= 0.800\n",
      "epoch= 21   train_loss= 2.063   train_acc= 1.000   test_loss=2.976   test_acc= 0.800\n",
      "epoch= 22   train_loss= 2.049   train_acc= 1.000   test_loss=3.051   test_acc= 0.800\n",
      "epoch= 23   train_loss= 2.033   train_acc= 1.000   test_loss=2.955   test_acc= 0.800\n",
      "epoch= 24   train_loss= 2.040   train_acc= 1.000   test_loss=2.819   test_acc= 0.800\n",
      "epoch= 25   train_loss= 2.006   train_acc= 1.000   test_loss=2.922   test_acc= 0.800\n",
      "epoch= 26   train_loss= 1.992   train_acc= 1.000   test_loss=2.958   test_acc= 0.800\n",
      "epoch= 27   train_loss= 1.979   train_acc= 1.000   test_loss=2.987   test_acc= 0.800\n",
      "epoch= 28   train_loss= 1.968   train_acc= 1.000   test_loss=2.960   test_acc= 0.800\n",
      "epoch= 29   train_loss= 1.965   train_acc= 1.000   test_loss=2.962   test_acc= 0.800\n",
      "epoch= 30   train_loss= 1.945   train_acc= 1.000   test_loss=2.977   test_acc= 0.800\n",
      "epoch= 31   train_loss= 1.932   train_acc= 1.000   test_loss=2.953   test_acc= 0.800\n",
      "epoch= 32   train_loss= 1.925   train_acc= 1.000   test_loss=2.868   test_acc= 0.800\n",
      "epoch= 33   train_loss= 1.910   train_acc= 1.000   test_loss=2.898   test_acc= 0.800\n",
      "epoch= 34   train_loss= 1.898   train_acc= 1.000   test_loss=2.878   test_acc= 0.800\n",
      "epoch= 35   train_loss= 1.881   train_acc= 1.000   test_loss=2.919   test_acc= 0.800\n",
      "epoch= 36   train_loss= 1.876   train_acc= 1.000   test_loss=2.868   test_acc= 0.800\n",
      "epoch= 37   train_loss= 1.858   train_acc= 1.000   test_loss=2.922   test_acc= 0.800\n",
      "epoch= 38   train_loss= 1.847   train_acc= 1.000   test_loss=2.844   test_acc= 0.800\n",
      "epoch= 39   train_loss= 1.845   train_acc= 1.000   test_loss=2.919   test_acc= 0.800\n",
      "epoch= 40   train_loss= 1.831   train_acc= 1.000   test_loss=2.900   test_acc= 0.800\n",
      "epoch= 41   train_loss= 1.819   train_acc= 1.000   test_loss=2.859   test_acc= 0.800\n",
      "epoch= 42   train_loss= 1.812   train_acc= 1.000   test_loss=2.892   test_acc= 0.800\n",
      "epoch= 43   train_loss= 1.793   train_acc= 1.000   test_loss=2.876   test_acc= 0.800\n",
      "epoch= 44   train_loss= 1.788   train_acc= 1.000   test_loss=2.810   test_acc= 0.800\n",
      "epoch= 45   train_loss= 1.782   train_acc= 1.000   test_loss=2.737   test_acc= 0.800\n",
      "epoch= 46   train_loss= 1.764   train_acc= 1.000   test_loss=2.785   test_acc= 0.800\n",
      "epoch= 47   train_loss= 1.754   train_acc= 1.000   test_loss=2.795   test_acc= 0.800\n",
      "epoch= 48   train_loss= 1.746   train_acc= 1.000   test_loss=2.759   test_acc= 0.800\n",
      "epoch= 49   train_loss= 1.738   train_acc= 1.000   test_loss=2.744   test_acc= 0.800\n",
      "run time: 1.2317257165908813 min\n",
      "test_acc=0.800\n",
      "run= 4   fold= 1\n",
      "epoch= 0   train_loss= 4.460   train_acc= 0.646   test_loss=4.122   test_acc= 0.700\n",
      "epoch= 1   train_loss= 3.337   train_acc= 0.890   test_loss=3.568   test_acc= 0.700\n",
      "epoch= 2   train_loss= 2.872   train_acc= 0.963   test_loss=3.749   test_acc= 0.600\n",
      "epoch= 3   train_loss= 2.850   train_acc= 0.951   test_loss=3.813   test_acc= 0.700\n",
      "epoch= 4   train_loss= 2.692   train_acc= 0.951   test_loss=3.589   test_acc= 0.800\n",
      "epoch= 5   train_loss= 2.595   train_acc= 0.976   test_loss=3.398   test_acc= 0.800\n",
      "epoch= 6   train_loss= 2.511   train_acc= 0.988   test_loss=3.458   test_acc= 0.800\n",
      "epoch= 7   train_loss= 2.472   train_acc= 0.988   test_loss=2.966   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.405   train_acc= 0.988   test_loss=3.093   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.356   train_acc= 1.000   test_loss=3.208   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.338   train_acc= 0.988   test_loss=3.147   test_acc= 0.800\n",
      "epoch= 11   train_loss= 2.271   train_acc= 1.000   test_loss=2.967   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.231   train_acc= 1.000   test_loss=3.103   test_acc= 0.800\n",
      "epoch= 13   train_loss= 2.226   train_acc= 1.000   test_loss=2.945   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.212   train_acc= 1.000   test_loss=3.026   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.182   train_acc= 1.000   test_loss=2.922   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.152   train_acc= 1.000   test_loss=2.901   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.144   train_acc= 1.000   test_loss=2.749   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.122   train_acc= 1.000   test_loss=2.787   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.116   train_acc= 1.000   test_loss=2.848   test_acc= 0.800\n",
      "epoch= 20   train_loss= 2.105   train_acc= 1.000   test_loss=2.753   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.091   train_acc= 1.000   test_loss=2.782   test_acc= 0.900\n",
      "epoch= 22   train_loss= 2.077   train_acc= 1.000   test_loss=2.732   test_acc= 0.900\n",
      "epoch= 23   train_loss= 2.062   train_acc= 1.000   test_loss=2.752   test_acc= 0.900\n",
      "epoch= 24   train_loss= 2.036   train_acc= 1.000   test_loss=2.687   test_acc= 0.900\n",
      "epoch= 25   train_loss= 2.029   train_acc= 1.000   test_loss=2.652   test_acc= 0.900\n",
      "epoch= 26   train_loss= 2.017   train_acc= 1.000   test_loss=2.728   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.993   train_acc= 1.000   test_loss=2.707   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.983   train_acc= 1.000   test_loss=2.679   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.976   train_acc= 1.000   test_loss=2.634   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.957   train_acc= 1.000   test_loss=2.636   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.949   train_acc= 1.000   test_loss=2.594   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.925   train_acc= 1.000   test_loss=2.599   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.918   train_acc= 1.000   test_loss=2.599   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.903   train_acc= 1.000   test_loss=2.627   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.893   train_acc= 1.000   test_loss=2.555   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.889   train_acc= 1.000   test_loss=2.563   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.877   train_acc= 1.000   test_loss=2.536   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.856   train_acc= 1.000   test_loss=2.496   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.848   train_acc= 1.000   test_loss=2.465   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.838   train_acc= 1.000   test_loss=2.466   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.847   train_acc= 1.000   test_loss=2.497   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.819   train_acc= 1.000   test_loss=2.459   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.808   train_acc= 1.000   test_loss=2.459   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.799   train_acc= 1.000   test_loss=2.454   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.789   train_acc= 1.000   test_loss=2.500   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.776   train_acc= 1.000   test_loss=2.471   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.769   train_acc= 1.000   test_loss=2.451   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.759   train_acc= 1.000   test_loss=2.455   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.744   train_acc= 1.000   test_loss=2.433   test_acc= 0.900\n",
      "run time: 1.2004557649294536 min\n",
      "test_acc=0.900\n",
      "run= 4   fold= 2\n",
      "epoch= 0   train_loss= 4.731   train_acc= 0.651   test_loss=3.248   test_acc= 1.000\n",
      "epoch= 1   train_loss= 3.487   train_acc= 0.855   test_loss=3.108   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.913   train_acc= 0.940   test_loss=2.966   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.803   train_acc= 0.952   test_loss=2.712   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.555   train_acc= 0.988   test_loss=2.596   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.518   train_acc= 0.976   test_loss=2.602   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.467   train_acc= 0.988   test_loss=2.575   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.358   train_acc= 1.000   test_loss=2.513   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.303   train_acc= 1.000   test_loss=2.471   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.387   train_acc= 0.988   test_loss=2.518   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.272   train_acc= 1.000   test_loss=2.416   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.251   train_acc= 1.000   test_loss=2.408   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 12   train_loss= 2.222   train_acc= 1.000   test_loss=2.343   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.191   train_acc= 1.000   test_loss=2.321   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.154   train_acc= 1.000   test_loss=2.313   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.149   train_acc= 1.000   test_loss=2.301   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.128   train_acc= 1.000   test_loss=2.273   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.125   train_acc= 1.000   test_loss=2.231   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.081   train_acc= 1.000   test_loss=2.216   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.083   train_acc= 1.000   test_loss=2.202   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.067   train_acc= 1.000   test_loss=2.184   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.045   train_acc= 1.000   test_loss=2.167   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.036   train_acc= 1.000   test_loss=2.149   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.036   train_acc= 1.000   test_loss=2.148   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.003   train_acc= 1.000   test_loss=2.130   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.002   train_acc= 1.000   test_loss=2.118   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.980   train_acc= 1.000   test_loss=2.106   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.967   train_acc= 1.000   test_loss=2.089   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.962   train_acc= 1.000   test_loss=2.074   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.948   train_acc= 1.000   test_loss=2.053   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.928   train_acc= 1.000   test_loss=2.039   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.917   train_acc= 1.000   test_loss=2.025   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.895   train_acc= 1.000   test_loss=2.019   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.887   train_acc= 1.000   test_loss=2.008   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.893   train_acc= 1.000   test_loss=1.997   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.872   train_acc= 1.000   test_loss=1.989   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.860   train_acc= 1.000   test_loss=1.978   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.853   train_acc= 1.000   test_loss=1.966   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.838   train_acc= 1.000   test_loss=1.954   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.827   train_acc= 1.000   test_loss=1.948   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.809   train_acc= 1.000   test_loss=1.930   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.803   train_acc= 1.000   test_loss=1.917   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.789   train_acc= 1.000   test_loss=1.904   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.777   train_acc= 1.000   test_loss=1.898   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.780   train_acc= 1.000   test_loss=1.892   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.764   train_acc= 1.000   test_loss=1.884   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.746   train_acc= 1.000   test_loss=1.877   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.751   train_acc= 1.000   test_loss=1.862   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.737   train_acc= 1.000   test_loss=1.853   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.719   train_acc= 1.000   test_loss=1.843   test_acc= 1.000\n",
      "run time: 1.4511677503585816 min\n",
      "test_acc=1.000\n",
      "run= 4   fold= 3\n",
      "epoch= 0   train_loss= 4.522   train_acc= 0.651   test_loss=3.721   test_acc= 0.778\n",
      "epoch= 1   train_loss= 3.244   train_acc= 0.880   test_loss=3.634   test_acc= 0.778\n",
      "epoch= 2   train_loss= 3.191   train_acc= 0.843   test_loss=3.144   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.804   train_acc= 0.940   test_loss=3.274   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.750   train_acc= 0.952   test_loss=3.055   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.628   train_acc= 0.964   test_loss=3.112   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.523   train_acc= 0.976   test_loss=2.855   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.403   train_acc= 1.000   test_loss=2.879   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.351   train_acc= 1.000   test_loss=2.802   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.337   train_acc= 0.988   test_loss=2.749   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.281   train_acc= 1.000   test_loss=2.806   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.296   train_acc= 1.000   test_loss=2.692   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.242   train_acc= 1.000   test_loss=2.704   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.212   train_acc= 1.000   test_loss=2.634   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.197   train_acc= 1.000   test_loss=2.672   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.203   train_acc= 1.000   test_loss=2.669   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.184   train_acc= 1.000   test_loss=2.571   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.158   train_acc= 1.000   test_loss=2.617   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.146   train_acc= 1.000   test_loss=2.559   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.129   train_acc= 1.000   test_loss=2.523   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.122   train_acc= 1.000   test_loss=2.553   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.079   train_acc= 1.000   test_loss=2.542   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.072   train_acc= 1.000   test_loss=2.479   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.048   train_acc= 1.000   test_loss=2.490   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.037   train_acc= 1.000   test_loss=2.454   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.020   train_acc= 1.000   test_loss=2.485   test_acc= 0.889\n",
      "epoch= 26   train_loss= 2.002   train_acc= 1.000   test_loss=2.444   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.996   train_acc= 1.000   test_loss=2.476   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.972   train_acc= 1.000   test_loss=2.431   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.974   train_acc= 1.000   test_loss=2.400   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.951   train_acc= 1.000   test_loss=2.397   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.952   train_acc= 1.000   test_loss=2.399   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.931   train_acc= 1.000   test_loss=2.375   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.906   train_acc= 1.000   test_loss=2.359   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.904   train_acc= 1.000   test_loss=2.396   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.885   train_acc= 1.000   test_loss=2.371   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.876   train_acc= 1.000   test_loss=2.357   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.867   train_acc= 1.000   test_loss=2.363   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.857   train_acc= 1.000   test_loss=2.359   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.843   train_acc= 1.000   test_loss=2.352   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.835   train_acc= 1.000   test_loss=2.346   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.841   train_acc= 1.000   test_loss=2.361   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.814   train_acc= 1.000   test_loss=2.328   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.804   train_acc= 1.000   test_loss=2.296   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.793   train_acc= 1.000   test_loss=2.295   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.780   train_acc= 1.000   test_loss=2.272   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.769   train_acc= 1.000   test_loss=2.267   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.759   train_acc= 1.000   test_loss=2.263   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.750   train_acc= 1.000   test_loss=2.249   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.744   train_acc= 1.000   test_loss=2.290   test_acc= 0.889\n",
      "run time: 1.8011295835177104 min\n",
      "test_acc=0.889\n",
      "run= 4   fold= 4\n",
      "epoch= 0   train_loss= 4.432   train_acc= 0.651   test_loss=3.676   test_acc= 0.667\n",
      "epoch= 1   train_loss= 3.310   train_acc= 0.904   test_loss=3.163   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.996   train_acc= 0.940   test_loss=3.561   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.714   train_acc= 0.976   test_loss=3.297   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.611   train_acc= 0.952   test_loss=3.358   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.540   train_acc= 1.000   test_loss=3.167   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.482   train_acc= 0.988   test_loss=3.325   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.401   train_acc= 1.000   test_loss=3.074   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 8   train_loss= 2.323   train_acc= 1.000   test_loss=3.088   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.328   train_acc= 1.000   test_loss=3.091   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.353   train_acc= 0.988   test_loss=3.253   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.263   train_acc= 0.988   test_loss=3.033   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.225   train_acc= 1.000   test_loss=3.114   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.229   train_acc= 1.000   test_loss=2.975   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.191   train_acc= 1.000   test_loss=3.202   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.176   train_acc= 1.000   test_loss=2.926   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.149   train_acc= 1.000   test_loss=2.945   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.124   train_acc= 1.000   test_loss=2.925   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.116   train_acc= 1.000   test_loss=2.870   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.106   train_acc= 1.000   test_loss=3.021   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.087   train_acc= 1.000   test_loss=2.846   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.102   train_acc= 1.000   test_loss=2.854   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.052   train_acc= 1.000   test_loss=2.806   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.045   train_acc= 1.000   test_loss=2.810   test_acc= 0.778\n",
      "epoch= 24   train_loss= 2.046   train_acc= 1.000   test_loss=2.818   test_acc= 0.889\n",
      "epoch= 25   train_loss= 2.008   train_acc= 1.000   test_loss=2.786   test_acc= 0.778\n",
      "epoch= 26   train_loss= 2.016   train_acc= 1.000   test_loss=2.815   test_acc= 0.889\n",
      "epoch= 27   train_loss= 2.000   train_acc= 1.000   test_loss=2.816   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.983   train_acc= 1.000   test_loss=2.782   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.961   train_acc= 1.000   test_loss=2.827   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.941   train_acc= 1.000   test_loss=2.776   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.934   train_acc= 1.000   test_loss=2.689   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.922   train_acc= 1.000   test_loss=2.733   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.906   train_acc= 1.000   test_loss=2.764   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.909   train_acc= 1.000   test_loss=2.750   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.882   train_acc= 1.000   test_loss=2.771   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.874   train_acc= 1.000   test_loss=2.688   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.872   train_acc= 1.000   test_loss=2.599   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.856   train_acc= 1.000   test_loss=2.624   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.846   train_acc= 1.000   test_loss=2.571   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.827   train_acc= 1.000   test_loss=2.668   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.821   train_acc= 1.000   test_loss=2.666   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.808   train_acc= 1.000   test_loss=2.613   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.802   train_acc= 1.000   test_loss=2.595   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.788   train_acc= 1.000   test_loss=2.581   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.779   train_acc= 1.000   test_loss=2.548   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.769   train_acc= 1.000   test_loss=2.547   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.757   train_acc= 1.000   test_loss=2.603   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.758   train_acc= 1.000   test_loss=2.597   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.739   train_acc= 1.000   test_loss=2.597   test_acc= 0.778\n",
      "run time: 1.2793924450874328 min\n",
      "test_acc=0.778\n",
      "run= 4   fold= 5\n",
      "epoch= 0   train_loss= 4.649   train_acc= 0.711   test_loss=3.392   test_acc= 0.889\n",
      "epoch= 1   train_loss= 3.374   train_acc= 0.867   test_loss=3.182   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.895   train_acc= 0.976   test_loss=2.875   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.729   train_acc= 0.940   test_loss=2.780   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.586   train_acc= 0.964   test_loss=2.641   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.519   train_acc= 0.976   test_loss=2.547   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.493   train_acc= 0.988   test_loss=2.481   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.465   train_acc= 0.988   test_loss=2.425   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.447   train_acc= 0.976   test_loss=2.427   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.299   train_acc= 1.000   test_loss=2.382   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.271   train_acc= 1.000   test_loss=2.341   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.222   train_acc= 1.000   test_loss=2.306   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.214   train_acc= 1.000   test_loss=2.284   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.191   train_acc= 1.000   test_loss=2.274   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.174   train_acc= 1.000   test_loss=2.245   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.135   train_acc= 1.000   test_loss=2.232   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.138   train_acc= 1.000   test_loss=2.205   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.124   train_acc= 1.000   test_loss=2.193   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.095   train_acc= 1.000   test_loss=2.176   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.095   train_acc= 1.000   test_loss=2.157   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.061   train_acc= 1.000   test_loss=2.141   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.047   train_acc= 1.000   test_loss=2.121   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.040   train_acc= 1.000   test_loss=2.113   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.017   train_acc= 1.000   test_loss=2.094   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.012   train_acc= 1.000   test_loss=2.080   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.004   train_acc= 1.000   test_loss=2.072   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.981   train_acc= 1.000   test_loss=2.060   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.979   train_acc= 1.000   test_loss=2.040   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.955   train_acc= 1.000   test_loss=2.028   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.953   train_acc= 1.000   test_loss=2.017   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.931   train_acc= 1.000   test_loss=2.004   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.915   train_acc= 1.000   test_loss=1.991   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.902   train_acc= 1.000   test_loss=1.980   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.900   train_acc= 1.000   test_loss=1.975   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.888   train_acc= 1.000   test_loss=1.963   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.872   train_acc= 1.000   test_loss=1.945   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.867   train_acc= 1.000   test_loss=1.939   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.877   train_acc= 1.000   test_loss=1.937   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.837   train_acc= 1.000   test_loss=1.918   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.834   train_acc= 1.000   test_loss=1.910   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.811   train_acc= 1.000   test_loss=1.896   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.807   train_acc= 1.000   test_loss=1.884   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.793   train_acc= 1.000   test_loss=1.876   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.782   train_acc= 1.000   test_loss=1.861   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.776   train_acc= 1.000   test_loss=1.851   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.762   train_acc= 1.000   test_loss=1.838   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.748   train_acc= 1.000   test_loss=1.831   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.742   train_acc= 1.000   test_loss=1.820   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.730   train_acc= 1.000   test_loss=1.809   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.720   train_acc= 1.000   test_loss=1.798   test_acc= 1.000\n",
      "run time: 1.2144821325937907 min\n",
      "test_acc=1.000\n",
      "run= 4   fold= 6\n",
      "epoch= 0   train_loss= 4.462   train_acc= 0.651   test_loss=3.493   test_acc= 0.889\n",
      "epoch= 1   train_loss= 3.342   train_acc= 0.916   test_loss=3.129   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.950   train_acc= 0.940   test_loss=3.340   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.960   train_acc= 0.928   test_loss=2.976   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 4   train_loss= 2.838   train_acc= 0.952   test_loss=2.817   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.610   train_acc= 0.976   test_loss=2.784   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.515   train_acc= 0.988   test_loss=2.673   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.433   train_acc= 0.988   test_loss=2.621   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.388   train_acc= 0.988   test_loss=2.589   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.388   train_acc= 0.988   test_loss=2.543   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.311   train_acc= 1.000   test_loss=2.457   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.315   train_acc= 0.988   test_loss=2.456   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.260   train_acc= 1.000   test_loss=2.410   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.253   train_acc= 1.000   test_loss=2.398   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.212   train_acc= 1.000   test_loss=2.350   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.198   train_acc= 1.000   test_loss=2.339   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.165   train_acc= 1.000   test_loss=2.306   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.141   train_acc= 1.000   test_loss=2.276   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.106   train_acc= 1.000   test_loss=2.250   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.096   train_acc= 1.000   test_loss=2.217   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.089   train_acc= 1.000   test_loss=2.203   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.074   train_acc= 1.000   test_loss=2.181   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.067   train_acc= 1.000   test_loss=2.158   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.036   train_acc= 1.000   test_loss=2.155   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.031   train_acc= 1.000   test_loss=2.133   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.009   train_acc= 1.000   test_loss=2.131   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.999   train_acc= 1.000   test_loss=2.100   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.975   train_acc= 1.000   test_loss=2.079   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.970   train_acc= 1.000   test_loss=2.073   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.964   train_acc= 1.000   test_loss=2.055   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.941   train_acc= 1.000   test_loss=2.051   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.941   train_acc= 1.000   test_loss=2.024   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.923   train_acc= 1.000   test_loss=2.015   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.906   train_acc= 1.000   test_loss=1.999   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.901   train_acc= 1.000   test_loss=1.981   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.879   train_acc= 1.000   test_loss=1.969   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.871   train_acc= 1.000   test_loss=1.957   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.860   train_acc= 1.000   test_loss=1.948   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.854   train_acc= 1.000   test_loss=1.938   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.839   train_acc= 1.000   test_loss=1.941   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.828   train_acc= 1.000   test_loss=1.926   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.815   train_acc= 1.000   test_loss=1.921   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.811   train_acc= 1.000   test_loss=1.895   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.801   train_acc= 1.000   test_loss=1.890   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.783   train_acc= 1.000   test_loss=1.873   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.776   train_acc= 1.000   test_loss=1.858   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.769   train_acc= 1.000   test_loss=1.850   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.764   train_acc= 1.000   test_loss=1.849   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.742   train_acc= 1.000   test_loss=1.837   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.735   train_acc= 1.000   test_loss=1.825   test_acc= 1.000\n",
      "run time: 1.2809348503748577 min\n",
      "test_acc=1.000\n",
      "run= 4   fold= 7\n",
      "epoch= 0   train_loss= 4.412   train_acc= 0.687   test_loss=4.023   test_acc= 0.778\n",
      "epoch= 1   train_loss= 3.259   train_acc= 0.940   test_loss=3.756   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.875   train_acc= 0.952   test_loss=3.818   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.691   train_acc= 0.964   test_loss=3.578   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.579   train_acc= 0.976   test_loss=3.675   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.438   train_acc= 1.000   test_loss=3.598   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.383   train_acc= 1.000   test_loss=3.532   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.389   train_acc= 1.000   test_loss=3.415   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.339   train_acc= 1.000   test_loss=3.522   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.329   train_acc= 1.000   test_loss=3.496   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.273   train_acc= 1.000   test_loss=3.357   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.263   train_acc= 1.000   test_loss=3.367   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.219   train_acc= 1.000   test_loss=3.377   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.188   train_acc= 1.000   test_loss=3.436   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.176   train_acc= 1.000   test_loss=3.329   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.157   train_acc= 1.000   test_loss=3.331   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.134   train_acc= 1.000   test_loss=3.266   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.120   train_acc= 1.000   test_loss=3.356   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.114   train_acc= 1.000   test_loss=3.317   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.088   train_acc= 1.000   test_loss=3.270   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.071   train_acc= 1.000   test_loss=3.268   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.060   train_acc= 1.000   test_loss=3.251   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.047   train_acc= 1.000   test_loss=3.238   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.036   train_acc= 1.000   test_loss=3.188   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.015   train_acc= 1.000   test_loss=3.136   test_acc= 0.889\n",
      "epoch= 25   train_loss= 2.000   train_acc= 1.000   test_loss=3.175   test_acc= 0.889\n",
      "epoch= 26   train_loss= 2.008   train_acc= 1.000   test_loss=3.184   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.981   train_acc= 1.000   test_loss=3.151   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.968   train_acc= 1.000   test_loss=3.117   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.965   train_acc= 1.000   test_loss=3.103   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.935   train_acc= 1.000   test_loss=3.079   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.924   train_acc= 1.000   test_loss=3.107   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.908   train_acc= 1.000   test_loss=3.079   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.895   train_acc= 1.000   test_loss=3.083   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.891   train_acc= 1.000   test_loss=3.106   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.876   train_acc= 1.000   test_loss=3.009   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.876   train_acc= 1.000   test_loss=3.070   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.854   train_acc= 1.000   test_loss=3.113   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.842   train_acc= 1.000   test_loss=3.030   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.831   train_acc= 1.000   test_loss=2.987   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.823   train_acc= 1.000   test_loss=2.958   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.815   train_acc= 1.000   test_loss=3.016   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.798   train_acc= 1.000   test_loss=3.000   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.784   train_acc= 1.000   test_loss=2.977   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.777   train_acc= 1.000   test_loss=2.947   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.767   train_acc= 1.000   test_loss=2.924   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.759   train_acc= 1.000   test_loss=2.926   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.748   train_acc= 1.000   test_loss=2.925   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.738   train_acc= 1.000   test_loss=2.955   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.732   train_acc= 1.000   test_loss=2.914   test_acc= 0.889\n",
      "run time: 1.4041141788164775 min\n",
      "test_acc=0.889\n",
      "run= 4   fold= 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0   train_loss= 4.349   train_acc= 0.675   test_loss=4.071   test_acc= 0.778\n",
      "epoch= 1   train_loss= 3.517   train_acc= 0.855   test_loss=3.668   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.958   train_acc= 0.952   test_loss=3.866   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.739   train_acc= 0.952   test_loss=3.498   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.633   train_acc= 0.976   test_loss=3.398   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.480   train_acc= 0.988   test_loss=3.388   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.428   train_acc= 0.988   test_loss=3.458   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.378   train_acc= 1.000   test_loss=3.345   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.311   train_acc= 1.000   test_loss=3.404   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.335   train_acc= 1.000   test_loss=3.350   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.269   train_acc= 1.000   test_loss=3.317   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.243   train_acc= 1.000   test_loss=3.404   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.228   train_acc= 1.000   test_loss=3.395   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.209   train_acc= 1.000   test_loss=3.336   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.211   train_acc= 1.000   test_loss=3.321   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.157   train_acc= 1.000   test_loss=3.304   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.132   train_acc= 1.000   test_loss=3.333   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.131   train_acc= 1.000   test_loss=3.327   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.121   train_acc= 1.000   test_loss=3.282   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.104   train_acc= 1.000   test_loss=3.349   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.076   train_acc= 1.000   test_loss=3.344   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.056   train_acc= 1.000   test_loss=3.301   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.047   train_acc= 1.000   test_loss=3.303   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.025   train_acc= 1.000   test_loss=3.291   test_acc= 0.778\n",
      "epoch= 24   train_loss= 2.022   train_acc= 1.000   test_loss=3.298   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.998   train_acc= 1.000   test_loss=3.275   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.987   train_acc= 1.000   test_loss=3.308   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.976   train_acc= 1.000   test_loss=3.305   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.962   train_acc= 1.000   test_loss=3.309   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.943   train_acc= 1.000   test_loss=3.282   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.928   train_acc= 1.000   test_loss=3.268   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.923   train_acc= 1.000   test_loss=3.241   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.912   train_acc= 1.000   test_loss=3.240   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.900   train_acc= 1.000   test_loss=3.278   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.900   train_acc= 1.000   test_loss=3.284   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.873   train_acc= 1.000   test_loss=3.251   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.862   train_acc= 1.000   test_loss=3.226   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.847   train_acc= 1.000   test_loss=3.220   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.843   train_acc= 1.000   test_loss=3.123   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.826   train_acc= 1.000   test_loss=3.119   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.818   train_acc= 1.000   test_loss=3.114   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.809   train_acc= 1.000   test_loss=3.130   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.793   train_acc= 1.000   test_loss=3.124   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.790   train_acc= 1.000   test_loss=3.105   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.780   train_acc= 1.000   test_loss=3.104   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.771   train_acc= 1.000   test_loss=3.102   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.752   train_acc= 1.000   test_loss=3.110   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.743   train_acc= 1.000   test_loss=3.100   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.741   train_acc= 1.000   test_loss=3.081   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.731   train_acc= 1.000   test_loss=3.076   test_acc= 0.778\n",
      "run time: 1.396974003314972 min\n",
      "test_acc=0.778\n",
      "run= 4   fold= 9\n",
      "epoch= 0   train_loss= 4.377   train_acc= 0.699   test_loss=4.119   test_acc= 0.556\n",
      "epoch= 1   train_loss= 3.430   train_acc= 0.892   test_loss=3.770   test_acc= 0.778\n",
      "epoch= 2   train_loss= 3.032   train_acc= 0.940   test_loss=3.515   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.811   train_acc= 0.964   test_loss=3.562   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.602   train_acc= 0.964   test_loss=3.408   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.544   train_acc= 0.976   test_loss=3.385   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.424   train_acc= 1.000   test_loss=3.407   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.405   train_acc= 0.988   test_loss=3.378   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.326   train_acc= 0.988   test_loss=3.311   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.309   train_acc= 1.000   test_loss=3.187   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.261   train_acc= 1.000   test_loss=3.221   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.254   train_acc= 1.000   test_loss=3.217   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.232   train_acc= 1.000   test_loss=3.128   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.206   train_acc= 1.000   test_loss=3.106   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.189   train_acc= 1.000   test_loss=3.102   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.160   train_acc= 1.000   test_loss=3.093   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.147   train_acc= 1.000   test_loss=3.077   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.126   train_acc= 1.000   test_loss=3.079   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.105   train_acc= 1.000   test_loss=3.078   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.088   train_acc= 1.000   test_loss=3.075   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.080   train_acc= 1.000   test_loss=3.082   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.070   train_acc= 1.000   test_loss=3.066   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.054   train_acc= 1.000   test_loss=3.023   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.032   train_acc= 1.000   test_loss=2.983   test_acc= 0.778\n",
      "epoch= 24   train_loss= 2.009   train_acc= 1.000   test_loss=2.983   test_acc= 0.778\n",
      "epoch= 25   train_loss= 2.004   train_acc= 1.000   test_loss=2.968   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.993   train_acc= 1.000   test_loss=2.922   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.969   train_acc= 1.000   test_loss=2.906   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.961   train_acc= 1.000   test_loss=2.893   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.947   train_acc= 1.000   test_loss=2.883   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.932   train_acc= 1.000   test_loss=2.871   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.925   train_acc= 1.000   test_loss=2.850   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.910   train_acc= 1.000   test_loss=2.854   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.909   train_acc= 1.000   test_loss=2.888   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.888   train_acc= 1.000   test_loss=2.880   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.878   train_acc= 1.000   test_loss=2.835   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.863   train_acc= 1.000   test_loss=2.832   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.851   train_acc= 1.000   test_loss=2.822   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.850   train_acc= 1.000   test_loss=2.835   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.831   train_acc= 1.000   test_loss=2.809   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.820   train_acc= 1.000   test_loss=2.796   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.823   train_acc= 1.000   test_loss=2.743   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.805   train_acc= 1.000   test_loss=2.733   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.791   train_acc= 1.000   test_loss=2.721   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.782   train_acc= 1.000   test_loss=2.723   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.765   train_acc= 1.000   test_loss=2.698   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 46   train_loss= 1.758   train_acc= 1.000   test_loss=2.699   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.747   train_acc= 1.000   test_loss=2.683   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.737   train_acc= 1.000   test_loss=2.665   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.737   train_acc= 1.000   test_loss=2.669   test_acc= 0.778\n",
      "run time: 1.343178351720174 min\n",
      "test_acc=0.778\n",
      "MI-Net with DS mean accuracy =  0.8822222262620926\n",
      "std =  0.12047231613055681\n"
     ]
    }
   ],
   "source": [
    "# perform five times 10-fold cross=validation experiments\n",
    "run = 5\n",
    "n_folds = 10\n",
    "acc = np.zeros((run, n_folds), dtype=float)\n",
    "for irun in range(run):\n",
    "    dataset = load_dataset('musk1', n_folds)\n",
    "    for ifold in range(n_folds):\n",
    "        print('run=', irun, '  fold=', ifold)\n",
    "        acc[irun][ifold] = MI_Net_with_DS(dataset[ifold])\n",
    "print('MI-Net with DS mean accuracy = ', np.mean(acc))\n",
    "print('std = ', np.std(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faWw_6VIy8hr"
   },
   "source": [
    "*CkNN*\n",
    "\n",
    "https://github.com/chlorochrule/cknn/blob/master/cknn/cknn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "111HAmcXzfaI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X = pd.read_table(\"./clean2.data\") #pd.read_csv(\"sample_data/mnist_test.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJ5Z_R-2zCgc"
   },
   "outputs": [],
   "source": [
    "from cknn import cknneighbors_graph\n",
    "\n",
    "#ckng = cknneighbors_graph(X, n_neighbors=5, delta=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "yqlyFu8TzfyT",
    "outputId": "f1ca7a87-35da-4542-e15c-8fa0d32cb873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     MUSK-211,211_1+1,46,-108,-60,-69,-117,49,38,-161,-8,5,-323,-220,-113,-299,-283,-307,-31,-106,-227,-42,-59,-22,-67,189,81,17,-27,-89,-67,105,-116,124,-106,5,-120,63,-165,40,-27,68,-44,98,-33,-314,-282,-335,-144,-13,-197,-2,-144,-13,-11,-131,108,-43,42,-151,-4,8,-102,51,-15,108,-135,59,-166,20,-20,23,-48,-68,-299,-256,-97,-183,-24,-271,-229,-177,-6,0,-129,112,15,36,-66,-54,-75,132,-188,119,-120,-312,23,-55,-53,-26,-71,41,-55,148,-247,-306,-308,-230,-166,-35,-205,-280,-239,-53,-10,-23,25,-5,163,61,59,-39,92,72,113,-107,80,25,-27,81,-114,-187,45,-118,-75,-182,-234,-19,12,-13,-41,-119,-149,70,17,-20,-177,-101,-116,-14,-50,24,-81,-125,-114,-44,128,3,-244,-308,52,-7,39,126,156,-50,-112,96,1.\n",
      "0     MUSK-211,211_1+10,41,-188,-145,22,-117,-6,57,-...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "1     MUSK-211,211_1+11,46,-194,-145,28,-117,73,57,-...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "2     MUSK-211,211_1+12,41,-188,-145,22,-117,-7,57,-...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "3     MUSK-211,211_1+13,41,-188,-145,22,-117,-7,57,-...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "4     MUSK-211,211_1+14,46,-194,-145,28,-117,72,57,-...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "...                                                 ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "6592  NON-MUSK-jp13,jp13_2+5,51,-123,-23,-108,-117,1...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "6593  NON-MUSK-jp13,jp13_2+6,44,-104,-19,-105,-117,1...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "6594  NON-MUSK-jp13,jp13_2+7,44,-102,-19,-104,-117,7...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "6595  NON-MUSK-jp13,jp13_2+8,51,-121,-23,-106,-117,6...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "6596  NON-MUSK-jp13,jp13_2+9,51,-122,-23,-106,-117,1...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "\n",
      "[6597 rows x 1 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-43ee63d835e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m#plot2d_label(y_cknn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-43ee63d835e0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mmodel_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpectralEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0my_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mplot2d_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_normal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_spectral_embedding.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mSpectral\u001b[0m \u001b[0membedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \"\"\"\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_spectral_embedding.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         X = self._validate_data(\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         )\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNpDtype\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m     def __array_wrap__(\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'MUSK-211,211_1+10,41,-188,-145,22,-117,-6,57,-171,-39,-100,-319,-111,-228,-281,-281,-300,54,-149,-98,-196,-27,-22,2,75,49,-34,45,-91,32,95,-116,85,-23,42,-58,61,-171,2,-144,38,-153,113,-166,-318,-241,-329,-97,-69,-108,-179,-71,-27,-12,-133,107,-96,92,-140,48,26,-62,2,13,58,-12,59,-166,-85,-131,-57,-156,-121,-285,-189,-255,-181,2,-284,-103,-186,-18,15,-31,127,-5,24,-82,-168,10,79,-200,82,14,-101,28,-52,-43,31,-156,79,-158,137,-281,-305,-294,-262,-165,-117,-244,-246,-231,3,-2,-3,15,49,99,37,84,22,66,131,109,-77,-10,-17,17,88,-21,-32,32,-128,-72,-124,-218,-94,53,-79,-20,-35,-26,4,50,17,-177,-102,-121,-66,-77,51,-41,-34,-32,-63,115,-5,-235,-59,-2,52,103,136,169,-61,-136,79,1.'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "import seaborn as sns\n",
    "\n",
    "from cknn import cknneighbors_graph\n",
    "\n",
    "sns.set()\n",
    "\n",
    "\n",
    "def plot2d_label(X, title=None):\n",
    "    digits = load_digits()\n",
    "    y = digits.target\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(digits.target[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = X\n",
    "    print(data)\n",
    "    n_neighbors = 2\n",
    "\n",
    "    model_normal = SpectralEmbedding(n_components=2, n_neighbors=n_neighbors)\n",
    "    y_normal = model_normal.fit_transform(data)\n",
    "    plot2d_label(y_normal)\n",
    "\n",
    "    #ckng = cknneighbors_graph(data, n_neighbors=n_neighbors, delta=1.5)\n",
    "    #model_cknn = SpectralEmbedding(n_components=2, affinity='precomputed')\n",
    "    #y_cknn = model_cknn.fit_transform(ckng.toarray())\n",
    "    #plot2d_label(y_cknn)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETGD3s42kNwx",
    "outputId": "677c1083-bc60-4941-e623-348c7604e1d7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m#plot2d_label(y_cknn)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(y_cknn)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [1], line 25\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m---> 25\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\n\u001b[1;32m     26\u001b[0m     n_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     28\u001b[0m     model_normal \u001b[38;5;241m=\u001b[39m SpectralEmbedding(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "\n",
    "def plot2d_label(X, title=None):\n",
    "    y = X[1]\n",
    "    x_min, x_max = np.min(X[0], 0), np.max(X[0], 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = X\n",
    "    n_neighbors = 10\n",
    "\n",
    "    model_normal = SpectralEmbedding(n_components=2, n_neighbors=n_neighbors)\n",
    "    y_normal = model_normal.fit_transform(data)\n",
    "    #plot2d_label(y_normal)\n",
    "\n",
    "    ckng = cknneighbors_graph(data, n_neighbors=n_neighbors, delta=1.5)\n",
    "    model_cknn = SpectralEmbedding(n_components=2, affinity='precomputed')\n",
    "    y_cknn = model_cknn.fit_transform(ckng.toarray())\n",
    "    #plot2d_label(y_cknn)\n",
    "    print(y_cknn)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcgREL30sCJL"
   },
   "source": [
    "## instance-Space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MI-SVM and mi-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import misvm\n",
    "from misvmio import parse_c45, bag_set\n",
    "from __future__ import print_function, division\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 189, 0.802929, 0.180789, 1.85438, -0.916105, -0.325939, -0.325438, 1.81043, -0.969697, -0.318149, 0.712379, 0.775048, -8.8E-5, -0.054535, -0.297912, -0.261335, -0.29087, -0.225084, -0.014952, -0.08711, -0.167402, -0.169238, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.165093, -0.315465, -0.426341, -0.278355, -0.025787, -0.073582, -0.113664, -0.113083, -0.139197, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.082166, 0.00209, -0.498543, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.143036, -0.105975, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.29613, 2.01902, -0.474142, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.698127, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.314025, -0.349996, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"189, 0.802929, 0.180789, 1.85438, -0.916105, -0.325939, -0.325438, 1.81043, -0.969697, -0.318149, 0.712379, 0.775048, -8.8E-5, -0.054535, -0.297912, -0.261335, -0.29087, -0.225084, -0.014952, -0.08711, -0.167402, -0.169238, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.165093, -0.315465, -0.426341, -0.278355, -0.025787, -0.073582, -0.113664, -0.113083, -0.139197, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.082166, 0.00209, -0.498543, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.143036, -0.105975, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.29613, 2.01902, -0.474142, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.698127, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.314025, -0.349996, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 189, -1.15024, -1.01615, 1.36347, 0.079629, -0.538715, 2.46169, 1.35814, 0.046101, -0.584217, 0.885653, 0.945363, 0.344416, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.516617, -0.429993, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.105479, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.042775, -0.521605, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.07056, -0.068808, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 2.34814, -0.469253, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 1.53736, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.230092, -0.397792, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"189, -1.15024, -1.01615, 1.36347, 0.079629, -0.538715, 2.46169, 1.35814, 0.046101, -0.584217, 0.885653, 0.945363, 0.344416, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.516617, -0.429993, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.105479, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.042775, -0.521605, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.07056, -0.068808, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 2.34814, -0.469253, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 1.53736, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.230092, -0.397792, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 189, -0.18056, -0.061882, 0.525657, 0.460321, -0.603603, 0.477358, 0.421007, 0.486145, -0.623879, 0.73112, 1.07933, 0.241866, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.932403, -0.587641, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.17138, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.60979, -0.640679, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.140921, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.51348, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"189, -0.18056, -0.061882, 0.525657, 0.460321, -0.603603, 0.477358, 0.421007, 0.486145, -0.623879, 0.73112, 1.07933, 0.241866, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.932403, -0.587641, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.17138, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.60979, -0.640679, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.140921, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.51348, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 189, 1.2107, 1.29128, -0.873116, -1.59389, -0.750058, -1.81529, -0.886541, -1.62462, -0.759723, -0.126133, 0.402964, -1.0924, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.568544, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.97147, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.1293, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.13366, -0.406175, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"189, 1.2107, 1.29128, -0.873116, -1.59389, -0.750058, -1.81529, -0.886541, -1.62462, -0.759723, -0.126133, 0.402964, -1.0924, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.568544, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.97147, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.1293, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.13366, -0.406175, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 126, -1.2933, -1.20441, 0.396129, 0.716598, 1.35005, -0.961135, 0.367535, 0.624256, 1.62597, 1.60588, 0.159415, -0.861614, -0.054535, -0.297912, -0.220779, -0.011808, -0.093668, -0.014952, -0.08711, -0.167402, -0.181293, 0.35909, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.074168, -0.140373, -0.347995, -0.26576, -0.080866, -0.176579, 0.128938, -0.333912, -0.309461, -0.025787, -0.073582, -0.114571, -0.156674, -0.141618, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.105588, -0.216754, -0.406361, -0.263784, -0.139878, 0.0, -0.06989, -0.141007, 0.173178, -0.560363, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.16696, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.253202, -0.391181, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.015257, -0.115709, -0.596044, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.169091, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 0.388796, 1.55882, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"126, -1.2933, -1.20441, 0.396129, 0.716598, 1.35005, -0.961135, 0.367535, 0.624256, 1.62597, 1.60588, 0.159415, -0.861614, -0.054535, -0.297912, -0.220779, -0.011808, -0.093668, -0.014952, -0.08711, -0.167402, -0.181293, 0.35909, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.074168, -0.140373, -0.347995, -0.26576, -0.080866, -0.176579, 0.128938, -0.333912, -0.309461, -0.025787, -0.073582, -0.114571, -0.156674, -0.141618, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.105588, -0.216754, -0.406361, -0.263784, -0.139878, 0.0, -0.06989, -0.141007, 0.173178, -0.560363, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.16696, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.253202, -0.391181, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.015257, -0.115709, -0.596044, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.169091, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 0.388796, 1.55882, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 126, 0.395013, 0.411134, 1.20124, -1.14003, 0.36742, -0.205211, 0.936274, -1.18146, 0.342891, 0.688549, 0.182907, 0.155365, -0.054535, -0.297912, -0.252144, -0.212198, -0.177595, -0.014952, -0.08711, -0.167402, -0.200329, -0.008037, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.304468, -0.347995, -0.26576, -0.080866, -0.218533, -0.389817, -0.355026, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.148703, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.259781, -0.449132, -0.53935, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.093788, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.233855, 0.72244, -0.560756, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.243104, -0.190422, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 0.633242, 1.55313, -0.380619, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 0.46486, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"126, 0.395013, 0.411134, 1.20124, -1.14003, 0.36742, -0.205211, 0.936274, -1.18146, 0.342891, 0.688549, 0.182907, 0.155365, -0.054535, -0.297912, -0.252144, -0.212198, -0.177595, -0.014952, -0.08711, -0.167402, -0.200329, -0.008037, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.304468, -0.347995, -0.26576, -0.080866, -0.218533, -0.389817, -0.355026, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.148703, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.259781, -0.449132, -0.53935, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.093788, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.233855, 0.72244, -0.560756, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.243104, -0.190422, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 0.633242, 1.55313, -0.380619, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 0.46486, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 126, 1.35124, 1.60929, 0.484712, -0.220993, -0.1206, 0.682554, 0.482247, -0.299439, -0.296004, -0.359694, 0.530799, -0.091721, -0.054535, -0.297912, -0.241405, -0.244743, -0.22196, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.13747, -0.321942, -0.264382, -0.080866, -0.203295, -0.362501, -0.203011, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.325617, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.260874, -0.04238, -0.458617, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.253561, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.363255, -0.210655, -0.341493, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 0.383884, 1.99511, -0.379276, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"126, 1.35124, 1.60929, 0.484712, -0.220993, -0.1206, 0.682554, 0.482247, -0.299439, -0.296004, -0.359694, 0.530799, -0.091721, -0.054535, -0.297912, -0.241405, -0.244743, -0.22196, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.13747, -0.321942, -0.264382, -0.080866, -0.203295, -0.362501, -0.203011, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.325617, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.260874, -0.04238, -0.458617, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.253561, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.363255, -0.210655, -0.341493, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 0.383884, 1.99511, -0.379276, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 126, 0.768553, 1.24362, -0.988455, -0.735925, -0.108063, 0.704008, -0.939024, -0.822734, -0.309555, -0.654208, 0.156875, -0.18155, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.430533, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.239431, -0.746585, -0.584256, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.293107, -0.744474, -0.580084, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 2.16063, 2.69263, -0.410064, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"126, 0.768553, 1.24362, -0.988455, -0.735925, -0.108063, 0.704008, -0.939024, -0.822734, -0.309555, -0.654208, 0.156875, -0.18155, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.430533, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.239431, -0.746585, -0.584256, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.293107, -0.744474, -0.580084, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 2.16063, 2.69263, -0.410064, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 126, -0.508229, -0.271775, -0.482869, 0.030202, -0.416691, 1.02507, -0.393298, -0.109585, -0.422593, 0.722607, 1.14255, 0.065482, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.295506, -0.347995, -0.26576, -0.080866, -0.224814, -0.528029, -0.447674, -0.187363, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.085338, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.234864, -0.327098, -0.421119, -0.230675, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.227631, -0.382524, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.362909, 0.132744, 0.188615, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 0.249242, 1.62419, -0.352326, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"126, -0.508229, -0.271775, -0.482869, 0.030202, -0.416691, 1.02507, -0.393298, -0.109585, -0.422593, 0.722607, 1.14255, 0.065482, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.295506, -0.347995, -0.26576, -0.080866, -0.224814, -0.528029, -0.447674, -0.187363, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.085338, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.234864, -0.327098, -0.421119, -0.230675, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.227631, -0.382524, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.362909, 0.132744, 0.188615, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 0.249242, 1.62419, -0.352326, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 126, 0.669696, 0.808878, -0.680719, -1.19102, -0.759973, -0.50464, -0.816718, -1.31573, -0.764681, 0.962647, 0.008446, 0.218488, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.355814, -1.03773, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 1.4435, 3.21555, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"126, 0.669696, 0.808878, -0.680719, -1.19102, -0.759973, -0.50464, -0.816718, -1.31573, -0.764681, 0.962647, 0.008446, 0.218488, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.355814, -1.03773, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 1.4435, 3.21555, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 126, -1.00969, -0.906052, -0.739504, -1.54675, -0.823607, 0.154057, -0.751781, -1.57733, -0.845989, 1.78928, 0.20081, -0.322653, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.196192, -0.613368, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.50751, -0.214553, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 1.37564, 1.59992, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"126, -1.00969, -0.906052, -0.739504, -1.54675, -0.823607, 0.154057, -0.751781, -1.57733, -0.845989, 1.78928, 0.20081, -0.322653, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.196192, -0.613368, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.50751, -0.214553, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 1.37564, 1.59992, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 126, -0.693705, -0.506487, -1.67416, -1.4566, -0.843177, 2.10441, -1.72749, -1.48169, -0.867142, 1.78233, 0.216973, -0.311508, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.169283, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.185906, -0.753714, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.770585, -0.717842, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 1.74567, 2.31584, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"126, -0.693705, -0.506487, -1.67416, -1.4566, -0.843177, 2.10441, -1.72749, -1.48169, -0.867142, 1.78233, 0.216973, -0.311508, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.169283, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.185906, -0.753714, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.770585, -0.717842, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 1.74567, 2.31584, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 111, 0.193163, -0.499417, -0.531338, 0.78599, 1.16694, -0.414156, -0.413427, 0.780277, 0.91469, -0.831642, -0.854079, -0.960369, -0.054535, -0.2751, -0.080361, -0.241803, -0.225084, -0.014952, 0.314552, 0.907466, -0.068477, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 2.29974, 0.329235, 0.322295, 3.96628, 0.264107, -0.26576, -0.062763, 1.65056, 0.432378, -0.435138, -0.309461, -0.025787, -0.073582, 0.204428, 0.033895, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 0.717732, -0.072645, -0.091775, -0.050171, -0.114846, 2.14181, 0.197944, -0.263784, -0.139878, 0.0, -0.06989, 0.697415, 0.119814, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.2033, -0.375005, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.65832, -1.01784, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.022952, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.247179, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"111, 0.193163, -0.499417, -0.531338, 0.78599, 1.16694, -0.414156, -0.413427, 0.780277, 0.91469, -0.831642, -0.854079, -0.960369, -0.054535, -0.2751, -0.080361, -0.241803, -0.225084, -0.014952, 0.314552, 0.907466, -0.068477, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 2.29974, 0.329235, 0.322295, 3.96628, 0.264107, -0.26576, -0.062763, 1.65056, 0.432378, -0.435138, -0.309461, -0.025787, -0.073582, 0.204428, 0.033895, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 0.717732, -0.072645, -0.091775, -0.050171, -0.114846, 2.14181, 0.197944, -0.263784, -0.139878, 0.0, -0.06989, 0.697415, 0.119814, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.2033, -0.375005, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.65832, -1.01784, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.022952, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.247179, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 111, -0.595111, -0.63658, 1.34009, -0.906862, 0.849286, -0.299069, 1.1671, -0.875255, 0.65391, -0.608617, -2.56686, 0.026007, -0.054535, -0.292984, -0.258644, -0.302051, -0.225084, -0.014952, -0.075487, -0.155239, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.517787, -0.117633, 0.151444, 1.26363, -0.188542, -0.26576, -0.032953, 0.538098, -0.420878, -0.448434, -0.309461, -0.025787, -0.073582, -0.096986, -0.026164, -0.112049, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, 0.018549, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 0.240637, -0.154609, -0.091775, -0.050171, -0.114846, 2.46753, -0.092807, -0.263784, -0.139878, 0.0, -0.068591, 6.38696, 0.404297, -0.58229, -0.260113, -0.102253, 0.0, 0.0, -0.040796, 3.44E-4, 0.084951, -0.166399, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, 0.022593, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.121949, -0.388535, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.556862, -0.928817, -0.65168, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.05331, 0.291437, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.241581, -0.67972, -0.374399, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 1.52802, 0.120308, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"111, -0.595111, -0.63658, 1.34009, -0.906862, 0.849286, -0.299069, 1.1671, -0.875255, 0.65391, -0.608617, -2.56686, 0.026007, -0.054535, -0.292984, -0.258644, -0.302051, -0.225084, -0.014952, -0.075487, -0.155239, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.517787, -0.117633, 0.151444, 1.26363, -0.188542, -0.26576, -0.032953, 0.538098, -0.420878, -0.448434, -0.309461, -0.025787, -0.073582, -0.096986, -0.026164, -0.112049, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, 0.018549, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 0.240637, -0.154609, -0.091775, -0.050171, -0.114846, 2.46753, -0.092807, -0.263784, -0.139878, 0.0, -0.068591, 6.38696, 0.404297, -0.58229, -0.260113, -0.102253, 0.0, 0.0, -0.040796, 3.44E-4, 0.084951, -0.166399, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, 0.022593, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.121949, -0.388535, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.556862, -0.928817, -0.65168, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.05331, 0.291437, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.241581, -0.67972, -0.374399, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 1.52802, 0.120308, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 111, -0.829753, -0.751141, -0.922066, -1.07082, 0.170402, 0.251124, -0.620761, -1.04205, 0.198123, -0.136926, 0.164667, -2.11385, -0.054535, -0.28716, -0.253509, -0.302051, -0.225084, -0.014952, -0.002067, -0.12464, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.674329, -0.153132, 0.611948, 2.54279, -0.102975, -0.26576, -0.070437, 1.20766, -0.214557, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.147897, -0.173978, -0.091775, -0.050171, -0.114459, 1.70998, -0.209106, -0.263784, -0.139878, 0.0, -0.06989, 5.08662, 0.319794, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.180997, -0.391265, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.59182, -0.969458, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.249696, -0.745725, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"111, -0.829753, -0.751141, -0.922066, -1.07082, 0.170402, 0.251124, -0.620761, -1.04205, 0.198123, -0.136926, 0.164667, -2.11385, -0.054535, -0.28716, -0.253509, -0.302051, -0.225084, -0.014952, -0.002067, -0.12464, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.674329, -0.153132, 0.611948, 2.54279, -0.102975, -0.26576, -0.070437, 1.20766, -0.214557, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.147897, -0.173978, -0.091775, -0.050171, -0.114459, 1.70998, -0.209106, -0.263784, -0.139878, 0.0, -0.06989, 5.08662, 0.319794, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.180997, -0.391265, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.59182, -0.969458, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.249696, -0.745725, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 111, 0.895884, 1.1739, -0.084508, -0.132296, 0.127889, -0.91476, -0.39062, -0.144646, 0.0051, 0.34192, 1.27433, -0.294806, -0.054535, -0.267178, -0.260672, -0.302051, -0.225084, -0.014952, -0.04506, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 9.28217, 1.42721, -0.032077, 1.56454, 1.28762, -0.134094, -0.080866, 0.093429, -0.47454, -0.451394, -0.309461, -0.025787, -0.073582, -0.077648, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 1.79079, 0.009402, -0.091775, -0.050171, -0.114846, 2.5651, 1.19094, -0.263784, -0.139878, 0.0, -0.06989, 0.331324, -0.501154, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 0.053255, -0.348758, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 4.00323, -1.00281, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.08695, -0.744592, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"111, 0.895884, 1.1739, -0.084508, -0.132296, 0.127889, -0.91476, -0.39062, -0.144646, 0.0051, 0.34192, 1.27433, -0.294806, -0.054535, -0.267178, -0.260672, -0.302051, -0.225084, -0.014952, -0.04506, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 9.28217, 1.42721, -0.032077, 1.56454, 1.28762, -0.134094, -0.080866, 0.093429, -0.47454, -0.451394, -0.309461, -0.025787, -0.073582, -0.077648, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 1.79079, 0.009402, -0.091775, -0.050171, -0.114846, 2.5651, 1.19094, -0.263784, -0.139878, 0.0, -0.06989, 0.331324, -0.501154, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 0.053255, -0.348758, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 4.00323, -1.00281, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.08695, -0.744592, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 111, 0.683944, 0.503637, 1.44738, 0.404571, -0.144034, -0.666094, 1.42064, 0.254255, -0.142643, -1.30424, 0.912528, -2.199, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.477997, 0.101814, 0.033619, 0.618214, -0.224257, -0.26576, -0.080866, 0.130993, -0.448403, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, 0.087335, 0.121663, -0.153336, -0.091775, -0.050171, -0.106997, 2.47584, -0.134241, -0.263784, -0.139878, 0.0, -0.061749, 2.14376, 0.570468, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.027704, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.07878, -0.384936, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.70453, -0.874843, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -9.62E-4, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.221, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"111, 0.683944, 0.503637, 1.44738, 0.404571, -0.144034, -0.666094, 1.42064, 0.254255, -0.142643, -1.30424, 0.912528, -2.199, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.477997, 0.101814, 0.033619, 0.618214, -0.224257, -0.26576, -0.080866, 0.130993, -0.448403, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, 0.087335, 0.121663, -0.153336, -0.091775, -0.050171, -0.106997, 2.47584, -0.134241, -0.263784, -0.139878, 0.0, -0.061749, 2.14376, 0.570468, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.027704, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.07878, -0.384936, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.70453, -0.874843, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -9.62E-4, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.221, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 188, -0.071983, -0.257217, 0.92372, 0.596613, 0.958844, -0.987063, 0.683852, 0.678114, 0.817517, 0.343013, 0.243194, -0.353242, -0.054535, -0.297912, 1.02344, 0.413842, 0.028127, -0.014952, -0.08711, -0.167402, -0.200329, -0.22328, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.015378, -0.345565, -0.26576, -0.080866, -0.198457, 0.491183, -0.328052, -0.259009, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.089004, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.4399, -0.502963, -0.253708, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.41227, -0.651447, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.171909, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.421123, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"188, -0.071983, -0.257217, 0.92372, 0.596613, 0.958844, -0.987063, 0.683852, 0.678114, 0.817517, 0.343013, 0.243194, -0.353242, -0.054535, -0.297912, 1.02344, 0.413842, 0.028127, -0.014952, -0.08711, -0.167402, -0.200329, -0.22328, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.015378, -0.345565, -0.26576, -0.080866, -0.198457, 0.491183, -0.328052, -0.259009, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.089004, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.4399, -0.502963, -0.253708, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.41227, -0.651447, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.171909, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.421123, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 188, 1.05224, 0.978661, -1.19715, -0.666583, 0.216685, 1.69319, -1.06615, -0.585164, 0.0755, -0.77758, -0.128107, -0.006913, -0.054535, -0.297912, 4.06724, 0.110345, 0.03164, -0.014952, -0.08711, -0.167402, -0.200329, -0.227768, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.020685, 0.380772, -0.310596, -0.26576, -0.080866, 3.17507, 2.2028, 0.353139, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.228928, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.253137, 0.430171, -0.573421, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.166539, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.258903, -0.653375, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.532087, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"188, 1.05224, 0.978661, -1.19715, -0.666583, 0.216685, 1.69319, -1.06615, -0.585164, 0.0755, -0.77758, -0.128107, -0.006913, -0.054535, -0.297912, 4.06724, 0.110345, 0.03164, -0.014952, -0.08711, -0.167402, -0.200329, -0.227768, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.020685, 0.380772, -0.310596, -0.26576, -0.080866, 3.17507, 2.2028, 0.353139, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.228928, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.253137, 0.430171, -0.573421, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.166539, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.258903, -0.653375, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.532087, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 188, 0.591507, 0.271412, 0.096941, -0.021695, -0.298759, 1.09172, 0.051954, -0.086263, -0.260638, 0.681054, 0.819708, 0.885936, -0.054535, -0.297912, 0.169896, 0.2202, 1.84289, -0.014952, -0.08711, -0.167402, -0.200329, -0.055716, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.069954, 1.16804, -0.104419, -0.26576, -0.080866, 1.75722, 3.54481, 3.31144, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.291574, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.494371, -0.430612, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.09543, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.707137, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"188, 0.591507, 0.271412, 0.096941, -0.021695, -0.298759, 1.09172, 0.051954, -0.086263, -0.260638, 0.681054, 0.819708, 0.885936, -0.054535, -0.297912, 0.169896, 0.2202, 1.84289, -0.014952, -0.08711, -0.167402, -0.200329, -0.055716, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.069954, 1.16804, -0.104419, -0.26576, -0.080866, 1.75722, 3.54481, 3.31144, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.291574, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.494371, -0.430612, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.09543, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.707137, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 188, 0.252661, -0.056165, -0.362201, -1.21153, -0.166923, 0.074052, -0.302987, -1.21483, -0.295673, 0.580934, 0.726259, 0.162086, -0.054535, -0.297912, 2.04068, -0.047271, -0.221722, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.232923, 0.851421, -0.347995, -0.26576, -0.080866, 6.05277, 3.73119, -0.367967, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.215432, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.260546, 0.442068, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.336489, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.731599, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"188, 0.252661, -0.056165, -0.362201, -1.21153, -0.166923, 0.074052, -0.302987, -1.21483, -0.295673, 0.580934, 0.726259, 0.162086, -0.054535, -0.297912, 2.04068, -0.047271, -0.221722, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.232923, 0.851421, -0.347995, -0.26576, -0.080866, 6.05277, 3.73119, -0.367967, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.215432, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.260546, 0.442068, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.336489, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.731599, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 188, -0.536863, -0.442008, 1.58122, -1.06192, -0.363885, 0.213344, 1.61423, -1.07408, -0.442755, -2.2645, 0.304422, -0.641955, -0.054535, -0.297912, -0.261335, -0.302051, -0.222946, -0.014952, -0.08711, -0.167402, -0.200329, -0.12461, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.054295, -0.168601, -0.347995, -0.26576, -0.080866, 0.176422, 1.24044, 0.755903, -0.090565, -0.025787, -0.073582, -0.114571, -0.18949, -0.07568, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.324516, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.926624, 0.457651, -0.237517, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.143158, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.582731, -0.534809, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.079998, -0.410037, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"188, -0.536863, -0.442008, 1.58122, -1.06192, -0.363885, 0.213344, 1.61423, -1.07408, -0.442755, -2.2645, 0.304422, -0.641955, -0.054535, -0.297912, -0.261335, -0.302051, -0.222946, -0.014952, -0.08711, -0.167402, -0.200329, -0.12461, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.054295, -0.168601, -0.347995, -0.26576, -0.080866, 0.176422, 1.24044, 0.755903, -0.090565, -0.025787, -0.073582, -0.114571, -0.18949, -0.07568, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.324516, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.926624, 0.457651, -0.237517, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.143158, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.582731, -0.534809, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.079998, -0.410037, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 188, 1.73039, 1.59599, 0.68111, -1.25549, -0.588202, -0.490534, 0.626601, -1.23786, -0.630159, 0.317865, 0.573559, 0.529763, -0.054535, -0.297912, 0.113509, 0.278702, 0.013959, -0.014952, -0.08711, -0.167402, -0.200329, -0.22576, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 1.69234, 0.894151, -0.347995, -0.26576, -0.080866, 4.12597, 5.83014, 0.619219, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.326463, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.168745, -0.377724, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.128589, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.811953, -0.590145, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.190329, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.657807, -0.409056, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"188, 1.73039, 1.59599, 0.68111, -1.25549, -0.588202, -0.490534, 0.626601, -1.23786, -0.630159, 0.317865, 0.573559, 0.529763, -0.054535, -0.297912, 0.113509, 0.278702, 0.013959, -0.014952, -0.08711, -0.167402, -0.200329, -0.22576, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 1.69234, 0.894151, -0.347995, -0.26576, -0.080866, 4.12597, 5.83014, 0.619219, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.326463, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.168745, -0.377724, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.128589, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.811953, -0.590145, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.190329, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.657807, -0.409056, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 188, -0.113031, 0.105711, -0.742144, 0.562029, -0.613682, -0.534623, -0.780404, 0.582402, -0.651313, 0.786673, 0.497116, -0.549897, -0.054535, -0.297912, 9.38048, 1.41576, -0.179751, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, 0.067008, -0.317704, -0.26576, -0.080866, 1.54089, 0.702093, 0.92265, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.100602, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.37958, -0.422203, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.595179, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.745805, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"188, -0.113031, 0.105711, -0.742144, 0.562029, -0.613682, -0.534623, -0.780404, 0.582402, -0.651313, 0.786673, 0.497116, -0.549897, -0.054535, -0.297912, 9.38048, 1.41576, -0.179751, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, 0.067008, -0.317704, -0.26576, -0.080866, 1.54089, 0.702093, 0.92265, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.100602, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.37958, -0.422203, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.595179, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.745805, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 188, -0.154634, -0.039108, -1.66191, 0.569496, -0.65839, -0.756762, -1.66365, 0.512791, -0.658584, -2.35279, 0.436289, -1.44735, -0.054535, -0.297912, 9.41456, 3.48599, 0.353111, -0.014952, -0.08711, -0.167402, -0.200329, -0.199009, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.065315, -0.298243, -0.26576, -0.080866, -0.19033, 0.681266, 0.89663, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.056674, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.706447, -0.589662, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.376966, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.647498, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"188, -0.154634, -0.039108, -1.66191, 0.569496, -0.65839, -0.756762, -1.66365, 0.512791, -0.658584, -2.35279, 0.436289, -1.44735, -0.054535, -0.297912, 9.41456, 3.48599, 0.353111, -0.014952, -0.08711, -0.167402, -0.200329, -0.199009, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.065315, -0.298243, -0.26576, -0.080866, -0.19033, 0.681266, 0.89663, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.056674, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.706447, -0.589662, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.376966, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.647498, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 188, 0.229683, 0.749579, 0.970973, -0.519314, -0.705732, -0.289988, 1.06957, -0.638316, -0.685026, -1.76186, 0.35273, 0.912468, -0.054535, -0.297912, -0.261335, -0.269463, -0.204445, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, 0.166228, -0.120439, -0.26576, -0.080866, 0.068418, 2.26366, 3.98663, -0.210746, -0.025787, -0.073582, -0.114571, -0.18949, -0.146932, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.328512, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.252975, 0.102326, -0.221904, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.962725, -0.58501, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.293658, -0.409973, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"188, 0.229683, 0.749579, 0.970973, -0.519314, -0.705732, -0.289988, 1.06957, -0.638316, -0.685026, -1.76186, 0.35273, 0.912468, -0.054535, -0.297912, -0.261335, -0.269463, -0.204445, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, 0.166228, -0.120439, -0.26576, -0.080866, 0.068418, 2.26366, 3.98663, -0.210746, -0.025787, -0.073582, -0.114571, -0.18949, -0.146932, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.328512, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.252975, 0.102326, -0.221904, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.962725, -0.58501, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.293658, -0.409973, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 188, 0.184132, 0.638688, 1.83015, 1.17679, -0.698475, -0.795434, 1.81489, 1.18964, -0.745841, -1.79479, -0.18599, -0.86322, -0.054535, -0.297912, -0.261335, -0.302051, -0.208063, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.066443, -0.3025, -0.26576, -0.080866, -0.198669, 0.644874, -0.078704, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.25436, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.5884, -0.590148, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 2.12214, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.743719, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"188, 0.184132, 0.638688, 1.83015, 1.17679, -0.698475, -0.795434, 1.81489, 1.18964, -0.745841, -1.79479, -0.18599, -0.86322, -0.054535, -0.297912, -0.261335, -0.302051, -0.208063, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.066443, -0.3025, -0.26576, -0.080866, -0.198669, 0.644874, -0.078704, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.25436, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.5884, -0.590148, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 2.12214, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.743719, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 163, 0.935559, 0.871678, 0.123945, 0.088136, 2.86628, 0.260473, 0.242985, -0.016262, 3.57207, -0.458392, 0.388349, -0.26047, -0.054535, -0.095275, 0.238499, -0.100215, -0.225084, -0.014952, -0.08711, 2.17235, 1.15315, -0.200763, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.186062, 0.499655, -0.080866, 0.647251, 1.52366, -0.052614, -0.094513, -0.025787, -0.073582, 0.603022, 4.84886, -0.135666, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.172308, -0.166556, -0.019222, -0.050171, -0.114846, -0.288801, -0.028622, 0.054181, -0.067878, 0.0, -0.06989, -0.090689, 0.22514, 0.082916, -0.057758, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.057693, -0.171397, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.077758, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.186171, 0.348352, -0.075458, -0.077681, -0.03269, 0.0, -0.053085, -0.364321, -0.646734, -0.112213, -0.163331, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.148775, -0.189525, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.108887, 0.784363, -0.033384, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.398478, 0.232445, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"163, 0.935559, 0.871678, 0.123945, 0.088136, 2.86628, 0.260473, 0.242985, -0.016262, 3.57207, -0.458392, 0.388349, -0.26047, -0.054535, -0.095275, 0.238499, -0.100215, -0.225084, -0.014952, -0.08711, 2.17235, 1.15315, -0.200763, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.186062, 0.499655, -0.080866, 0.647251, 1.52366, -0.052614, -0.094513, -0.025787, -0.073582, 0.603022, 4.84886, -0.135666, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.172308, -0.166556, -0.019222, -0.050171, -0.114846, -0.288801, -0.028622, 0.054181, -0.067878, 0.0, -0.06989, -0.090689, 0.22514, 0.082916, -0.057758, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.057693, -0.171397, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.077758, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.186171, 0.348352, -0.075458, -0.077681, -0.03269, 0.0, -0.053085, -0.364321, -0.646734, -0.112213, -0.163331, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.148775, -0.189525, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.108887, 0.784363, -0.033384, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.398478, 0.232445, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 163, 0.928208, 0.718022, 0.907765, -1.37665, -0.504548, -1.01118, 1.16167, -1.47104, -0.594133, 0.935143, 0.869209, 0.089649, -0.054535, -0.297912, -0.167264, -0.295763, -0.225084, -0.014952, -0.08711, 0.522859, 0.074762, -0.113919, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 2.16648, -0.196817, -0.239694, -0.309461, -0.025787, -0.073582, 0.913819, 0.354028, 0.114779, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.822239, 1.00589, -0.055753, -0.178991, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.217524, 0.009997, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.706879, -0.283242, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.164571, -0.178382, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.36225, 0.619526, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"163, 0.928208, 0.718022, 0.907765, -1.37665, -0.504548, -1.01118, 1.16167, -1.47104, -0.594133, 0.935143, 0.869209, 0.089649, -0.054535, -0.297912, -0.167264, -0.295763, -0.225084, -0.014952, -0.08711, 0.522859, 0.074762, -0.113919, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 2.16648, -0.196817, -0.239694, -0.309461, -0.025787, -0.073582, 0.913819, 0.354028, 0.114779, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.822239, 1.00589, -0.055753, -0.178991, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.217524, 0.009997, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.706879, -0.283242, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.164571, -0.178382, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.36225, 0.619526, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 163, 0.09964, 0.337304, 1.51913, -1.09483, -0.702655, 1.48626, 1.55141, -1.09266, -0.796741, -1.76724, 1.78142, -0.802326, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 1.85405, 0.183441, -0.363096, -0.309461, -0.025787, -0.073582, 0.702115, 0.761787, -0.083142, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.863295, 1.31836, -0.260591, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.896803, -0.327934, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.515053, 0.540975, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"163, 0.09964, 0.337304, 1.51913, -1.09483, -0.702655, 1.48626, 1.55141, -1.09266, -0.796741, -1.76724, 1.78142, -0.802326, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 1.85405, 0.183441, -0.363096, -0.309461, -0.025787, -0.073582, 0.702115, 0.761787, -0.083142, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.863295, 1.31836, -0.260591, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.896803, -0.327934, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.515053, 0.540975, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 163, 0.196767, 0.245129, -0.412457, -0.069565, -0.790348, 0.796669, -0.444732, -0.032001, -0.806326, 0.019433, 1.18313, -0.045116, -0.054535, -0.297912, 0.059985, 0.172322, -0.225084, -0.014952, -0.08711, 5.00129, 12.0925, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.530903, 0.464353, -0.451394, -0.309461, -0.025787, -0.073582, 0.329765, 3.36951, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, 0.277981, 0.199153, -0.050171, -0.114846, -0.3317, -0.15868, -0.022587, -0.139878, 0.0, -0.06989, -0.257114, -0.018542, -0.468928, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.029386, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, 0.375149, 0.022887, -0.071579, -0.017934, -0.063725, -0.182197, -0.151049, 1.68782, -0.164838, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.744799, -0.396391, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.035032, 1.6182, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.354645, -0.171687, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"163, 0.196767, 0.245129, -0.412457, -0.069565, -0.790348, 0.796669, -0.444732, -0.032001, -0.806326, 0.019433, 1.18313, -0.045116, -0.054535, -0.297912, 0.059985, 0.172322, -0.225084, -0.014952, -0.08711, 5.00129, 12.0925, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.530903, 0.464353, -0.451394, -0.309461, -0.025787, -0.073582, 0.329765, 3.36951, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, 0.277981, 0.199153, -0.050171, -0.114846, -0.3317, -0.15868, -0.022587, -0.139878, 0.0, -0.06989, -0.257114, -0.018542, -0.468928, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.029386, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, 0.375149, 0.022887, -0.071579, -0.017934, -0.063725, -0.182197, -0.151049, 1.68782, -0.164838, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.744799, -0.396391, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.035032, 1.6182, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.354645, -0.171687, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 163, -1.22864, -1.16852, 0.497138, -0.243487, -0.779972, 1.19815, 0.509125, -0.310798, -0.820208, -1.08171, 0.991358, -0.666569, -0.054535, -0.297912, 2.19657, -0.229566, -0.225084, -0.014952, -0.08711, 9.99319, 1.89323, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 1.24233, 0.215944, -0.451394, -0.309461, -0.025787, -0.073582, 1.65691, 3.42122, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.388758, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.300414, 0.302269, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.250962, -0.331845, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.030843, -0.017661, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.026575, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.448591, -0.387124, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"163, -1.22864, -1.16852, 0.497138, -0.243487, -0.779972, 1.19815, 0.509125, -0.310798, -0.820208, -1.08171, 0.991358, -0.666569, -0.054535, -0.297912, 2.19657, -0.229566, -0.225084, -0.014952, -0.08711, 9.99319, 1.89323, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 1.24233, 0.215944, -0.451394, -0.309461, -0.025787, -0.073582, 1.65691, 3.42122, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.388758, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.300414, 0.302269, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.250962, -0.331845, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.030843, -0.017661, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.026575, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.448591, -0.387124, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 168, 0.126614, -0.337328, -0.190876, 0.574754, 1.15466, -0.918074, 0.119499, 0.580015, 1.34767, -0.303801, -4.14519, 2.57391, 2.25313, 2.65936, -0.261335, 0.164726, 0.234108, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 3.69285, 5.18578, -0.080912, 2.62565, 2.55753, 0.793251, -0.080866, -0.224814, -0.352461, -0.078259, -0.050167, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.9817, -0.049974, -0.014952, 0.058216, 6.18043, 17.3309, 5.49383, -0.050171, -0.084749, 1.95746, 1.41488, 2.46399, -0.139878, 0.0, -0.06989, -0.263975, -0.746704, -0.560372, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, 1.94037, 2.4509, -0.071579, -0.017934, -0.063725, -0.182197, 0.248694, 0.747225, 0.943707, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.04675, -0.650363, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.180495, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"168, 0.126614, -0.337328, -0.190876, 0.574754, 1.15466, -0.918074, 0.119499, 0.580015, 1.34767, -0.303801, -4.14519, 2.57391, 2.25313, 2.65936, -0.261335, 0.164726, 0.234108, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 3.69285, 5.18578, -0.080912, 2.62565, 2.55753, 0.793251, -0.080866, -0.224814, -0.352461, -0.078259, -0.050167, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.9817, -0.049974, -0.014952, 0.058216, 6.18043, 17.3309, 5.49383, -0.050171, -0.084749, 1.95746, 1.41488, 2.46399, -0.139878, 0.0, -0.06989, -0.263975, -0.746704, -0.560372, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, 1.94037, 2.4509, -0.071579, -0.017934, -0.063725, -0.182197, 0.248694, 0.747225, 0.943707, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.04675, -0.650363, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.180495, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 168, -0.211189, -0.272805, 1.37308, -0.888256, 0.945053, 1.1233, 1.27218, -0.848372, 0.656554, 0.617913, -0.759155, 0.626432, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.241576, 2.56239, -0.080912, -0.32343, -0.293244, 0.398544, -0.080866, -0.224814, -0.529019, -0.451394, -0.190986, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 0.635842, 7.7934, 2.37892, -0.050171, -0.114846, 0.266407, 2.05134, 9.59134, 2.26115, 0.0, -0.06989, -0.263975, -0.780059, -0.496627, 0.935146, 0.530497, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, 0.082139, 0.307476, -0.071579, -0.017934, -0.063725, -0.182197, 0.41684, 2.19925, 1.07717, -0.071346, -0.03269, 0.0, -0.053085, -0.364504, -0.663128, 0.232626, 0.39546, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.180157, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.671808, -0.265023, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"168, -0.211189, -0.272805, 1.37308, -0.888256, 0.945053, 1.1233, 1.27218, -0.848372, 0.656554, 0.617913, -0.759155, 0.626432, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.241576, 2.56239, -0.080912, -0.32343, -0.293244, 0.398544, -0.080866, -0.224814, -0.529019, -0.451394, -0.190986, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 0.635842, 7.7934, 2.37892, -0.050171, -0.114846, 0.266407, 2.05134, 9.59134, 2.26115, 0.0, -0.06989, -0.263975, -0.780059, -0.496627, 0.935146, 0.530497, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, 0.082139, 0.307476, -0.071579, -0.017934, -0.063725, -0.182197, 0.41684, 2.19925, 1.07717, -0.071346, -0.03269, 0.0, -0.053085, -0.364504, -0.663128, 0.232626, 0.39546, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.180157, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.671808, -0.265023, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 168, 0.487409, 0.504226, -1.0318, -1.13107, 0.343468, 0.910122, -0.816495, -1.07643, 0.058644, -0.09861, -1.83872, 0.668262, -0.054535, -0.182762, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 1.2169, 5.1817, -0.080912, 1.1836, 0.751078, 1.36818, -0.080866, -0.224814, -0.52758, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, 0.023875, 0.607998, 8.89819, 2.27477, -0.050171, -0.109081, 0.823709, 3.12697, 8.9471, 0.185276, 0.0, -0.06989, -0.263975, -0.756165, -0.47958, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, 0.316245, -0.071579, -0.017934, -0.063725, -0.182197, 0.427895, 1.45534, 0.25456, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.506208, -0.164372, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.17777, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.735737, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"168, 0.487409, 0.504226, -1.0318, -1.13107, 0.343468, 0.910122, -0.816495, -1.07643, 0.058644, -0.09861, -1.83872, 0.668262, -0.054535, -0.182762, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 1.2169, 5.1817, -0.080912, 1.1836, 0.751078, 1.36818, -0.080866, -0.224814, -0.52758, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, 0.023875, 0.607998, 8.89819, 2.27477, -0.050171, -0.109081, 0.823709, 3.12697, 8.9471, 0.185276, 0.0, -0.06989, -0.263975, -0.756165, -0.47958, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, 0.316245, -0.071579, -0.017934, -0.063725, -0.182197, 0.427895, 1.45534, 0.25456, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.506208, -0.164372, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.17777, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.735737, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 168, 0.799071, 0.752062, 0.117931, -0.13464, 0.097357, 1.21763, 0.129377, -0.307423, 0.016007, 0.426053, 0.908403, 2.50109, -0.054535, -0.221903, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 5.29017, 10.672, -0.080912, 1.35004, 1.42443, 0.822759, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.3838, -0.049974, -0.014952, 0.396898, 2.23539, 13.2111, 4.49742, -0.050171, -0.105154, 2.26423, 2.14939, 4.2051, 0.211449, 0.0, -0.06989, -0.263975, -0.779274, -0.534026, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.006393, 0.50418, -0.071579, -0.017934, -0.063725, -0.182197, 0.542686, 0.834031, 0.957833, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.794509, -0.21133, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183051, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.703706, -0.365829, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"168, 0.799071, 0.752062, 0.117931, -0.13464, 0.097357, 1.21763, 0.129377, -0.307423, 0.016007, 0.426053, 0.908403, 2.50109, -0.054535, -0.221903, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 5.29017, 10.672, -0.080912, 1.35004, 1.42443, 0.822759, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.3838, -0.049974, -0.014952, 0.396898, 2.23539, 13.2111, 4.49742, -0.050171, -0.105154, 2.26423, 2.14939, 4.2051, 0.211449, 0.0, -0.06989, -0.263975, -0.779274, -0.534026, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.006393, 0.50418, -0.071579, -0.017934, -0.063725, -0.182197, 0.542686, 0.834031, 0.957833, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.794509, -0.21133, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183051, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.703706, -0.365829, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 168, 0.595596, -0.175068, 1.54254, 1.19686, -0.619017, -0.94418, 1.57264, 1.20705, -0.643711, -2.0405, -2.24507, 1.09693, 7.92776, 5.16521, -0.048121, 1.41477, -0.020866, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 1.9215, 3.92775, -0.080912, -0.060494, 2.73686, 0.673709, -0.080866, -0.224814, -0.419126, -0.407996, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.23608, -0.049974, -0.014952, -0.048732, 4.12908, 18.2527, 3.12788, -0.050171, -0.114846, -0.205906, 1.72002, 1.74551, -0.139878, 0.0, -0.06989, -0.263975, -0.784751, -0.572037, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, 2.48418, 1.55857, -0.071579, -0.017934, -0.063725, -0.182197, -0.215829, 2.61734, 0.129796, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.06626, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.178552, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"168, 0.595596, -0.175068, 1.54254, 1.19686, -0.619017, -0.94418, 1.57264, 1.20705, -0.643711, -2.0405, -2.24507, 1.09693, 7.92776, 5.16521, -0.048121, 1.41477, -0.020866, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 1.9215, 3.92775, -0.080912, -0.060494, 2.73686, 0.673709, -0.080866, -0.224814, -0.419126, -0.407996, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.23608, -0.049974, -0.014952, -0.048732, 4.12908, 18.2527, 3.12788, -0.050171, -0.114846, -0.205906, 1.72002, 1.74551, -0.139878, 0.0, -0.06989, -0.263975, -0.784751, -0.572037, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, 2.48418, 1.55857, -0.071579, -0.017934, -0.063725, -0.182197, -0.215829, 2.61734, 0.129796, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.06626, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.178552, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 178, -0.133604, -0.303306, -0.13373, -0.798013, 1.10414, -1.18468, 0.120614, -0.478922, 1.35858, -0.482325, 0.231808, 0.50899, -0.054535, -0.297912, -0.077071, 0.190099, -0.13514, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, 0.28226, 0.025035, -0.26576, -0.080866, -0.184323, 1.68402, 0.371302, -0.30174, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.315692, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 2.82028, 2.00271, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.718027, -0.555046, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"178, -0.133604, -0.303306, -0.13373, -0.798013, 1.10414, -1.18468, 0.120614, -0.478922, 1.35858, -0.482325, 0.231808, 0.50899, -0.054535, -0.297912, -0.077071, 0.190099, -0.13514, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, 0.28226, 0.025035, -0.26576, -0.080866, -0.184323, 1.68402, 0.371302, -0.30174, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.315692, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 2.82028, 2.00271, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.718027, -0.555046, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 178, -1.30685, -1.23563, 0.391391, 0.919115, 0.686158, -1.05965, 0.161182, 0.824919, 0.814542, -0.475483, -0.372749, 0.022253, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.112836, -0.347995, -0.26576, -0.080866, -0.224814, 0.202394, -0.406223, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.009905, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 3.13962, 0.52142, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.100777, 0.100584, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"178, -1.30685, -1.23563, 0.391391, 0.919115, 0.686158, -1.05965, 0.161182, 0.824919, 0.814542, -0.475483, -0.372749, 0.022253, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.112836, -0.347995, -0.26576, -0.080866, -0.224814, 0.202394, -0.406223, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.009905, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 3.13962, 0.52142, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.100777, 0.100584, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 178, 1.27835, 1.55199, 0.564997, -0.272812, 0.091591, -0.543124, 0.662065, -0.300661, 0.086407, -0.841684, 0.567458, 0.873965, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, 0.550925, -0.268245, -0.26576, -0.080866, -0.220901, 0.08322, -0.249914, -0.305372, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.287472, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 3.53402, 2.30769, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.527605, -0.400654, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"178, 1.27835, 1.55199, 0.564997, -0.272812, 0.091591, -0.543124, 0.662065, -0.300661, 0.086407, -0.841684, 0.567458, 0.873965, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, 0.550925, -0.268245, -0.26576, -0.080866, -0.220901, 0.08322, -0.249914, -0.305372, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.287472, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 3.53402, 2.30769, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.527605, -0.400654, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 178, -1.28595, -1.21657, -1.22594, -0.434512, -0.624212, 1.46667, -1.30354, -0.461785, -0.649329, 0.009177, 0.209445, 0.444993, -0.054535, -0.297912, -0.243992, -0.013539, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, 0.611855, 0.033692, -0.26576, -0.080866, -0.199566, 1.81246, 0.590189, -0.29627, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.327338, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 2.46835, 2.87305, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.837178, -0.569763, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"178, -1.28595, -1.21657, -1.22594, -0.434512, -0.624212, 1.46667, -1.30354, -0.461785, -0.649329, 0.009177, 0.209445, 0.444993, -0.054535, -0.297912, -0.243992, -0.013539, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, 0.611855, 0.033692, -0.26576, -0.080866, -0.199566, 1.81246, 0.590189, -0.29627, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.327338, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 2.46835, 2.87305, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.837178, -0.569763, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 178, -0.377595, -0.123757, 0.706772, 0.227426, -0.687828, -0.915953, 0.684797, 0.203083, -0.711798, 0.386673, 0.481208, 0.500256, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.465122, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, 0.195134, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 3.15712, 0.947361, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.025345, 0.30011, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"178, -0.377595, -0.123757, 0.706772, 0.227426, -0.687828, -0.915953, 0.684797, 0.203083, -0.711798, 0.386673, 0.481208, 0.500256, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.465122, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, 0.195134, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 3.15712, 0.947361, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.025345, 0.30011, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 178, -1.02248, -1.021, 1.97225, -0.52568, -0.818681, 1.832, 2.0416, -0.537228, -0.845658, -0.041464, 0.322572, 0.463597, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.252972, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 4.42585, 2.21075, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.607602, -0.579038, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"178, -1.02248, -1.021, 1.97225, -0.52568, -0.818681, 1.832, 2.0416, -0.537228, -0.845658, -0.041464, 0.322572, 0.463597, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.252972, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 4.42585, 2.21075, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.607602, -0.579038, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 178, -1.18029, -1.08747, 1.10507, -0.118843, -0.82533, 1.31975, 1.16975, -0.115726, -0.849955, -1.76506, -0.658744, -0.333046, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.524851, -0.451108, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.302807, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 4.39931, 1.60188, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.51651, -0.332834, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"178, -1.18029, -1.08747, 1.10507, -0.118843, -0.82533, 1.31975, 1.16975, -0.115726, -0.849955, -1.76506, -0.658744, -0.333046, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.524851, -0.451108, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.302807, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 4.39931, 1.60188, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.51651, -0.332834, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 131, 0.798962, 0.86861, 0.032954, 0.430179, 1.09644, -0.285628, -0.047711, 0.433759, 0.90213, 0.420668, 1.06068, 0.838697, -0.054535, -0.297912, -0.254872, 0.121006, 0.458628, -0.014952, -0.08711, -0.167402, -0.15873, 0.371097, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.290922, -0.218156, -0.261432, -0.080866, -0.224814, 0.262664, 0.570708, -0.26537, -0.025787, -0.073582, -0.114571, -0.185759, -0.160144, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.323415, -0.392912, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.593921, 0.311538, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.125094, -0.11934, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.308839, -0.070923, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.111601, -0.15896, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.196999, 0.344556, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.113881, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"131, 0.798962, 0.86861, 0.032954, 0.430179, 1.09644, -0.285628, -0.047711, 0.433759, 0.90213, 0.420668, 1.06068, 0.838697, -0.054535, -0.297912, -0.254872, 0.121006, 0.458628, -0.014952, -0.08711, -0.167402, -0.15873, 0.371097, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.290922, -0.218156, -0.261432, -0.080866, -0.224814, 0.262664, 0.570708, -0.26537, -0.025787, -0.073582, -0.114571, -0.185759, -0.160144, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.323415, -0.392912, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.593921, 0.311538, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.125094, -0.11934, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.308839, -0.070923, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.111601, -0.15896, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.196999, 0.344556, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.113881, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 131, 1.97822, 1.33029, 1.45617, 0.369838, 0.291894, 0.042353, 1.36947, 0.233476, 0.577229, -0.10441, 0.332254, 0.137719, -0.054535, -0.297912, -0.229772, 0.11518, 1.30144, -0.014952, -0.08711, -0.167402, -0.183398, 1.34751, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.303404, -0.275647, -0.26576, -0.080866, -0.223461, -0.12573, 1.20979, -0.116517, -0.025787, -0.073582, -0.114571, -0.187972, 0.026276, 0.028403, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.294804, -0.404601, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.427074, 0.274303, -0.258561, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.138382, -0.094685, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.358908, 0.376957, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.021221, -0.029925, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.078942, 0.458697, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.110433, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"131, 1.97822, 1.33029, 1.45617, 0.369838, 0.291894, 0.042353, 1.36947, 0.233476, 0.577229, -0.10441, 0.332254, 0.137719, -0.054535, -0.297912, -0.229772, 0.11518, 1.30144, -0.014952, -0.08711, -0.167402, -0.183398, 1.34751, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.303404, -0.275647, -0.26576, -0.080866, -0.223461, -0.12573, 1.20979, -0.116517, -0.025787, -0.073582, -0.114571, -0.187972, 0.026276, 0.028403, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.294804, -0.404601, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.427074, 0.274303, -0.258561, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.138382, -0.094685, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.358908, 0.376957, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.021221, -0.029925, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.078942, 0.458697, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.110433, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 131, -0.615004, -0.364957, 0.531696, -1.3556, 0.232258, -1.55684, 0.437006, -1.29376, 0.344874, 0.188511, -0.269427, 0.133619, -0.054535, -0.297912, 0.034874, -0.285719, 0.66195, -0.014952, -0.08711, -0.167402, -0.200329, 0.873685, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.166203, -0.289619, -0.347859, -0.26576, -0.080866, 0.893832, -0.06311, 1.10012, 0.185576, -0.025787, -0.073582, -0.114571, -0.18949, 0.021903, -0.040487, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.107014, -0.284585, -0.403338, -0.263784, -0.139878, 0.0, -0.06989, 0.189594, 0.606497, 0.695517, -0.241083, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.155368, -0.095685, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.390924, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.315864, 0.83829, -0.00337, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.412197, -0.286009, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"131, -0.615004, -0.364957, 0.531696, -1.3556, 0.232258, -1.55684, 0.437006, -1.29376, 0.344874, 0.188511, -0.269427, 0.133619, -0.054535, -0.297912, 0.034874, -0.285719, 0.66195, -0.014952, -0.08711, -0.167402, -0.200329, 0.873685, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.166203, -0.289619, -0.347859, -0.26576, -0.080866, 0.893832, -0.06311, 1.10012, 0.185576, -0.025787, -0.073582, -0.114571, -0.18949, 0.021903, -0.040487, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.107014, -0.284585, -0.403338, -0.263784, -0.139878, 0.0, -0.06989, 0.189594, 0.606497, 0.695517, -0.241083, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.155368, -0.095685, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.390924, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.315864, 0.83829, -0.00337, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.412197, -0.286009, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 131, 2.5414, 3.76261, -0.934464, -0.294259, -0.182493, 0.397961, -0.884688, -0.287291, -0.272537, -1.21279, 0.408076, 0.495813, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.185004, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.336336, -0.253045, -0.080866, -0.224814, -0.414424, -0.096728, 0.173474, -0.025787, -0.073582, -0.114571, -0.18949, -0.048291, 1.37917, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.224033, -0.329444, -0.241575, -0.139878, 0.0, -0.06989, -0.263975, 0.795269, 0.303522, -0.205518, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.148463, 0.029322, -0.061726, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.094444, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.019867, 0.313961, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.133774, -0.108171, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.121894, -0.086786, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.265836, 0.475554, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.113814, -0.073169, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"131, 2.5414, 3.76261, -0.934464, -0.294259, -0.182493, 0.397961, -0.884688, -0.287291, -0.272537, -1.21279, 0.408076, 0.495813, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.185004, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.336336, -0.253045, -0.080866, -0.224814, -0.414424, -0.096728, 0.173474, -0.025787, -0.073582, -0.114571, -0.18949, -0.048291, 1.37917, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.224033, -0.329444, -0.241575, -0.139878, 0.0, -0.06989, -0.263975, 0.795269, 0.303522, -0.205518, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.148463, 0.029322, -0.061726, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.094444, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.019867, 0.313961, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.133774, -0.108171, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.121894, -0.086786, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.265836, 0.475554, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.113814, -0.073169, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 131, 1.94649, 1.67627, -0.644307, -0.917823, -0.528707, -1.34217, -0.747529, -0.941967, -0.470188, -0.277471, -0.064827, 0.181926, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.212916, -0.327191, -0.347995, -0.26576, -0.080866, -0.115054, -0.479457, 0.05538, 0.360257, -0.025787, -0.073582, -0.114571, -0.18949, 0.105562, 1.05757, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.095301, -0.3317, -0.407882, -0.263784, -0.139878, 0.0, -0.06989, 0.166309, 0.239334, 0.361902, -0.208728, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.051498, 1.15107, -0.035175, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.328923, 0.772507, -0.149136, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.094117, -0.023562, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.45334, -0.10279, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.11371, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"131, 1.94649, 1.67627, -0.644307, -0.917823, -0.528707, -1.34217, -0.747529, -0.941967, -0.470188, -0.277471, -0.064827, 0.181926, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.212916, -0.327191, -0.347995, -0.26576, -0.080866, -0.115054, -0.479457, 0.05538, 0.360257, -0.025787, -0.073582, -0.114571, -0.18949, 0.105562, 1.05757, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.095301, -0.3317, -0.407882, -0.263784, -0.139878, 0.0, -0.06989, 0.166309, 0.239334, 0.361902, -0.208728, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.051498, 1.15107, -0.035175, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.328923, 0.772507, -0.149136, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.094117, -0.023562, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.45334, -0.10279, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.11371, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 131, 1.10876, 0.171641, -1.60385, 0.644712, -0.760105, 1.65889, -1.60939, 0.643342, -0.784512, 1.05134, 0.456575, 0.503725, -0.054535, -0.297912, -0.1097, 0.241751, 0.691631, -0.014952, -0.08711, -0.167402, -0.118989, 0.837248, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.187853, -0.258108, -0.26576, -0.080866, -0.102682, 1.72505, 0.307849, -0.138296, -0.025787, -0.073582, -0.114571, -0.182195, -0.057007, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.662942, -0.165544, 0.27297, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.034138, 0.24382, 1.04756, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.355177, -0.247325, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.225083, -0.149629, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.036175, -0.08925, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"131, 1.10876, 0.171641, -1.60385, 0.644712, -0.760105, 1.65889, -1.60939, 0.643342, -0.784512, 1.05134, 0.456575, 0.503725, -0.054535, -0.297912, -0.1097, 0.241751, 0.691631, -0.014952, -0.08711, -0.167402, -0.118989, 0.837248, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.187853, -0.258108, -0.26576, -0.080866, -0.102682, 1.72505, 0.307849, -0.138296, -0.025787, -0.073582, -0.114571, -0.182195, -0.057007, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.662942, -0.165544, 0.27297, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.034138, 0.24382, 1.04756, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.355177, -0.247325, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.225083, -0.149629, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.036175, -0.08925, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 131, 2.05067, 1.54716, -1.41435, 0.222904, -0.795556, -1.49751, -1.37066, 0.268237, -0.803682, 0.271199, 0.471121, 0.319741, -0.054535, -0.297912, -0.261335, -0.078101, 1.13102, -0.014952, -0.08711, -0.167402, -0.080002, 1.35782, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.300785, -0.215025, -0.26576, -0.080866, -0.224814, 0.003882, 0.521345, -0.309461, -0.025787, -0.073582, -0.114571, -0.178699, 0.032189, 2.55186, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.082805, -0.175893, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, 0.521634, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.44644, 1.23803, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, 0.409046, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.038071, 1.32573, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"131, 2.05067, 1.54716, -1.41435, 0.222904, -0.795556, -1.49751, -1.37066, 0.268237, -0.803682, 0.271199, 0.471121, 0.319741, -0.054535, -0.297912, -0.261335, -0.078101, 1.13102, -0.014952, -0.08711, -0.167402, -0.080002, 1.35782, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.300785, -0.215025, -0.26576, -0.080866, -0.224814, 0.003882, 0.521345, -0.309461, -0.025787, -0.073582, -0.114571, -0.178699, 0.032189, 2.55186, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.082805, -0.175893, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, 0.521634, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.44644, 1.23803, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, 0.409046, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.038071, 1.32573, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 171, 0.130106, 0.351448, -0.620561, -0.972546, 0.927251, 0.324948, -0.539979, -0.766967, 0.676055, 0.441372, -0.19556, 0.04157, -0.054535, -0.297912, -0.261335, -0.293158, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.325322, -0.334729, -0.26576, -0.080866, -0.224814, -0.527728, -0.344896, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.329158, -0.403001, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.446023, -0.018837, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.254589, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.332321, -0.486172, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 2.03913, -0.399139, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"171, 0.130106, 0.351448, -0.620561, -0.972546, 0.927251, 0.324948, -0.539979, -0.766967, 0.676055, 0.441372, -0.19556, 0.04157, -0.054535, -0.297912, -0.261335, -0.293158, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.325322, -0.334729, -0.26576, -0.080866, -0.224814, -0.527728, -0.344896, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.329158, -0.403001, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.446023, -0.018837, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.254589, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.332321, -0.486172, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 2.03913, -0.399139, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 171, -0.073539, 0.246061, 0.671413, 0.742945, 0.065191, -1.20046, 0.688712, 0.733173, -0.073895, 0.713438, -0.049796, 0.052394, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.307274, -0.317911, -0.26576, -0.080866, -0.224814, -0.238786, -0.381436, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, 0.13093, -0.405392, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.16604, -0.265169, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.227397, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.80366, -0.654014, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.052564, -0.412493, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"171, -0.073539, 0.246061, 0.671413, 0.742945, 0.065191, -1.20046, 0.688712, 0.733173, -0.073895, 0.713438, -0.049796, 0.052394, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.307274, -0.317911, -0.26576, -0.080866, -0.224814, -0.238786, -0.381436, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, 0.13093, -0.405392, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.16604, -0.265169, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.227397, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.80366, -0.654014, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.052564, -0.412493, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 171, 0.410885, 0.807645, -0.84547, 0.550449, -0.100735, -0.618283, -0.885193, 0.529928, -0.26824, -0.957888, -0.479916, 0.176015, -0.054535, 0.541224, -0.14816, 0.218705, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, 0.470993, 0.002444, -0.26576, -0.080866, -0.224584, -0.009778, -0.438215, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, 0.935594, -0.395455, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.959274, -0.332057, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.249598, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.723068, -0.499669, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.374769, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"171, 0.410885, 0.807645, -0.84547, 0.550449, -0.100735, -0.618283, -0.885193, 0.529928, -0.26824, -0.957888, -0.479916, 0.176015, -0.054535, 0.541224, -0.14816, 0.218705, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, 0.470993, 0.002444, -0.26576, -0.080866, -0.224584, -0.009778, -0.438215, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, 0.935594, -0.395455, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.959274, -0.332057, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.249598, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.723068, -0.499669, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.374769, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 171, -1.15754, -1.01987, 0.297193, -0.099245, -0.314505, -0.141114, 0.310727, -0.107619, -0.386236, 1.25553, 0.058233, -0.372097, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.340116, -0.26576, -0.080866, -0.224814, -0.485369, -0.421894, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.22982, -0.397324, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.862223, 0.159152, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.250725, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.0614, -0.496622, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.628367, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"171, -1.15754, -1.01987, 0.297193, -0.099245, -0.314505, -0.141114, 0.310727, -0.107619, -0.386236, 1.25553, 0.058233, -0.372097, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.340116, -0.26576, -0.080866, -0.224814, -0.485369, -0.421894, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.22982, -0.397324, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.862223, 0.159152, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.250725, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.0614, -0.496622, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.628367, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 171, -1.20926, -1.08689, 1.81298, -0.364202, -0.336984, 0.427686, 1.83144, -0.35307, -0.441102, 1.15637, 0.188349, -0.248812, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.334012, -0.26576, -0.080866, -0.224814, -0.085258, -0.318896, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.23086, -0.396895, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.80772, -0.052482, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.225445, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.04789, -0.593506, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.060209, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"171, -1.20926, -1.08689, 1.81298, -0.364202, -0.336984, 0.427686, 1.83144, -0.35307, -0.441102, 1.15637, 0.188349, -0.248812, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.334012, -0.26576, -0.080866, -0.224814, -0.085258, -0.318896, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.23086, -0.396895, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.80772, -0.052482, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.225445, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.04789, -0.593506, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.060209, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 171, -1.29786, -1.22032, 1.08597, 1.08495, -0.574492, -1.21229, 1.33624, 1.1064, -0.530342, 1.10164, 0.132368, -0.050224, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.281167, -0.157777, -0.26576, -0.080866, -0.224814, -0.529019, -0.428844, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.211237, -0.402481, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.42284, -0.554344, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.244507, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.95632, -0.651597, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.99409, -0.406645, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"171, -1.29786, -1.22032, 1.08597, 1.08495, -0.574492, -1.21229, 1.33624, 1.1064, -0.530342, 1.10164, 0.132368, -0.050224, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.281167, -0.157777, -0.26576, -0.080866, -0.224814, -0.529019, -0.428844, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.211237, -0.402481, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.42284, -0.554344, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.244507, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.95632, -0.651597, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.99409, -0.406645, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 171, -1.17638, -1.05362, -1.49358, 0.208203, -0.510354, 0.485614, -1.4669, 0.169268, -0.576285, 1.28999, 0.323305, -0.241873, -0.054535, 1.19056, -0.215008, 1.35996, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.230203, 0.127977, -0.26576, -0.080866, -0.224814, -0.409974, 0.109028, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.330979, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.515233, -0.521276, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.25534, -0.639816, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.04565, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"171, -1.17638, -1.05362, -1.49358, 0.208203, -0.510354, 0.485614, -1.4669, 0.169268, -0.576285, 1.28999, 0.323305, -0.241873, -0.054535, 1.19056, -0.215008, 1.35996, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.230203, 0.127977, -0.26576, -0.080866, -0.224814, -0.409974, 0.109028, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.330979, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.515233, -0.521276, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.25534, -0.639816, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.04565, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 171, -0.243195, -0.057318, 1.77505, -1.10476, -0.692235, -0.76252, 1.86631, -1.10904, -0.722705, 0.083072, -0.267296, 0.368754, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.601342, -0.56754, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.546626, -0.624819, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 2.253, -0.408609, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"171, -0.243195, -0.057318, 1.77505, -1.10476, -0.692235, -0.76252, 1.86631, -1.10904, -0.722705, 0.083072, -0.267296, 0.368754, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.601342, -0.56754, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.546626, -0.624819, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 2.253, -0.408609, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 171, -1.22988, -1.14042, -0.53707, -1.57512, -0.759295, -0.791152, -0.580324, -1.61321, -0.789139, 1.10065, 0.324837, 0.025643, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.722938, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 3.3799, -0.381875, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"171, -1.22988, -1.14042, -0.53707, -1.57512, -0.759295, -0.791152, -0.580324, -1.61321, -0.789139, 1.10065, 0.324837, 0.025643, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.722938, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 3.3799, -0.381875, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 171, 0.767732, 1.71371, 1.01734, -1.02734, -0.806393, -1.3645, 1.06991, -1.05137, -0.811945, -2.53762, 0.055099, 0.010966, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.200399, -0.26576, -0.080866, -0.224814, -0.529019, -0.293473, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.367904, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.332741, -0.050201, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.716437, -0.655026, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 2.89668, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"171, 0.767732, 1.71371, 1.01734, -1.02734, -0.806393, -1.3645, 1.06991, -1.05137, -0.811945, -2.53762, 0.055099, 0.010966, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.200399, -0.26576, -0.080866, -0.224814, -0.529019, -0.293473, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.367904, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.332741, -0.050201, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.716437, -0.655026, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 2.89668, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 154, 0.73136, 0.702086, 0.148769, -0.853193, 0.767961, -0.022364, -0.041937, -0.710671, 0.842636, -0.719786, 1.73162, 0.288237, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.32687, -0.335381, -0.249325, -0.080866, -0.218049, -0.372163, -0.198526, 0.323583, -0.025787, -0.073582, -0.114571, -0.058313, -0.128211, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.32318, -0.404054, -0.230966, -0.139878, 0.0, -0.06989, -0.218353, 0.677498, 0.224818, 0.702551, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.833062, -0.179837, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.242982, -0.28357, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.34507, -0.20584, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.294183, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.180753, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.534546, 1.64852, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 0.314054, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"154, 0.73136, 0.702086, 0.148769, -0.853193, 0.767961, -0.022364, -0.041937, -0.710671, 0.842636, -0.719786, 1.73162, 0.288237, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.32687, -0.335381, -0.249325, -0.080866, -0.218049, -0.372163, -0.198526, 0.323583, -0.025787, -0.073582, -0.114571, -0.058313, -0.128211, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.32318, -0.404054, -0.230966, -0.139878, 0.0, -0.06989, -0.218353, 0.677498, 0.224818, 0.702551, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.833062, -0.179837, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.242982, -0.28357, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.34507, -0.20584, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.294183, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.180753, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.534546, 1.64852, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 0.314054, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 154, 1.97684, 3.03665, -0.526521, 0.349091, 0.260763, -0.674783, -0.654868, 0.447646, -0.069267, 0.294683, 0.137186, 0.333739, -0.054535, -0.297912, 0.156008, -0.199093, 0.034957, -0.014952, -0.08711, 0.34977, 0.513805, -0.243418, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.244474, -0.312861, -0.26576, -0.080866, -0.030122, 0.970288, 0.570056, -0.039473, -0.025787, -0.073582, -0.022113, 0.081862, -0.075059, 1.26308, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, 0.032473, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.255142, -0.139878, 0.0, -0.06989, -0.263975, -0.19718, 0.548634, 0.591494, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.239086, 1.89639, 1.57977, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.352317, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.642782, 0.350758, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 2.06723, 2.69611, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.354956, -0.009198, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 3.83391, 1.73058, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"154, 1.97684, 3.03665, -0.526521, 0.349091, 0.260763, -0.674783, -0.654868, 0.447646, -0.069267, 0.294683, 0.137186, 0.333739, -0.054535, -0.297912, 0.156008, -0.199093, 0.034957, -0.014952, -0.08711, 0.34977, 0.513805, -0.243418, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.244474, -0.312861, -0.26576, -0.080866, -0.030122, 0.970288, 0.570056, -0.039473, -0.025787, -0.073582, -0.022113, 0.081862, -0.075059, 1.26308, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, 0.032473, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.255142, -0.139878, 0.0, -0.06989, -0.263975, -0.19718, 0.548634, 0.591494, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.239086, 1.89639, 1.57977, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.352317, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.642782, 0.350758, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 2.06723, 2.69611, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.354956, -0.009198, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 3.83391, 1.73058, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 154, -1.28163, -1.2049, 1.60727, 0.474214, 0.009823, 1.46444, 1.64203, 0.433974, -0.147601, 1.07851, -0.399342, 0.946999, -0.054535, -0.297912, -0.242395, -0.302051, -0.225084, -0.014952, -0.08711, -0.058435, 0.052158, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.297522, -0.34676, -0.26576, -0.080866, 0.26702, 1.12042, -0.123165, -0.182799, -0.025787, -0.073582, 0.009272, 0.769986, 0.105305, -0.049343, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.23164, 0.986517, 0.296963, -0.217898, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 2.46699, 0.257787, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.553223, 0.179329, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.019797, -0.152591, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.25337, -0.010636, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 1.1591, 2.2003, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"154, -1.28163, -1.2049, 1.60727, 0.474214, 0.009823, 1.46444, 1.64203, 0.433974, -0.147601, 1.07851, -0.399342, 0.946999, -0.054535, -0.297912, -0.242395, -0.302051, -0.225084, -0.014952, -0.08711, -0.058435, 0.052158, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.297522, -0.34676, -0.26576, -0.080866, 0.26702, 1.12042, -0.123165, -0.182799, -0.025787, -0.073582, 0.009272, 0.769986, 0.105305, -0.049343, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.23164, 0.986517, 0.296963, -0.217898, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 2.46699, 0.257787, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.553223, 0.179329, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.019797, -0.152591, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.25337, -0.010636, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 1.1591, 2.2003, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 154, -1.2727, -1.18785, 1.68277, -1.13383, -1.26E-4, 0.790413, 1.70008, -1.14254, -0.183958, -1.14125, -0.432677, -0.225351, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.50509, -0.440557, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.325567, -0.407112, -0.236749, -0.139878, 0.0, -0.06989, -0.249029, 0.260942, -0.169308, 0.217946, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.10598, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.201231, 0.4175, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.231388, -0.199032, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.826704, -0.183034, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, 0.015368, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.308078, 2.35098, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 1.1491, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"154, -1.2727, -1.18785, 1.68277, -1.13383, -1.26E-4, 0.790413, 1.70008, -1.14254, -0.183958, -1.14125, -0.432677, -0.225351, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.50509, -0.440557, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.325567, -0.407112, -0.236749, -0.139878, 0.0, -0.06989, -0.249029, 0.260942, -0.169308, 0.217946, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.10598, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.201231, 0.4175, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.231388, -0.199032, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.826704, -0.183034, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, 0.015368, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.308078, 2.35098, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 1.1491, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 154, -1.27977, -1.186, 0.660989, 1.25786, -0.490516, -1.79181, 0.694239, 1.28326, -0.459942, -1.74716, 0.482189, -0.692847, -0.054535, -0.297912, 3.59977, -0.146757, -0.012627, -0.014952, -0.08711, 5.57025, 2.3403, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.256743, -0.282253, -0.26576, -0.080866, 0.68388, 0.779695, 0.986234, -0.309461, -0.025787, -0.073582, -0.039032, -0.114955, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.327761, 0.213267, -0.263784, -0.139878, 0.0, -0.06989, -0.039312, -0.120801, 0.095295, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.249944, -0.162685, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.670264, 0.810687, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.649257, -0.26577, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"154, -1.27977, -1.186, 0.660989, 1.25786, -0.490516, -1.79181, 0.694239, 1.28326, -0.459942, -1.74716, 0.482189, -0.692847, -0.054535, -0.297912, 3.59977, -0.146757, -0.012627, -0.014952, -0.08711, 5.57025, 2.3403, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.256743, -0.282253, -0.26576, -0.080866, 0.68388, 0.779695, 0.986234, -0.309461, -0.025787, -0.073582, -0.039032, -0.114955, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.327761, 0.213267, -0.263784, -0.139878, 0.0, -0.06989, -0.039312, -0.120801, 0.095295, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.249944, -0.162685, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.670264, 0.810687, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.649257, -0.26577, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 154, 1.37021, 2.40047, -1.34763, -1.16627, -0.618769, 0.126172, -1.38178, -1.15297, -0.688, 0.038725, 0.86883, -0.446471, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.323183, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.256122, 1.52728, -0.279126, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.145877, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.369334, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.353053, 0.573764, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.049899, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.360359, 2.33578, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 1.11966, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"154, 1.37021, 2.40047, -1.34763, -1.16627, -0.618769, 0.126172, -1.38178, -1.15297, -0.688, 0.038725, 0.86883, -0.446471, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.323183, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.256122, 1.52728, -0.279126, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.145877, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.369334, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.353053, 0.573764, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.049899, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.360359, 2.33578, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 1.11966, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 154, 0.39094, 0.813816, -1.64079, -0.666538, -0.710473, 0.310473, -1.7219, -0.531648, -0.732951, 0.839045, 1.12919, 0.455347, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.170929, -0.562032, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.003429, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.686602, -0.426812, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.176502, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.893146, 4.21041, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"154, 0.39094, 0.813816, -1.64079, -0.666538, -0.710473, 0.310473, -1.7219, -0.531648, -0.732951, 0.839045, 1.12919, 0.455347, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.170929, -0.562032, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.003429, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.686602, -0.426812, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.176502, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.893146, 4.21041, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 154, -1.1886, -1.06367, -0.041967, 0.695586, -0.698465, 1.53083, -0.047534, 0.724818, -0.748155, 1.1779, -0.05697, 0.682648, -0.054535, -0.297912, 3.30032, -0.126836, -0.225084, -0.014952, -0.08711, 4.80507, 6.30746, -0.238846, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.310112, -0.347995, -0.26576, -0.080866, 0.169132, 0.810402, -0.248182, -0.309461, -0.025787, -0.073582, 0.482029, 0.257109, -0.068453, 0.275021, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.437104, 0.223912, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.121323, 3.15296, 1.47183, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.368012, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.934907, 0.388597, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.26765, 2.12147, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.006515, -0.067914, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 2.85501, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"154, -1.1886, -1.06367, -0.041967, 0.695586, -0.698465, 1.53083, -0.047534, 0.724818, -0.748155, 1.1779, -0.05697, 0.682648, -0.054535, -0.297912, 3.30032, -0.126836, -0.225084, -0.014952, -0.08711, 4.80507, 6.30746, -0.238846, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.310112, -0.347995, -0.26576, -0.080866, 0.169132, 0.810402, -0.248182, -0.309461, -0.025787, -0.073582, 0.482029, 0.257109, -0.068453, 0.275021, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.437104, 0.223912, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.121323, 3.15296, 1.47183, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.368012, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.934907, 0.388597, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.26765, 2.12147, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.006515, -0.067914, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 2.85501, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 154, -1.15157, -1.02005, -1.01618, 0.097928, -0.782399, 0.57446, -1.05534, 0.108741, -0.810293, 0.943987, -1.00037, 1.00361, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.482305, -0.026624, 2.0742, -0.025787, -0.073582, -0.114571, -0.18949, 0.117669, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.335443, 0.688198, 0.211505, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.204604, 0.606112, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.492139, 0.671866, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.469526, 0.770399, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.27662, -0.267792, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 2.22266, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"154, -1.15157, -1.02005, -1.01618, 0.097928, -0.782399, 0.57446, -1.05534, 0.108741, -0.810293, 0.943987, -1.00037, 1.00361, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.482305, -0.026624, 2.0742, -0.025787, -0.073582, -0.114571, -0.18949, 0.117669, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.335443, 0.688198, 0.211505, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.204604, 0.606112, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.492139, 0.671866, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.469526, 0.770399, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.27662, -0.267792, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, 2.22266, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 154, -1.20723, -1.09429, -1.50419, 1.22849, -0.805012, 0.114292, -1.54243, 1.2543, -0.827149, -2.29449, 0.962344, -1.01808, -0.054535, -0.297912, 7.22131, 0.72564, -0.225084, -0.014952, -0.08711, 8.47309, 1.32524, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.336706, -0.26576, -0.080866, 0.043811, -0.488528, -0.101618, 0.340577, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.167677, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.503498, 1.15628, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.365149, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.449429, 0.508998, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.675042, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"154, -1.20723, -1.09429, -1.50419, 1.22849, -0.805012, 0.114292, -1.54243, 1.2543, -0.827149, -2.29449, 0.962344, -1.01808, -0.054535, -0.297912, 7.22131, 0.72564, -0.225084, -0.014952, -0.08711, 8.47309, 1.32524, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.336706, -0.26576, -0.080866, 0.043811, -0.488528, -0.101618, 0.340577, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.167677, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.503498, 1.15628, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.365149, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.449429, 0.508998, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.675042, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 187, -0.968154, -1.01388, -1.26356, -0.180891, 0.477614, -1.26133, -1.07671, -0.379589, 0.657215, -1.68291, 0.147208, -0.347202, -0.054535, -0.297912, 0.111173, 0.251092, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.238407, 9.05367, -0.360294, -0.309461, -0.025787, -0.073582, -0.114571, -0.183484, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.037823, 0.482813, -0.588346, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.962008, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"187, -0.968154, -1.01388, -1.26356, -0.180891, 0.477614, -1.26133, -1.07671, -0.379589, 0.657215, -1.68291, 0.147208, -0.347202, -0.054535, -0.297912, 0.111173, 0.251092, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.238407, 9.05367, -0.360294, -0.309461, -0.025787, -0.073582, -0.114571, -0.183484, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.037823, 0.482813, -0.588346, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.962008, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 187, -1.3323, -1.24755, -0.607825, 2.65695, 0.703227, -0.15947, -0.604988, 2.68385, 0.522032, -0.576659, 0.225626, -0.96705, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.190689, 5.16068, -0.306519, -0.309461, -0.025787, -0.073582, -0.114571, -0.135258, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.167362, 0.701038, -0.58426, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.26806, 0.864274, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"187, -1.3323, -1.24755, -0.607825, 2.65695, 0.703227, -0.15947, -0.604988, 2.68385, 0.522032, -0.576659, 0.225626, -0.96705, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.190689, 5.16068, -0.306519, -0.309461, -0.025787, -0.073582, -0.114571, -0.135258, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.167362, 0.701038, -0.58426, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.26806, 0.864274, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 187, -1.26295, -1.1949, 0.188035, 0.869489, 0.539821, -0.248755, 0.192229, 0.597752, 0.504515, -1.59238, 0.117644, -0.3477, -0.054535, -0.297912, -0.253789, -0.114712, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.187803, 6.90776, -0.372049, -0.309461, -0.025787, -0.073582, -0.114571, -0.179277, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.39629, 1.92278, -0.588191, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.990027, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"187, -1.26295, -1.1949, 0.188035, 0.869489, 0.539821, -0.248755, 0.192229, 0.597752, 0.504515, -1.59238, 0.117644, -0.3477, -0.054535, -0.297912, -0.253789, -0.114712, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.187803, 6.90776, -0.372049, -0.309461, -0.025787, -0.073582, -0.114571, -0.179277, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.39629, 1.92278, -0.588191, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.990027, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 187, 0.189193, 0.431793, -0.094988, -0.928706, -0.222455, 0.404622, -0.311857, -0.944098, -0.435483, -2.24379, 0.309364, -0.565134, -0.054535, -0.297912, -0.210445, 0.311778, -0.225084, -0.014952, -0.08711, -0.167402, -0.095851, -0.083394, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.040322, 10.7837, -0.248329, -0.309461, -0.025787, -0.073582, -0.114571, -0.112037, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.116138, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.06626, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"187, 0.189193, 0.431793, -0.094988, -0.928706, -0.222455, 0.404622, -0.311857, -0.944098, -0.435483, -2.24379, 0.309364, -0.565134, -0.054535, -0.297912, -0.210445, 0.311778, -0.225084, -0.014952, -0.08711, -0.167402, -0.095851, -0.083394, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.040322, 10.7837, -0.248329, -0.309461, -0.025787, -0.073582, -0.114571, -0.112037, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.116138, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.06626, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 187, -0.719123, -0.347052, -0.922501, 0.387585, -0.467881, -0.334079, -0.936474, 0.321417, -0.532326, 1.62443, 0.259005, -0.460678, -0.054535, -0.297912, 0.035702, -0.118882, -0.225084, -0.014952, -0.08711, -0.167402, -0.145331, -0.161746, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.293405, 8.57731, -0.409658, -0.309461, -0.025787, -0.073582, -0.114571, -0.176387, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.036817, 0.811663, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.861878, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"187, -0.719123, -0.347052, -0.922501, 0.387585, -0.467881, -0.334079, -0.936474, 0.321417, -0.532326, 1.62443, 0.259005, -0.460678, -0.054535, -0.297912, 0.035702, -0.118882, -0.225084, -0.014952, -0.08711, -0.167402, -0.145331, -0.161746, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.293405, 8.57731, -0.409658, -0.309461, -0.025787, -0.073582, -0.114571, -0.176387, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.036817, 0.811663, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.861878, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 187, 0.918818, 1.52657, -0.705596, -0.853346, -0.624229, 1.41574, -0.777739, -0.815337, -0.696263, -0.285641, 0.861544, 0.080392, -0.054535, -0.297912, 0.113704, 0.275364, -0.225084, -0.014952, -0.08711, -0.167402, -0.090332, -0.074656, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.083679, 10.3397, 0.219854, -0.309461, -0.025787, -0.073582, -0.114571, 0.146327, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.088448, -0.588565, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.06626, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"187, 0.918818, 1.52657, -0.705596, -0.853346, -0.624229, 1.41574, -0.777739, -0.815337, -0.696263, -0.285641, 0.861544, 0.080392, -0.054535, -0.297912, 0.113704, 0.275364, -0.225084, -0.014952, -0.08711, -0.167402, -0.090332, -0.074656, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.083679, 10.3397, 0.219854, -0.309461, -0.025787, -0.073582, -0.114571, 0.146327, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.088448, -0.588565, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.06626, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 187, 0.659079, 0.924838, -0.812022, 1.65361, -0.671486, 0.82797, -0.888722, 1.58354, -0.735595, -1.10516, 1.57967, 0.170705, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.200867, 5.50395, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.930552, 2.31082, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.811135, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"187, 0.659079, 0.924838, -0.812022, 1.65361, -0.671486, 0.82797, -0.888722, 1.58354, -0.735595, -1.10516, 1.57967, 0.170705, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.200867, 5.50395, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.930552, 2.31082, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.811135, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 187, 0.334118, 0.970807, -0.445423, 0.30508, -0.712632, -0.458384, -0.467165, 0.275185, -0.741214, -0.344779, 0.237834, -0.097441, -0.054535, -0.297912, -0.094738, -0.0757, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.400579, 9.3422, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.164592, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.142909, 0.488347, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.909039, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"187, 0.334118, 0.970807, -0.445423, 0.30508, -0.712632, -0.458384, -0.467165, 0.275185, -0.741214, -0.344779, 0.237834, -0.097441, -0.054535, -0.297912, -0.094738, -0.0757, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.400579, 9.3422, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.164592, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.142909, 0.488347, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.909039, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 104, 1.18948, 0.807698, -0.60291, 1.12434, 4.14774, 0.380075, -0.598531, 0.40545, 4.39044, 0.171329, 0.532504, 0.206411, -0.054535, -0.297912, -0.259644, -0.295439, -0.216584, -0.014952, -0.08711, -0.167097, -0.18794, 0.479414, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.063418, -0.323452, -0.317048, -0.207833, -0.080866, -0.213213, -0.307172, 0.791172, -0.030976, -0.025787, -0.073582, -0.114571, -0.154156, 0.271641, 0.768688, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, 2.11243, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.312227, -0.384261, -0.258531, -0.139878, 0.0, -0.06989, -0.228767, 0.348945, 0.654809, -0.080555, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.102056, 0.151796, 0.05122, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.254874, -0.38734, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.319808, 0.633783, 0.171976, -0.171207, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.139971, -0.132649, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265291, 0.149564, 0.025039, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.025824, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"104, 1.18948, 0.807698, -0.60291, 1.12434, 4.14774, 0.380075, -0.598531, 0.40545, 4.39044, 0.171329, 0.532504, 0.206411, -0.054535, -0.297912, -0.259644, -0.295439, -0.216584, -0.014952, -0.08711, -0.167097, -0.18794, 0.479414, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.063418, -0.323452, -0.317048, -0.207833, -0.080866, -0.213213, -0.307172, 0.791172, -0.030976, -0.025787, -0.073582, -0.114571, -0.154156, 0.271641, 0.768688, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, 2.11243, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.312227, -0.384261, -0.258531, -0.139878, 0.0, -0.06989, -0.228767, 0.348945, 0.654809, -0.080555, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.102056, 0.151796, 0.05122, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.254874, -0.38734, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.319808, 0.633783, 0.171976, -0.171207, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.139971, -0.132649, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265291, 0.149564, 0.025039, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.025824, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 104, 0.357172, 0.465572, -0.080056, -1.16408, -0.204218, -0.314339, -0.186557, -1.15463, -0.298318, -1.33951, -0.135082, -0.370464, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.195405, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.487388, 0.331904, 0.089049, -0.025787, -0.073582, -0.114571, -0.184638, 0.196897, 0.52886, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.636114, 0.012352, -0.19998, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, 0.012344, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.362169, 1.32428, 0.11915, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.159715, -0.15889, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.543874, 0.260241, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.113556, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"104, 0.357172, 0.465572, -0.080056, -1.16408, -0.204218, -0.314339, -0.186557, -1.15463, -0.298318, -1.33951, -0.135082, -0.370464, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.195405, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.487388, 0.331904, 0.089049, -0.025787, -0.073582, -0.114571, -0.184638, 0.196897, 0.52886, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.636114, 0.012352, -0.19998, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, 0.012344, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.362169, 1.32428, 0.11915, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.159715, -0.15889, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.543874, 0.260241, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.113556, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 104, -0.739947, -0.574434, -1.31283, -1.50999, -0.652275, -0.555111, -1.30571, -1.53666, -0.687339, 0.724403, 0.355769, -0.352433, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, 0.928429, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.414901, 0.252795, 0.262402, -0.025787, -0.073582, -0.114571, -0.182753, 0.786079, 0.337364, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.402262, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.30414, 0.214095, 0.058108, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.091867, -0.100296, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.382312, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.2844, -0.213817, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.703491, -0.32842, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"104, -0.739947, -0.574434, -1.31283, -1.50999, -0.652275, -0.555111, -1.30571, -1.53666, -0.687339, 0.724403, 0.355769, -0.352433, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, 0.928429, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.414901, 0.252795, 0.262402, -0.025787, -0.073582, -0.114571, -0.182753, 0.786079, 0.337364, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.402262, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.30414, 0.214095, 0.058108, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.091867, -0.100296, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.382312, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.2844, -0.213817, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.703491, -0.32842, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 179, 1.15777, 0.762956, 0.941066, 0.042635, 2.59502, 0.838383, 0.592866, -0.149112, 1.95318, -0.393348, -0.624985, 0.919383, -0.054535, 3.01379, -0.132385, 0.158707, -0.225084, -0.014952, -0.08711, -0.154335, 0.443119, -0.245432, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 4.60281, 7.00194, -0.080912, 2.66981, 4.58116, 3.51501, -0.080866, -0.224814, 0.117196, -0.105473, -0.014298, -0.025787, -0.073582, -0.114571, -0.18791, -0.146162, -0.200915, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, 0.99891, -0.014952, -0.048732, 2.37191, 2.1846, -0.014957, -0.050171, -0.114846, 1.34021, 3.7299, 0.748032, 0.088724, 0.0, -0.06989, -0.262073, -0.478638, -0.080996, -0.239753, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.085074, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.145034, 0.197176, -0.134898, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.975717, -0.458132, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.106999, -0.088072, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.698938, -0.320336, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"179, 1.15777, 0.762956, 0.941066, 0.042635, 2.59502, 0.838383, 0.592866, -0.149112, 1.95318, -0.393348, -0.624985, 0.919383, -0.054535, 3.01379, -0.132385, 0.158707, -0.225084, -0.014952, -0.08711, -0.154335, 0.443119, -0.245432, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 4.60281, 7.00194, -0.080912, 2.66981, 4.58116, 3.51501, -0.080866, -0.224814, 0.117196, -0.105473, -0.014298, -0.025787, -0.073582, -0.114571, -0.18791, -0.146162, -0.200915, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, 0.99891, -0.014952, -0.048732, 2.37191, 2.1846, -0.014957, -0.050171, -0.114846, 1.34021, 3.7299, 0.748032, 0.088724, 0.0, -0.06989, -0.262073, -0.478638, -0.080996, -0.239753, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.085074, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.145034, 0.197176, -0.134898, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.975717, -0.458132, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.106999, -0.088072, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.698938, -0.320336, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 179, 0.372472, -0.164138, -0.965374, 0.657627, 0.405716, 0.270267, -0.823084, 0.662121, 0.236133, -1.39146, -2.47482, 0.987883, -0.054535, 3.92709, 0.177596, 0.820469, -0.225084, -0.014952, -0.08711, -0.058604, 5.20548, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 6.15984, 7.59184, -0.080912, 1.80191, 5.56466, 4.5673, -0.080866, -0.224814, 0.387261, 0.246791, -0.070782, -0.025787, -0.073582, -0.114571, -0.183759, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 2.4662, 1.57555, 0.146312, -0.050171, -0.114846, 0.957329, 2.86585, 0.86672, -0.139878, 0.0, -0.06989, -0.263975, -0.617228, -0.284796, -0.182027, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.091638, -0.071579, -0.017934, -0.063725, -0.182197, -0.146731, 0.127523, -0.10225, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.02376, -0.515558, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.126933, -0.037741, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.714027, -0.370511, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"179, 0.372472, -0.164138, -0.965374, 0.657627, 0.405716, 0.270267, -0.823084, 0.662121, 0.236133, -1.39146, -2.47482, 0.987883, -0.054535, 3.92709, 0.177596, 0.820469, -0.225084, -0.014952, -0.08711, -0.058604, 5.20548, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 6.15984, 7.59184, -0.080912, 1.80191, 5.56466, 4.5673, -0.080866, -0.224814, 0.387261, 0.246791, -0.070782, -0.025787, -0.073582, -0.114571, -0.183759, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 2.4662, 1.57555, 0.146312, -0.050171, -0.114846, 0.957329, 2.86585, 0.86672, -0.139878, 0.0, -0.06989, -0.263975, -0.617228, -0.284796, -0.182027, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.091638, -0.071579, -0.017934, -0.063725, -0.182197, -0.146731, 0.127523, -0.10225, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.02376, -0.515558, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.126933, -0.037741, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.714027, -0.370511, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 179, 0.633551, -1.2E-4, -0.832889, -1.34832, 0.035796, -1.06986, -0.580356, -1.39564, -0.046462, -1.41861, -2.82001, 0.811438, -0.054535, 3.94585, -0.211346, 0.060008, -0.225084, -0.014952, -0.08711, -0.159741, 0.402876, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 13.7686, 14.225, -0.080912, 1.84494, 10.8589, 7.39074, -0.080866, -0.224814, -0.13294, -0.290424, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, 0.071686, 1.79949, 0.714618, -0.091775, -0.050171, -0.114846, 0.732717, 3.59681, -0.240424, -0.139878, 0.0, -0.06989, -0.263975, -0.712785, -0.46066, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.123805, -0.376747, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.958489, -0.636401, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.714254, -0.412472, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"179, 0.633551, -1.2E-4, -0.832889, -1.34832, 0.035796, -1.06986, -0.580356, -1.39564, -0.046462, -1.41861, -2.82001, 0.811438, -0.054535, 3.94585, -0.211346, 0.060008, -0.225084, -0.014952, -0.08711, -0.159741, 0.402876, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 13.7686, 14.225, -0.080912, 1.84494, 10.8589, 7.39074, -0.080866, -0.224814, -0.13294, -0.290424, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, 0.071686, 1.79949, 0.714618, -0.091775, -0.050171, -0.114846, 0.732717, 3.59681, -0.240424, -0.139878, 0.0, -0.06989, -0.263975, -0.712785, -0.46066, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.123805, -0.376747, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.958489, -0.636401, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.714254, -0.412472, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 179, 1.32638, 0.470365, -0.490566, -0.781827, -0.500221, -1.62776, -0.560867, -0.792077, -0.495307, -0.571996, -2.71622, 1.6496, -0.054535, 5.0308, -0.261335, -0.225075, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 10.109, 6.96726, -0.080912, 3.61161, 13.6468, 8.39195, -0.080866, -0.224814, -0.199984, -0.03413, -0.215268, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 0.956266, 0.140336, -0.091775, -0.050171, -0.114846, 1.56642, 3.12016, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.674019, -0.471183, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.233733, -0.372551, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.06323, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"179, 1.32638, 0.470365, -0.490566, -0.781827, -0.500221, -1.62776, -0.560867, -0.792077, -0.495307, -0.571996, -2.71622, 1.6496, -0.054535, 5.0308, -0.261335, -0.225075, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 10.109, 6.96726, -0.080912, 3.61161, 13.6468, 8.39195, -0.080866, -0.224814, -0.199984, -0.03413, -0.215268, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 0.956266, 0.140336, -0.091775, -0.050171, -0.114846, 1.56642, 3.12016, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.674019, -0.471183, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.233733, -0.372551, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.06323, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 179, 0.898448, 0.473163, 2.0027, -1.14657, -0.609489, 1.17599, 2.03177, -1.14457, -0.648999, -1.05037, -0.897514, 1.03303, -0.054535, 2.3351, 0.115279, 0.474349, -0.225084, -0.014952, -0.08711, 0.012676, 8.23187, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 13.2501, 14.0621, -0.080912, 0.739003, 4.12691, 0.93271, -0.080866, -0.224814, -0.009511, 0.31164, -0.257879, -0.025787, -0.073582, -0.114571, -0.114545, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 6.35207, 0.840782, -0.091775, -0.050171, -0.114846, 0.372951, 3.90814, 0.083498, -0.139878, 0.0, -0.06989, -0.263975, -0.707884, -0.008585, 0.431931, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.390852, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.04473, -0.499608, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.744353, -0.381947, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"179, 0.898448, 0.473163, 2.0027, -1.14657, -0.609489, 1.17599, 2.03177, -1.14457, -0.648999, -1.05037, -0.897514, 1.03303, -0.054535, 2.3351, 0.115279, 0.474349, -0.225084, -0.014952, -0.08711, 0.012676, 8.23187, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 13.2501, 14.0621, -0.080912, 0.739003, 4.12691, 0.93271, -0.080866, -0.224814, -0.009511, 0.31164, -0.257879, -0.025787, -0.073582, -0.114571, -0.114545, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 6.35207, 0.840782, -0.091775, -0.050171, -0.114846, 0.372951, 3.90814, 0.083498, -0.139878, 0.0, -0.06989, -0.263975, -0.707884, -0.008585, 0.431931, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.390852, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.04473, -0.499608, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.744353, -0.381947, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 179, 1.83306, 1.49946, 1.04051, -0.612509, -0.793839, 0.555338, 1.14355, -0.612942, -0.822852, -1.37869, -1.68539, 1.00229, -0.054535, 7.08181, -0.190964, 0.045821, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 3.27516, 1.04508, -0.080912, 5.74526, 7.83541, 3.33682, -0.080866, -0.224814, 1.49058, 0.300959, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 4.33995, 0.195692, -0.091775, -0.050171, -0.114846, 2.43009, 1.96131, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.482659, -0.51387, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.05606, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\n",
      "Warning: skipping line: \"179, 1.83306, 1.49946, 1.04051, -0.612509, -0.793839, 0.555338, 1.14355, -0.612942, -0.822852, -1.37869, -1.68539, 1.00229, -0.054535, 7.08181, -0.190964, 0.045821, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 3.27516, 1.04508, -0.080912, 5.74526, 7.83541, 3.33682, -0.080866, -0.224814, 1.49058, 0.300959, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 4.33995, 0.195692, -0.091775, -0.050171, -0.114846, 2.43009, 1.96131, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.482659, -0.51387, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.05606, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 0\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 36, -1.30008, -1.21562, -0.121405, 0.670172, 1.93266, -0.409075, 0.112384, 0.576763, 1.65671, 0.482309, -1.41465, -1.99278, -0.054535, -0.275104, -0.261335, -0.297021, -0.069646, -0.014952, -0.08711, -0.167402, -0.200329, -0.242815, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.029815, -0.213975, -0.338628, -0.26576, -0.080866, -0.196028, -0.358276, -0.288078, -0.28482, -0.025787, -0.073582, -0.114571, -0.18949, -0.159999, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.019567, -0.257512, -0.40847, -0.263784, -0.139878, 0.0, -0.06989, -0.112706, -0.106807, -0.427326, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 6.55148, -0.254224, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.04651, -0.066523, -0.651015, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 0.076722, -0.530415, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"36, -1.30008, -1.21562, -0.121405, 0.670172, 1.93266, -0.409075, 0.112384, 0.576763, 1.65671, 0.482309, -1.41465, -1.99278, -0.054535, -0.275104, -0.261335, -0.297021, -0.069646, -0.014952, -0.08711, -0.167402, -0.200329, -0.242815, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.029815, -0.213975, -0.338628, -0.26576, -0.080866, -0.196028, -0.358276, -0.288078, -0.28482, -0.025787, -0.073582, -0.114571, -0.18949, -0.159999, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.019567, -0.257512, -0.40847, -0.263784, -0.139878, 0.0, -0.06989, -0.112706, -0.106807, -0.427326, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 6.55148, -0.254224, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.04651, -0.066523, -0.651015, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 0.076722, -0.530415, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 36, 0.412151, 0.248678, 0.087567, -0.705445, 0.122373, 1.25041, 0.124628, -0.870445, 0.178953, -0.565267, -0.07338, 0.787888, -0.054535, -0.297912, -0.261335, -0.302051, -0.16643, -0.014952, -0.08711, -0.167402, -0.200329, -0.241334, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.340454, -0.26576, -0.080866, -0.224814, -0.496763, 0.27997, -0.076883, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.086698, -0.3317, -0.40648, -0.263784, -0.139878, 0.0, -0.06989, -0.232953, -0.373676, 0.181038, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 3.37105, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.01354, -0.363943, -0.602023, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.28311, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"36, 0.412151, 0.248678, 0.087567, -0.705445, 0.122373, 1.25041, 0.124628, -0.870445, 0.178953, -0.565267, -0.07338, 0.787888, -0.054535, -0.297912, -0.261335, -0.302051, -0.16643, -0.014952, -0.08711, -0.167402, -0.200329, -0.241334, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.340454, -0.26576, -0.080866, -0.224814, -0.496763, 0.27997, -0.076883, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.086698, -0.3317, -0.40648, -0.263784, -0.139878, 0.0, -0.06989, -0.232953, -0.373676, 0.181038, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 3.37105, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.01354, -0.363943, -0.602023, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.28311, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 36, -0.762226, -0.653682, 1.42257, -0.697554, -0.349166, 0.141475, 1.46946, -0.703282, -0.415652, 1.28801, 0.415045, -0.67973, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.771256, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 1.42228, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.32345, -0.727412, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.253213, 1.19671, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"36, -0.762226, -0.653682, 1.42257, -0.697554, -0.349166, 0.141475, 1.46946, -0.703282, -0.415652, 1.28801, 0.415045, -0.67973, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.771256, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 1.42228, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.32345, -0.727412, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.253213, 1.19671, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 36, -0.613901, -0.389387, -1.00458, -1.42421, -0.501575, -0.459229, -1.05663, -1.43842, -0.549513, 1.31054, 0.460367, -0.479456, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, 0.066549, -0.32638, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.228014, -0.770937, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 1.30582, -0.252872, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.17431, -0.235747, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.263122, 0.870615, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"36, -0.613901, -0.389387, -1.00458, -1.42421, -0.501575, -0.459229, -1.05663, -1.43842, -0.549513, 1.31054, 0.460367, -0.479456, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, 0.066549, -0.32638, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.228014, -0.770937, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 1.30582, -0.252872, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.17431, -0.235747, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.263122, 0.870615, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 36, 1.9872, 1.35501, 1.78061, -1.35114, -0.473323, 0.280444, 1.79439, -1.32314, -0.555462, -1.27484, -0.287562, 0.26143, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 1.10092, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.65085, -1.04355, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.40354, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"36, 1.9872, 1.35501, 1.78061, -1.35114, -0.473323, 0.280444, 1.79439, -1.32314, -0.555462, -1.27484, -0.287562, 0.26143, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 1.10092, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.65085, -1.04355, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.40354, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 36, -0.320044, -0.607991, 1.82684, 0.712412, -0.716281, -0.174449, 1.86771, 0.750453, -0.710145, 0.107434, -1.07375, -1.52852, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.415317, -0.128888, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.402275, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.505823, 0.038065, -0.241878, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 6.43213, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.20001, -0.229102, -0.14581, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 0.496146, -0.613044, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"36, -0.320044, -0.607991, 1.82684, 0.712412, -0.716281, -0.174449, 1.86771, 0.750453, -0.710145, 0.107434, -1.07375, -1.52852, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.415317, -0.128888, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.402275, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.505823, 0.038065, -0.241878, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 6.43213, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.20001, -0.229102, -0.14581, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, 0.496146, -0.613044, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 36, 1.06176, 1.24863, -0.998227, -0.994188, -0.692006, -1.29078, -1.0705, -1.05685, -0.717747, 0.842892, 0.227216, -0.19144, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.496379, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.109142, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.252792, -0.507541, -0.563993, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 1.74285, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.14269, -0.837539, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.09441, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"36, 1.06176, 1.24863, -0.998227, -0.994188, -0.692006, -1.29078, -1.0705, -1.05685, -0.717747, 0.842892, 0.227216, -0.19144, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.496379, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.109142, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.252792, -0.507541, -0.563993, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 1.74285, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.14269, -0.837539, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.09441, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 36, 0.088004, -0.058165, 1.99945, 0.148861, -0.72089, -0.40739, 2.09601, 0.342469, -0.745841, -0.110845, -0.60964, -0.591387, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.36908, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.407036, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.416644, 0.773143, -0.237516, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 6.14916, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.461384, -0.830211, -0.464563, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.392185, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"36, 0.088004, -0.058165, 1.99945, 0.148861, -0.72089, -0.40739, 2.09601, 0.342469, -0.745841, -0.110845, -0.60964, -0.591387, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.36908, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.407036, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.416644, 0.773143, -0.237516, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 6.14916, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.461384, -0.830211, -0.464563, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.392185, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 36, -0.120475, -0.380149, -1.54255, -0.400738, -0.789131, 1.22627, -1.61098, -0.385894, -0.797072, 0.491751, -0.445571, -1.38244, -0.054535, -0.084922, -0.261335, -0.286395, 2.31026, -0.014952, -0.08711, -0.167402, -0.200329, 0.03229, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.014997, -0.309328, -0.210312, -0.26576, -0.080866, -0.224814, 0.037341, 4.99697, 0.24898, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, 0.735577, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.097507, 0.360445, 0.882776, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 1.52877, -0.230535, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.20584, -0.645919, -0.648246, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.744001, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"36, -0.120475, -0.380149, -1.54255, -0.400738, -0.789131, 1.22627, -1.61098, -0.385894, -0.797072, 0.491751, -0.445571, -1.38244, -0.054535, -0.084922, -0.261335, -0.286395, 2.31026, -0.014952, -0.08711, -0.167402, -0.200329, 0.03229, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, 0.014997, -0.309328, -0.210312, -0.26576, -0.080866, -0.224814, 0.037341, 4.99697, 0.24898, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, 0.735577, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.097507, 0.360445, 0.882776, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 1.52877, -0.230535, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.20584, -0.645919, -0.648246, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.744001, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 36, 0.767865, 1.28863, 0.378575, -1.05023, -0.830498, -0.601712, 0.469557, -0.977866, -0.830454, 0.415551, 0.020552, -0.220024, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 1.1592, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.62076, -0.960539, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.33434, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"36, 0.767865, 1.28863, 0.378575, -1.05023, -0.830498, -0.601712, 0.469557, -0.977866, -0.830454, 0.415551, 0.020552, -0.220024, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, 1.1592, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.62076, -0.960539, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.33434, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 42, -0.173597, -0.304188, 0.873782, -0.379411, 1.66667, 0.70089, 0.676525, -0.415611, 1.34866, 0.595118, 0.56107, 0.375358, -0.054535, -0.278693, -0.261335, -0.302051, -0.208427, -0.014952, -0.08711, -0.167402, -0.200329, -0.217555, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.167629, 0.226935, -0.080912, -0.321058, -0.167121, 0.946013, -0.080866, -0.224814, -0.402439, -0.074796, 0.165075, -0.025787, -0.073582, -0.114571, -0.144968, -0.110436, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 0.128506, 0.371806, -0.091775, -0.050171, -0.114846, -0.273241, 1.01914, 0.413602, -0.139878, 0.0, -0.06989, -0.263975, 0.289193, 1.03581, -0.210258, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.095824, -0.1442, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.08688, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.154768, 1.1704, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.307848, 0.508152, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.139249, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.133624, -0.159266, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.689396, -0.394206, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"42, -0.173597, -0.304188, 0.873782, -0.379411, 1.66667, 0.70089, 0.676525, -0.415611, 1.34866, 0.595118, 0.56107, 0.375358, -0.054535, -0.278693, -0.261335, -0.302051, -0.208427, -0.014952, -0.08711, -0.167402, -0.200329, -0.217555, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.167629, 0.226935, -0.080912, -0.321058, -0.167121, 0.946013, -0.080866, -0.224814, -0.402439, -0.074796, 0.165075, -0.025787, -0.073582, -0.114571, -0.144968, -0.110436, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, 0.128506, 0.371806, -0.091775, -0.050171, -0.114846, -0.273241, 1.01914, 0.413602, -0.139878, 0.0, -0.06989, -0.263975, 0.289193, 1.03581, -0.210258, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.095824, -0.1442, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.08688, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.154768, 1.1704, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.307848, 0.508152, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.139249, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.133624, -0.159266, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.689396, -0.394206, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 42, 0.818035, 0.465671, -1.14257, 0.104118, 1.40744, 0.415249, -1.05203, 0.026478, 1.1715, -0.245412, -0.981869, 0.866181, -0.054535, -0.297912, -0.261335, -0.302051, -0.208704, -0.014952, -0.08711, -0.167402, -0.200329, -0.233767, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.148599, 0.122482, -0.080912, -0.308008, -0.075158, 0.398079, -0.080866, -0.224814, -0.45149, -0.228496, 0.214478, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.172846, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.086366, 0.132445, -0.091775, -0.050171, -0.114846, -0.23595, 1.20649, 0.257553, -0.139878, 0.0, -0.06989, -0.263975, 0.894918, 1.30558, -0.140277, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.126009, -0.135736, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.186774, 0.375922, -0.185997, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.635453, 0.03933, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.112904, -0.192472, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.178381, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.726065, -0.404997, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"42, 0.818035, 0.465671, -1.14257, 0.104118, 1.40744, 0.415249, -1.05203, 0.026478, 1.1715, -0.245412, -0.981869, 0.866181, -0.054535, -0.297912, -0.261335, -0.302051, -0.208704, -0.014952, -0.08711, -0.167402, -0.200329, -0.233767, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.148599, 0.122482, -0.080912, -0.308008, -0.075158, 0.398079, -0.080866, -0.224814, -0.45149, -0.228496, 0.214478, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.172846, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.086366, 0.132445, -0.091775, -0.050171, -0.114846, -0.23595, 1.20649, 0.257553, -0.139878, 0.0, -0.06989, -0.263975, 0.894918, 1.30558, -0.140277, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.126009, -0.135736, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.186774, 0.375922, -0.185997, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.635453, 0.03933, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.112904, -0.192472, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.178381, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.726065, -0.404997, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 42, 1.60957, 1.21221, -0.164336, -0.405267, -0.112667, 0.656806, -0.088975, -0.326099, -0.215688, -0.441832, 0.154412, 0.387734, -0.054535, 0.163932, -0.261335, -0.291093, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.102, 0.062615, -0.080912, -0.253234, 0.649978, 2.08686, -0.080866, -0.224814, -0.412634, -0.057377, -0.182596, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.14411, -0.154356, -0.091775, -0.050171, -0.114846, -0.229579, 1.58272, 0.677335, -0.139878, 0.0, -0.06989, -0.263975, 0.676247, 1.65792, -0.216492, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.181088, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.079528, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.029779, 0.794914, -0.144666, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.177966, -0.088317, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.73449, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"42, 1.60957, 1.21221, -0.164336, -0.405267, -0.112667, 0.656806, -0.088975, -0.326099, -0.215688, -0.441832, 0.154412, 0.387734, -0.054535, 0.163932, -0.261335, -0.291093, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.102, 0.062615, -0.080912, -0.253234, 0.649978, 2.08686, -0.080866, -0.224814, -0.412634, -0.057377, -0.182596, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.14411, -0.154356, -0.091775, -0.050171, -0.114846, -0.229579, 1.58272, 0.677335, -0.139878, 0.0, -0.06989, -0.263975, 0.676247, 1.65792, -0.216492, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.181088, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.079528, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.029779, 0.794914, -0.144666, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.177966, -0.088317, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.73449, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 42, 1.47118, 0.946737, 1.88402, 0.048742, -0.374254, 0.041362, 1.98584, 0.07174, -0.265596, -0.401221, -0.30349, 0.830045, -0.054535, -0.297912, -0.261335, -0.290863, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.081053, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.202201, -0.080912, -0.327191, -0.06786, 0.349894, -0.080866, -0.224814, -0.477165, 0.150424, 0.478818, -0.025787, -0.073582, -0.114571, -0.161744, -0.064638, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.057335, -0.077711, -0.091775, -0.050171, -0.114846, -0.3317, 1.45522, 0.787754, -0.139878, 0.0, -0.06989, -0.263975, 0.248016, 1.00944, -0.196979, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.160498, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.149381, 1.48072, -0.18226, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.15653, 0.44233, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.172109, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.715593, -0.400004, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"42, 1.47118, 0.946737, 1.88402, 0.048742, -0.374254, 0.041362, 1.98584, 0.07174, -0.265596, -0.401221, -0.30349, 0.830045, -0.054535, -0.297912, -0.261335, -0.290863, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.081053, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.202201, -0.080912, -0.327191, -0.06786, 0.349894, -0.080866, -0.224814, -0.477165, 0.150424, 0.478818, -0.025787, -0.073582, -0.114571, -0.161744, -0.064638, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.057335, -0.077711, -0.091775, -0.050171, -0.114846, -0.3317, 1.45522, 0.787754, -0.139878, 0.0, -0.06989, -0.263975, 0.248016, 1.00944, -0.196979, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.160498, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.149381, 1.48072, -0.18226, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.15653, 0.44233, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.172109, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.715593, -0.400004, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 42, 0.523082, -0.136362, 2.0061, 0.313547, -0.823907, 0.077165, 2.06173, 0.325604, -0.84698, 0.762615, 0.872296, 0.713317, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.367794, 0.513546, 0.014645, -0.025787, -0.073582, -0.114571, 0.147141, -0.128536, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, 0.982755, -0.107612, -0.139878, 0.0, -0.06989, -0.263975, 1.40541, 2.43754, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.248111, 0.548075, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.329759, -0.013384, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.659094, -0.285739, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"42, 0.523082, -0.136362, 2.0061, 0.313547, -0.823907, 0.077165, 2.06173, 0.325604, -0.84698, 0.762615, 0.872296, 0.713317, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.367794, 0.513546, 0.014645, -0.025787, -0.073582, -0.114571, 0.147141, -0.128536, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, 0.982755, -0.107612, -0.139878, 0.0, -0.06989, -0.263975, 1.40541, 2.43754, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.248111, 0.548075, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.329759, -0.013384, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.659094, -0.285739, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, 0.484679, 0.965724, -1.10094, 0.157015, 1.17994, 0.847716, -0.928876, -0.141124, 0.84495, 0.951222, 0.475551, -0.896125, -0.054535, -0.297912, 0.694146, 0.165954, -0.225084, -0.014952, 2.49E-4, -0.167402, 1.68445, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.073878, -0.312611, -0.347995, -0.26576, -0.080866, 0.355943, 0.458342, -0.375537, -0.309461, -0.025787, -0.073582, 0.183261, -0.156169, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.08023, -0.648078, -0.574408, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.84562, 0.418423, -0.64901, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.170149, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.252749, 0.504746, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.112074, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, 0.484679, 0.965724, -1.10094, 0.157015, 1.17994, 0.847716, -0.928876, -0.141124, 0.84495, 0.951222, 0.475551, -0.896125, -0.054535, -0.297912, 0.694146, 0.165954, -0.225084, -0.014952, 2.49E-4, -0.167402, 1.68445, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.073878, -0.312611, -0.347995, -0.26576, -0.080866, 0.355943, 0.458342, -0.375537, -0.309461, -0.025787, -0.073582, 0.183261, -0.156169, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.08023, -0.648078, -0.574408, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.84562, 0.418423, -0.64901, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.170149, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.252749, 0.504746, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.112074, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, -1.20557, -1.10339, 0.763797, -0.166505, 0.278974, -0.644404, 0.741377, -0.359446, 0.258278, -0.176265, 0.074678, -0.461557, -0.054535, -0.297912, -0.23021, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, 1.32774, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.121064, -0.44235, -0.443591, -0.309461, -0.025787, -0.073582, -0.025284, 0.471319, -0.151788, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.084476, -0.490845, -0.588784, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.149274, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.27847, 0.562708, -0.655941, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.167776, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.223144, 0.779756, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, -1.20557, -1.10339, 0.763797, -0.166505, 0.278974, -0.644404, 0.741377, -0.359446, 0.258278, -0.176265, 0.074678, -0.461557, -0.054535, -0.297912, -0.23021, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, 1.32774, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.121064, -0.44235, -0.443591, -0.309461, -0.025787, -0.073582, -0.025284, 0.471319, -0.151788, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.084476, -0.490845, -0.588784, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.149274, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.27847, 0.562708, -0.655941, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.167776, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.223144, 0.779756, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, 0.14773, 0.305854, 0.199713, -0.901055, -0.63826, -0.071056, 0.222283, -0.92534, -0.625532, 0.973921, 1.11867, -0.178833, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.45632, -0.20452, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.68043, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, 0.14773, 0.305854, 0.199713, -0.901055, -0.63826, -0.071056, 0.222283, -0.92534, -0.625532, 0.973921, 1.11867, -0.178833, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.45632, -0.20452, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.68043, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, -1.27187, -1.17175, -1.0479, -1.53617, -0.622929, -1.02234, -1.06045, -1.56189, -0.650652, 0.16737, 0.00992, -1.73688, -0.054535, -0.297912, 0.017043, -0.298206, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.052527, -0.327191, -0.347995, -0.26576, -0.080866, 1.20483, -0.337698, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.110818, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 3.21684, -0.397359, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 5.50271, -0.984334, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, -1.27187, -1.17175, -1.0479, -1.53617, -0.622929, -1.02234, -1.06045, -1.56189, -0.650652, 0.16737, 0.00992, -1.73688, -0.054535, -0.297912, 0.017043, -0.298206, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.052527, -0.327191, -0.347995, -0.26576, -0.080866, 1.20483, -0.337698, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.110818, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 3.21684, -0.397359, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 5.50271, -0.984334, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, 0.113017, 0.362676, 1.40208, -0.817464, -0.618305, 1.83569, 1.46825, -0.853317, -0.656931, 0.906737, 0.914872, -0.21651, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.81611, -0.564517, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.262865, 1.79758, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, 0.113017, 0.362676, 1.40208, -0.817464, -0.618305, 1.83569, 1.46825, -0.853317, -0.656931, 0.906737, 0.914872, -0.21651, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.81611, -0.564517, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.262865, 1.79758, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, -1.31027, -1.22248, 0.602723, -1.48758, -0.642266, -0.486079, 0.612303, -1.50938, -0.684034, 0.239138, 0.329763, -1.52227, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.262441, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 5.03318, -0.565894, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.250371, 0.770794, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, -1.31027, -1.22248, 0.602723, -1.48758, -0.642266, -0.486079, 0.612303, -1.50938, -0.684034, 0.239138, 0.329763, -1.52227, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.262441, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 5.03318, -0.565894, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.250371, 0.770794, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, 1.09292, 1.73847, 2.06353, 0.129464, -0.701333, 0.303606, 2.13874, 0.195723, -0.741875, 1.20611, 0.496085, -0.534335, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.247876, -0.512689, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.88613, 1.1722, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.208219, 0.215549, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, 1.09292, 1.73847, 2.06353, 0.129464, -0.701333, 0.303606, 2.13874, 0.195723, -0.741875, 1.20611, 0.496085, -0.534335, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.247876, -0.512689, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.88613, 1.1722, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.208219, 0.215549, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, 0.023359, 0.420672, 1.59776, -1.50676, -0.71871, 0.06942, 1.63447, -1.5429, -0.764681, 0.572761, 0.486946, -1.04854, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.259036, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 5.03148, -0.528565, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.247267, 0.740333, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, 0.023359, 0.420672, 1.59776, -1.50676, -0.71871, 0.06942, 1.63447, -1.5429, -0.764681, 0.572761, 0.486946, -1.04854, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.259036, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 5.03148, -0.528565, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.247267, 0.740333, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, 1.92401, 2.45123, 0.070194, 1.03682, -0.750789, 0.847697, 0.094491, 1.05902, -0.781537, -0.568616, 0.412145, -0.569231, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.031222, -0.276354, -0.347995, -0.26576, -0.080866, 0.390077, 0.575831, -0.440028, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.328734, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.035907, 0.189401, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.466052, 1.00144, -0.653631, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.136958, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.512083, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, 1.92401, 2.45123, 0.070194, 1.03682, -0.750789, 0.847697, 0.094491, 1.05902, -0.781537, -0.568616, 0.412145, -0.569231, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.031222, -0.276354, -0.347995, -0.26576, -0.080866, 0.390077, 0.575831, -0.440028, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.328734, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.035907, 0.189401, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.466052, 1.00144, -0.653631, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.136958, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.512083, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, 0.353756, 0.87279, 1.55291, 1.19726, -0.766299, 0.177566, 1.61598, 1.22939, -0.806657, 0.851164, 0.387743, -0.871096, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.836176, -0.248119, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.257261, 2.46128, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, 0.353756, 0.87279, 1.55291, 1.19726, -0.766299, 0.177566, 1.61598, 1.22939, -0.806657, 0.851164, 0.387743, -0.871096, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.836176, -0.248119, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.257261, 2.46128, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, -0.025228, 0.325031, 0.683678, 1.02062, -0.783778, 0.952434, 0.72611, 1.04589, -0.807979, 1.2627, 0.397387, -0.803251, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.167772, -0.778091, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.37999, 0.599446, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.04783, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, -0.025228, 0.325031, 0.683678, 1.02062, -0.783778, 0.952434, 0.72611, 1.04589, -0.807979, 1.2627, 0.397387, -0.803251, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.167772, -0.778091, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.37999, 0.599446, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.04783, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, 0.914437, 0.889137, -0.549851, -0.535681, -0.806253, 1.86098, -0.563881, -0.54549, -0.831776, -1.49276, 0.207777, -0.449074, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.16813, 0.769359, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.248766, 1.04083, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, 0.914437, 0.889137, -0.549851, -0.535681, -0.806253, 1.86098, -0.563881, -0.54549, -0.831776, -1.49276, 0.207777, -0.449074, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.16813, 0.769359, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.248766, 1.04083, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 15, 1.12466, 2.14914, 0.277688, 0.614981, -0.835569, -0.665657, 0.308011, 0.63132, -0.854582, -0.170957, 0.529973, -0.297702, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.036427, -0.327191, -0.347995, -0.26576, -0.080866, -0.004286, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.015489, -0.681863, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.94749, -0.125906, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.752384, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"15, 1.12466, 2.14914, 0.277688, 0.614981, -0.835569, -0.665657, 0.308011, 0.63132, -0.854582, -0.170957, 0.529973, -0.297702, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.036427, -0.327191, -0.347995, -0.26576, -0.080866, -0.004286, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.015489, -0.681863, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.94749, -0.125906, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.752384, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 19, -1.26201, -1.16344, 0.049391, -0.332285, 5.74414, 0.80993, 0.237926, -0.099988, 3.67684, 1.35392, -0.445739, -0.619065, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.194787, -0.207686, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.528588, -0.439494, -0.283365, -0.025787, -0.073582, -0.114571, -0.18949, -0.104969, -0.009264, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.325417, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.779698, -0.557947, -0.116953, -0.04461, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.149384, -0.083705, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.027894, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.963472, -0.593417, -0.147559, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.191706, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, 2.77205, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265334, 3.23189, -0.389725, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"19, -1.26201, -1.16344, 0.049391, -0.332285, 5.74414, 0.80993, 0.237926, -0.099988, 3.67684, 1.35392, -0.445739, -0.619065, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.194787, -0.207686, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.528588, -0.439494, -0.283365, -0.025787, -0.073582, -0.114571, -0.18949, -0.104969, -0.009264, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.325417, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.779698, -0.557947, -0.116953, -0.04461, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.149384, -0.083705, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.027894, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.963472, -0.593417, -0.147559, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.191706, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, 2.77205, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265334, 3.23189, -0.389725, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 19, 1.70478, 1.99807, 0.339593, -0.296179, 0.179559, -1.05633, 0.441631, -0.338243, 0.037821, -0.102276, 0.144795, 0.207402, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.0635, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, 2.71452, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 3.44246, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"19, 1.70478, 1.99807, 0.339593, -0.296179, 0.179559, -1.05633, 0.441631, -0.338243, 0.037821, -0.102276, 0.144795, 0.207402, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.0635, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, 2.71452, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 3.44246, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 19, -0.500836, -0.5768, 1.32467, 1.09703, -0.389845, -1.08144, 1.36137, 1.1025, -0.359794, 1.15606, -0.672851, -0.491145, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.326191, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 0.189055, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.869108, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, 1.70766, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 3.27868, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"19, -0.500836, -0.5768, 1.32467, 1.09703, -0.389845, -1.08144, 1.36137, 1.1025, -0.359794, 1.15606, -0.672851, -0.491145, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.326191, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 0.189055, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.869108, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, 1.70766, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 3.27868, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 79, -1.2406, -1.12937, 0.039662, -0.149551, 6.36055, 1.17506, 0.169647, -0.059584, 4.16271, 1.58204, -0.045042, -0.731408, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263962, -0.784741, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.360737, -1.01705, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.210502, 3.60895, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"79, -1.2406, -1.12937, 0.039662, -0.149551, 6.36055, 1.17506, 0.169647, -0.059584, 4.16271, 1.58204, -0.045042, -0.731408, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263962, -0.784741, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.360737, -1.01705, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.210502, 3.60895, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 79, 1.66494, 2.36385, 0.763442, -0.47332, 0.343914, -0.011725, 0.810288, -0.503751, 0.039143, -0.102568, 1.99368, 0.734557, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.06595, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.263937, 3.66308, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"79, 1.66494, 2.36385, 0.763442, -0.47332, 0.343914, -0.011725, 0.810288, -0.503751, 0.039143, -0.102568, 1.99368, 0.734557, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.06595, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.263937, 3.66308, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 79, 0.198915, -0.371174, 0.62954, 0.989518, -0.816808, -0.5699, 0.630229, 0.984351, -0.779224, 1.23839, 0.111337, -0.791356, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.06626, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.26121, 3.66265, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"79, 0.198915, -0.371174, 0.62954, 0.989518, -0.816808, -0.5699, 0.630229, 0.984351, -0.779224, 1.23839, 0.111337, -0.791356, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -1.06626, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.26121, 3.66265, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 53, -1.18537, -1.08448, 0.907038, 0.379552, 1.41542, 0.992901, 0.615856, 0.29705, 1.33148, 0.509984, 0.374705, -0.617804, -0.054535, -0.297912, -0.257173, -0.269266, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.344164, -0.26576, -0.080866, -0.180152, 0.237985, -0.357189, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.40808, -0.263784, -0.139878, 0.0, -0.06989, -0.251355, -0.022779, -0.578213, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.166838, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.167981, 1.62887, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.222569, 0.691854, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"53, -1.18537, -1.08448, 0.907038, 0.379552, 1.41542, 0.992901, 0.615856, 0.29705, 1.33148, 0.509984, 0.374705, -0.617804, -0.054535, -0.297912, -0.257173, -0.269266, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.344164, -0.26576, -0.080866, -0.180152, 0.237985, -0.357189, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.40808, -0.263784, -0.139878, 0.0, -0.06989, -0.251355, -0.022779, -0.578213, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.166838, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.167981, 1.62887, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.222569, 0.691854, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 53, 0.012205, 0.19051, -0.684125, -0.693675, 0.144194, -1.06362, -0.302941, -0.680999, 0.39346, 0.922491, 0.502509, -0.82766, -0.054535, -0.297912, -0.096607, 0.285022, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.126964, -0.130158, -0.447464, -0.309461, -0.025787, -0.073582, -0.114477, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.170891, 0.641335, -0.589399, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.163821, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.191187, 2.02257, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.255868, -0.0377, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"53, 0.012205, 0.19051, -0.684125, -0.693675, 0.144194, -1.06362, -0.302941, -0.680999, 0.39346, 0.922491, 0.502509, -0.82766, -0.054535, -0.297912, -0.096607, 0.285022, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.126964, -0.130158, -0.447464, -0.309461, -0.025787, -0.073582, -0.114477, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.170891, 0.641335, -0.589399, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.163821, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.191187, 2.02257, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.255868, -0.0377, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 53, -1.25487, -1.15122, 0.729969, -1.32301, 0.441779, -1.08601, 0.686112, -1.32281, 0.27877, 1.13793, 0.493852, -0.986755, -0.054535, -0.297912, 0.619999, 0.932992, 1.8117, -0.014952, -0.08711, -0.167402, -0.200329, -0.087841, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.121315, 1.1133, 0.562098, -0.309461, -0.025787, -0.073582, -0.114473, -0.087061, 0.012481, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.407759, -0.263784, -0.139878, 0.0, -0.06989, -0.2096, 1.66928, -0.318294, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.158929, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.23559, 0.763361, -0.655991, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265728, -0.609288, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"53, -1.25487, -1.15122, 0.729969, -1.32301, 0.441779, -1.08601, 0.686112, -1.32281, 0.27877, 1.13793, 0.493852, -0.986755, -0.054535, -0.297912, 0.619999, 0.932992, 1.8117, -0.014952, -0.08711, -0.167402, -0.200329, -0.087841, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.121315, 1.1133, 0.562098, -0.309461, -0.025787, -0.073582, -0.114473, -0.087061, 0.012481, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.407759, -0.263784, -0.139878, 0.0, -0.06989, -0.2096, 1.66928, -0.318294, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.158929, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.23559, 0.763361, -0.655991, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265728, -0.609288, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 53, 0.761758, 0.734673, -1.34234, 0.752678, -0.231037, 1.16795, -1.34515, 0.772384, -0.351201, 0.129013, 0.384627, -0.705333, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.044051, -0.166076, -0.431557, -0.309461, -0.025787, -0.073582, -0.113978, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.135089, 0.104794, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.065341, 2.32214, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.25483, 0.068549, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"53, 0.761758, 0.734673, -1.34234, 0.752678, -0.231037, 1.16795, -1.34515, 0.772384, -0.351201, 0.129013, 0.384627, -0.705333, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.044051, -0.166076, -0.431557, -0.309461, -0.025787, -0.073582, -0.113978, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.135089, 0.104794, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.065341, 2.32214, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.25483, 0.068549, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 53, 0.35911, 0.466274, -0.541742, 0.972651, -0.507837, 0.0347, -0.649558, 1.01217, -0.594794, -0.007455, 0.298212, -0.36208, -0.054535, -0.297912, -0.261335, -0.256532, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.203764, 0.895359, -0.427319, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.21713, 0.00805, -0.581157, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.180471, 2.00717, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.223665, -0.0036, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"53, 0.35911, 0.466274, -0.541742, 0.972651, -0.507837, 0.0347, -0.649558, 1.01217, -0.594794, -0.007455, 0.298212, -0.36208, -0.054535, -0.297912, -0.261335, -0.256532, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.203764, 0.895359, -0.427319, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.21713, 0.00805, -0.581157, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.180471, 2.00717, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.223665, -0.0036, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 55, 0.234068, 0.114354, -0.643237, -0.533073, 0.782333, -0.741461, -0.096257, -0.713345, 1.97731, -0.343768, 0.86901, -0.701815, -0.054535, -0.297912, -0.224704, -0.302051, -0.225084, -0.014952, 0.826199, -0.014803, -0.195987, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, 0.811113, 3.43955, 0.295205, -0.445647, -0.309461, -0.025787, 0.234196, 2.26686, 0.016659, -0.159672, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.850912, 1.52717, -0.560868, -0.260113, -0.102253, 0.0, 0.0, -0.040796, 0.047758, 0.148388, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.146183, 0.457002, -0.654114, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.023836, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.260395, -0.692843, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"55, 0.234068, 0.114354, -0.643237, -0.533073, 0.782333, -0.741461, -0.096257, -0.713345, 1.97731, -0.343768, 0.86901, -0.701815, -0.054535, -0.297912, -0.224704, -0.302051, -0.225084, -0.014952, 0.826199, -0.014803, -0.195987, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, 0.811113, 3.43955, 0.295205, -0.445647, -0.309461, -0.025787, 0.234196, 2.26686, 0.016659, -0.159672, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.850912, 1.52717, -0.560868, -0.260113, -0.102253, 0.0, 0.0, -0.040796, 0.047758, 0.148388, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.146183, 0.457002, -0.654114, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.023836, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.260395, -0.692843, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 55, -0.374881, -0.460419, 1.15346, 0.267369, 0.302158, 0.220073, 1.2874, 0.140788, 0.224565, 0.534284, 1.34719, 0.021888, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, 0.324967, -0.158108, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, 0.016127, 0.593055, -0.296406, -0.451394, -0.309461, -0.025787, 0.300386, 0.19414, -0.182419, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 1.00112, 2.02819, -0.584931, -0.260113, -0.102253, 0.0, 0.0, -0.040796, 0.186309, 0.088742, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.060527, 1.33076, -0.639754, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.37583, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.264643, -0.733983, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"55, -0.374881, -0.460419, 1.15346, 0.267369, 0.302158, 0.220073, 1.2874, 0.140788, 0.224565, 0.534284, 1.34719, 0.021888, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, 0.324967, -0.158108, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, 0.016127, 0.593055, -0.296406, -0.451394, -0.309461, -0.025787, 0.300386, 0.19414, -0.182419, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 1.00112, 2.02819, -0.584931, -0.260113, -0.102253, 0.0, 0.0, -0.040796, 0.186309, 0.088742, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.060527, 1.33076, -0.639754, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.37583, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.264643, -0.733983, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 55, -1.20887, -1.19012, 1.71439, -0.3227, -0.162262, 0.399884, 1.7962, -0.12238, -0.184949, 0.703714, 1.49302, 0.313239, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, 0.155649, -0.087842, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, 0.092088, 0.957812, -0.123866, -0.435621, -0.309461, -0.025787, 0.06915, 0.412733, -0.02802, -0.156094, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 3.08153, 1.99683, -0.572525, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.055036, 0.325483, -0.183669, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.275158, 0.479615, -0.623346, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.570049, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.740893, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"55, -1.20887, -1.19012, 1.71439, -0.3227, -0.162262, 0.399884, 1.7962, -0.12238, -0.184949, 0.703714, 1.49302, 0.313239, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, 0.155649, -0.087842, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, 0.092088, 0.957812, -0.123866, -0.435621, -0.309461, -0.025787, 0.06915, 0.412733, -0.02802, -0.156094, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 3.08153, 1.99683, -0.572525, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.055036, 0.325483, -0.183669, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.275158, 0.479615, -0.623346, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.570049, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.740893, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 55, -0.343193, 0.10244, -0.272437, 1.21789, -0.293912, -1.6156, -0.237854, 1.25246, -0.320132, -0.865695, 0.235407, -1.04851, -0.054535, -0.297912, -0.136403, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.963468, 0.375172, -0.451394, -0.309461, -0.025787, -0.073582, 0.289431, -0.102383, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.261456, 1.13708, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.086361, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 2.26663, -0.65529, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.129558, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.678296, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"55, -0.343193, 0.10244, -0.272437, 1.21789, -0.293912, -1.6156, -0.237854, 1.25246, -0.320132, -0.865695, 0.235407, -1.04851, -0.054535, -0.297912, -0.136403, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.963468, 0.375172, -0.451394, -0.309461, -0.025787, -0.073582, 0.289431, -0.102383, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.261456, 1.13708, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.086361, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 2.26663, -0.65529, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 0.129558, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.678296, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 55, -1.13189, -1.00012, 0.128758, 0.433725, -0.668444, 0.844998, 0.13286, 0.389801, -0.724027, 1.60019, 0.554052, -0.359014, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, 1.70912, -0.139762, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 1.75052, 0.196878, -0.451394, -0.309461, -0.025787, 2.45708, 2.12075, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.816585, 1.65399, -0.582916, -0.260113, -0.102253, 0.0, 0.0, -0.040796, 1.97295, -0.031936, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.89114, 0.345074, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.083193, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.70634, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"55, -1.13189, -1.00012, 0.128758, 0.433725, -0.668444, 0.844998, 0.13286, 0.389801, -0.724027, 1.60019, 0.554052, -0.359014, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, 1.70912, -0.139762, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 1.75052, 0.196878, -0.451394, -0.309461, -0.025787, 2.45708, 2.12075, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 0.816585, 1.65399, -0.582916, -0.260113, -0.102253, 0.0, 0.0, -0.040796, 1.97295, -0.031936, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 0.89114, 0.345074, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.083193, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.70634, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 55, -1.02164, -0.932241, 0.306006, -0.701443, -0.712174, 0.062496, 0.371966, -0.729042, -0.748816, 1.66153, 0.378852, -0.636014, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.024495, 2.28974, -0.13214, -0.394559, -0.309461, -0.025787, -0.073582, 0.261583, -0.134743, -0.14386, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 2.5542, 2.13037, -0.577807, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.057001, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.470649, -0.65491, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.060456, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"55, -1.02164, -0.932241, 0.306006, -0.701443, -0.712174, 0.062496, 0.371966, -0.729042, -0.748816, 1.66153, 0.378852, -0.636014, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.024495, 2.28974, -0.13214, -0.394559, -0.309461, -0.025787, -0.073582, 0.261583, -0.134743, -0.14386, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 2.5542, 2.13037, -0.577807, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.057001, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.470649, -0.65491, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.060456, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 55, -1.00413, -0.959703, 0.182012, -1.32494, -0.758555, 0.420016, 0.164465, -1.34449, -0.792775, 0.465817, 0.814171, -0.881304, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.130217, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, 1.06309, 5.42856, -0.429135, -0.451394, -0.309461, -0.025787, 0.283671, 1.29479, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 2.42077, 0.63206, -0.546851, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.036331, -0.074997, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.869128, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.02864, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.725845, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"55, -1.00413, -0.959703, 0.182012, -1.32494, -0.758555, 0.420016, 0.164465, -1.34449, -0.792775, 0.465817, 0.814171, -0.881304, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.130217, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, 1.06309, 5.42856, -0.429135, -0.451394, -0.309461, -0.025787, 0.283671, 1.29479, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 2.42077, 0.63206, -0.546851, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.036331, -0.074997, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 0.869128, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.02864, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.725845, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 55, -0.282625, -0.70471, 2.13504, -0.399965, -0.813451, -0.857831, 2.20815, -0.44546, -0.830785, 0.61875, 1.42487, 0.410206, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.117698, 0.607142, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 1.07262, 1.29147, -0.451394, -0.309461, -0.025787, -0.073582, 0.758129, 0.598424, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 1.7648, 0.828675, -0.589138, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.024255, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.02385, -0.633644, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 2.9264, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.676024, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"55, -0.282625, -0.70471, 2.13504, -0.399965, -0.813451, -0.857831, 2.20815, -0.44546, -0.830785, 0.61875, 1.42487, 0.410206, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.117698, 0.607142, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 1.07262, 1.29147, -0.451394, -0.309461, -0.025787, -0.073582, 0.758129, 0.598424, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, 1.7648, 0.828675, -0.589138, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, 0.024255, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.02385, -0.633644, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, 2.9264, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.676024, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 30, -1.30976, -1.23065, -0.921113, -0.12189, 0.668705, 1.04071, -0.862162, -0.122672, 1.4346, 0.005583, -0.128752, -1.10065, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.070857, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.068679, -0.175526, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.165652, -0.782054, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 4.93883, 1.36385, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746092, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"30, -1.30976, -1.23065, -0.921113, -0.12189, 0.668705, 1.04071, -0.862162, -0.122672, 1.4346, 0.005583, -0.128752, -1.10065, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.070857, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.068679, -0.175526, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.165652, -0.782054, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 4.93883, 1.36385, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746092, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 30, 0.044971, 0.176325, -0.034226, -0.180165, 0.026279, 0.358323, -0.029256, -0.175726, 0.047406, -0.062837, 0.01371, -0.673663, -0.054535, -0.297912, -0.24938, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080741, -0.152651, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.184152, -0.75608, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 4.72029, 1.47396, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"30, 0.044971, 0.176325, -0.034226, -0.180165, 0.026279, 0.358323, -0.029256, -0.175726, 0.047406, -0.062837, 0.01371, -0.673663, -0.054535, -0.297912, -0.24938, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080741, -0.152651, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.184152, -0.75608, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 4.72029, 1.47396, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 30, -0.260119, 0.042588, 1.34289, -1.04985, -0.244578, 1.29356, 1.50684, -0.857908, -0.016384, 0.917973, 0.292179, -0.74371, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.303812, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.94908, 1.72562, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.711999, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"30, -0.260119, 0.042588, 1.34289, -1.04985, -0.244578, 1.29356, 1.50684, -0.857908, -0.016384, 0.917973, 0.292179, -0.74371, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.303812, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.94908, 1.72562, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.711999, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 30, -0.322958, -0.118787, 1.32423, 1.02307, -0.392453, -0.094685, 1.47769, 0.997756, -0.398465, 1.27352, 0.303529, -0.511354, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.783785, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.7385, 2.77007, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.336585, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"30, -0.322958, -0.118787, 1.32423, 1.02307, -0.392453, -0.094685, 1.47769, 0.997756, -0.398465, 1.27352, 0.303529, -0.511354, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.783785, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.7385, 2.77007, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.336585, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 30, -1.20622, -1.09643, 1.96465, 0.263955, -0.717932, 0.808945, 2.00506, 0.282178, -0.690975, 1.37501, 0.298053, -0.471791, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.592342, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.52615, 1.62563, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.551967, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"30, -1.20622, -1.09643, 1.96465, 0.263955, -0.717932, 0.808945, 2.00506, 0.282178, -0.690975, 1.37501, 0.298053, -0.471791, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.592342, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 1.52615, 1.62563, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.551967, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 30, -1.22771, -1.12473, 1.74258, -0.827992, -0.651303, 0.980697, 1.78677, -0.796515, -0.699899, 1.30929, 0.330462, -0.476385, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.4357, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.92401, 0.93922, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"30, -1.22771, -1.12473, 1.74258, -0.827992, -0.651303, 0.980697, 1.78677, -0.796515, -0.699899, 1.30929, 0.330462, -0.476385, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.4357, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 2.92401, 0.93922, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.746243, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 30, -0.412436, -0.279952, 0.045287, -1.03835, -0.717844, -0.35441, 0.057389, -1.08443, -0.7389, 1.12849, 0.370022, -0.511254, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.00442, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.10409, -0.421467, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.54481, 1.84555, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.744056, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"30, -0.412436, -0.279952, 0.045287, -1.03835, -0.717844, -0.35441, 0.057389, -1.08443, -0.7389, 1.12849, 0.370022, -0.511254, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, 0.00442, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.10409, -0.421467, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.54481, 1.84555, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.744056, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 30, -1.12313, -0.982282, 1.34612, 0.025426, -0.808888, 2.45612, 1.40291, 0.005073, -0.83409, 1.40028, 0.282, -0.463844, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.46979, 1.84939, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.408716, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"30, -1.12313, -0.982282, 1.34612, 0.025426, -0.808888, 2.45612, 1.40291, 0.005073, -0.83409, 1.40028, 0.282, -0.463844, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.785786, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, 3.46979, 1.84939, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.408716, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 9, -0.209907, -0.415595, 0.833442, 0.783914, 0.602433, -1.35774, 0.449659, 0.780044, 0.847264, -0.150377, -1.40627, 0.826925, -0.054535, 1.38289, -0.261335, -0.083487, -0.03975, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.159826, -0.15635, -0.080912, 0.219556, 0.715293, 0.607855, -0.080866, -0.22114, -0.086197, 0.368914, 1.05435, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.147168, -0.177573, -0.091775, -0.050171, -0.114846, 2.21933, 1.33734, 0.171722, -0.139878, 0.0, -0.06989, -0.263975, 0.011904, 0.61401, 0.227871, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 1.27975, -0.081942, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364263, -0.292159, 0.042341, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.127448, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.224366, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"9, -0.209907, -0.415595, 0.833442, 0.783914, 0.602433, -1.35774, 0.449659, 0.780044, 0.847264, -0.150377, -1.40627, 0.826925, -0.054535, 1.38289, -0.261335, -0.083487, -0.03975, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.159826, -0.15635, -0.080912, 0.219556, 0.715293, 0.607855, -0.080866, -0.22114, -0.086197, 0.368914, 1.05435, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.147168, -0.177573, -0.091775, -0.050171, -0.114846, 2.21933, 1.33734, 0.171722, -0.139878, 0.0, -0.06989, -0.263975, 0.011904, 0.61401, 0.227871, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 1.27975, -0.081942, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364263, -0.292159, 0.042341, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.127448, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.224366, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 9, -0.074039, 0.091244, -0.34393, -1.28765, 0.064434, -0.964721, -0.15656, -1.28779, 0.318763, 1.43035, -0.150255, -0.472364, -0.054535, 2.51957, -0.203682, 0.374784, -0.173541, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.170763, -0.20685, -0.080912, 2.26232, 0.742833, 0.065237, -0.080866, -0.180499, 1.18126, -0.047023, -0.207914, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, 0.050932, -0.337483, -0.191362, -0.139878, 0.0, -0.06989, -0.263975, 0.383159, -0.321063, -0.233735, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391045, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.837112, -0.625099, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.13504, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.35816, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"9, -0.074039, 0.091244, -0.34393, -1.28765, 0.064434, -0.964721, -0.15656, -1.28779, 0.318763, 1.43035, -0.150255, -0.472364, -0.054535, 2.51957, -0.203682, 0.374784, -0.173541, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.170763, -0.20685, -0.080912, 2.26232, 0.742833, 0.065237, -0.080866, -0.180499, 1.18126, -0.047023, -0.207914, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, 0.050932, -0.337483, -0.191362, -0.139878, 0.0, -0.06989, -0.263975, 0.383159, -0.321063, -0.233735, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391045, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.837112, -0.625099, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.13504, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.35816, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 9, 0.961096, 0.558821, -1.24223, 0.266691, 0.012406, 2.25766, -1.28915, 0.27022, -0.129091, -1.20791, -0.930725, 0.440628, -0.054535, 2.55794, -0.145454, -0.126612, 0.00869, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.79951, -0.20685, 0.05852, 2.31449, 2.16259, -0.123637, -0.080866, -0.007242, -0.061477, -0.276798, -0.188948, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.165401, -0.177573, -0.091775, -0.050171, -0.025532, 6.47417, 0.976692, -0.263784, -0.139878, 0.0, -0.06989, -0.256375, 0.590875, -0.385461, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 1.39521, -0.353868, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.356829, -0.649751, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.127114, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.59186, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"9, 0.961096, 0.558821, -1.24223, 0.266691, 0.012406, 2.25766, -1.28915, 0.27022, -0.129091, -1.20791, -0.930725, 0.440628, -0.054535, 2.55794, -0.145454, -0.126612, 0.00869, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.79951, -0.20685, 0.05852, 2.31449, 2.16259, -0.123637, -0.080866, -0.007242, -0.061477, -0.276798, -0.188948, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.165401, -0.177573, -0.091775, -0.050171, -0.025532, 6.47417, 0.976692, -0.263784, -0.139878, 0.0, -0.06989, -0.256375, 0.590875, -0.385461, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 1.39521, -0.353868, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.356829, -0.649751, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.127114, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.59186, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 9, 0.821406, 0.745193, 1.57439, -0.998021, -0.107943, 0.551141, 1.52064, -0.838358, -0.135041, -0.172742, -1.0571, 0.615721, -0.054535, 1.25319, -0.246318, -0.163878, 0.016569, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.003121, -0.20685, -0.080912, 0.731054, 1.03572, 0.754717, -0.080866, -0.224814, 0.024301, 0.827715, 1.09721, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, 1.50264, 0.052547, -0.253512, -0.139878, 0.0, -0.06989, -0.263975, 0.302905, 0.286668, -0.212317, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.175786, -0.391086, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.889899, -0.56927, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.134834, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.35198, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"9, 0.821406, 0.745193, 1.57439, -0.998021, -0.107943, 0.551141, 1.52064, -0.838358, -0.135041, -0.172742, -1.0571, 0.615721, -0.054535, 1.25319, -0.246318, -0.163878, 0.016569, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.003121, -0.20685, -0.080912, 0.731054, 1.03572, 0.754717, -0.080866, -0.224814, 0.024301, 0.827715, 1.09721, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, 1.50264, 0.052547, -0.253512, -0.139878, 0.0, -0.06989, -0.263975, 0.302905, 0.286668, -0.212317, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.175786, -0.391086, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.889899, -0.56927, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.134834, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.35198, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 9, 1.00153, 0.798209, -0.876998, -0.191233, -0.439911, -1.24546, -0.815949, -0.054475, -0.398465, -0.195955, -0.975328, 0.627424, -0.054535, 1.05118, -0.180358, 0.039453, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.243592, -0.20685, -0.080912, 2.10911, 0.568951, 0.220386, -0.080866, -0.095881, 1.18217, -0.302445, -0.024203, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.083288, 1.74241, 0.15727, -0.214235, -0.139878, 0.0, -0.06989, -0.263975, 0.244348, -0.262863, -0.223063, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 0.714717, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.698314, -0.618481, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.089309, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.812542, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"9, 1.00153, 0.798209, -0.876998, -0.191233, -0.439911, -1.24546, -0.815949, -0.054475, -0.398465, -0.195955, -0.975328, 0.627424, -0.054535, 1.05118, -0.180358, 0.039453, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.243592, -0.20685, -0.080912, 2.10911, 0.568951, 0.220386, -0.080866, -0.095881, 1.18217, -0.302445, -0.024203, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.083288, 1.74241, 0.15727, -0.214235, -0.139878, 0.0, -0.06989, -0.263975, 0.244348, -0.262863, -0.223063, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 0.714717, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.698314, -0.618481, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.089309, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.812542, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 9, 0.235755, 0.156058, -0.046882, -0.341668, -0.297443, 0.336325, -0.044144, -0.428368, -0.443085, 0.469575, -0.822121, 0.473436, -0.054535, 1.03898, -0.261335, -0.086696, -0.081603, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.135931, -0.20685, -0.080912, 2.23884, 0.671673, -0.26576, -0.080866, -0.217974, 0.526347, -0.380726, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, 0.110709, 4.0131, 0.135223, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.1224, -0.580904, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 1.62776, -0.369074, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.684004, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.083389, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.689404, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"9, 0.235755, 0.156058, -0.046882, -0.341668, -0.297443, 0.336325, -0.044144, -0.428368, -0.443085, 0.469575, -0.822121, 0.473436, -0.054535, 1.03898, -0.261335, -0.086696, -0.081603, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 0.135931, -0.20685, -0.080912, 2.23884, 0.671673, -0.26576, -0.080866, -0.217974, 0.526347, -0.380726, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, 0.110709, 4.0131, 0.135223, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 0.1224, -0.580904, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 1.62776, -0.369074, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.684004, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.083389, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.689404, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 9, 0.430497, 0.254252, 0.525583, 0.233782, -0.434132, -0.252894, 0.530455, 0.260397, -0.523401, -1.85211, -0.754433, -0.520505, -0.054535, 0.097264, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 1.06172, 0.009834, -0.080912, -0.023498, 0.531841, -0.122241, -0.080866, -0.224814, -0.50504, -0.381395, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.077386, -0.177573, -0.091775, -0.050171, -0.087747, 4.10486, 1.76796, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.484555, -0.384452, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 3.09728, -0.331149, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.001351, -0.654837, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.069793, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.312311, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"9, 0.430497, 0.254252, 0.525583, 0.233782, -0.434132, -0.252894, 0.530455, 0.260397, -0.523401, -1.85211, -0.754433, -0.520505, -0.054535, 0.097264, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, 1.06172, 0.009834, -0.080912, -0.023498, 0.531841, -0.122241, -0.080866, -0.224814, -0.50504, -0.381395, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.077386, -0.177573, -0.091775, -0.050171, -0.087747, 4.10486, 1.76796, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.484555, -0.384452, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, 3.09728, -0.331149, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, -0.001351, -0.654837, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.069793, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.312311, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 85, -1.28986, -1.1983, 1.02707, 0.131235, 2.30142, 0.522095, 0.81753, -0.122429, 2.2804, 1.55604, 0.082383, -0.579776, -0.054535, -0.297912, -0.261335, -0.301663, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.190058, -0.339631, -0.427897, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.331544, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.258454, 0.75261, -0.574277, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.346821, 1.39342, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.728868, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"85, -1.28986, -1.1983, 1.02707, 0.131235, 2.30142, 0.522095, 0.81753, -0.122429, 2.2804, 1.55604, 0.082383, -0.579776, -0.054535, -0.297912, -0.261335, -0.301663, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.190058, -0.339631, -0.427897, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.331544, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.258454, 0.75261, -0.574277, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.346821, 1.39342, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 0.728868, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 85, -1.30219, -1.21756, -1.08381, 0.141852, 0.616501, 1.59016, -0.971926, -0.016396, 0.408664, 0.33816, 0.328081, -0.415712, -0.054535, -0.297912, -0.224437, -0.270221, -0.225084, -0.014952, -0.08711, -0.167402, 0.273353, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.101398, 1.28888, -0.401241, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.256222, 0.864172, -0.585748, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.322068, 1.84995, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.298417, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"85, -1.30219, -1.21756, -1.08381, 0.141852, 0.616501, 1.59016, -0.971926, -0.016396, 0.408664, 0.33816, 0.328081, -0.415712, -0.054535, -0.297912, -0.224437, -0.270221, -0.225084, -0.014952, -0.08711, -0.167402, 0.273353, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.101398, 1.28888, -0.401241, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.256222, 0.864172, -0.585748, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.322068, 1.84995, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.298417, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 85, -0.05737, 0.311066, -0.574968, -0.864861, -0.438286, -0.369356, -0.699862, -0.880166, -0.555462, -0.414524, -0.004157, -0.202475, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.157178, -0.436727, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.214139, -0.371898, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.088879, 3.63227, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.4733, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"85, -0.05737, 0.311066, -0.574968, -0.864861, -0.438286, -0.369356, -0.699862, -0.880166, -0.555462, -0.414524, -0.004157, -0.202475, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.157178, -0.436727, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.214139, -0.371898, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.088879, 3.63227, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.4733, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 85, -1.30474, -1.2053, -1.13776, -1.50931, -0.562636, -0.844451, -1.0173, -1.53167, -0.615947, 0.918534, 0.113869, -0.796511, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.726845, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 2.00493, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.20535, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"85, -1.30474, -1.2053, -1.13776, -1.50931, -0.562636, -0.844451, -1.0173, -1.53167, -0.615947, 0.918534, 0.113869, -0.796511, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.529019, -0.451394, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, -0.726845, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 2.00493, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, 1.20535, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 357, in _parse_examples\n",
      "    exset.append(_parse_example(schema, line))\n",
      "  File \"C:\\Users\\seven\\Documents\\github\\MachineLearning\\misvmio.py\", line 367, in _parse_example\n",
      "    raise Exception('Feature-data size mismatch: %s' % line)\n",
      "Exception: Feature-data size mismatch: 85, 0.927627, 1.01977, -0.581196, 0.384244, -0.738154, 1.55334, -0.592872, 0.375628, -0.778563, -1.42388, 0.242673, -0.152822, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.116188, -0.417989, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.64051, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.62745, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.033434, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\n",
      "Warning: skipping line: \"85, 0.927627, 1.01977, -0.581196, 0.384244, -0.738154, 1.55334, -0.592872, 0.375628, -0.778563, -1.42388, 0.242673, -0.152822, -0.054535, -0.297912, -0.261335, -0.302051, -0.225084, -0.014952, -0.08711, -0.167402, -0.200329, -0.248836, -0.021302, -0.028883, -0.038542, -0.023332, -0.023412, 0.0, 0.0, -0.02702, -0.198246, -0.20685, -0.080912, -0.327191, -0.347995, -0.26576, -0.080866, -0.224814, -0.116188, -0.417989, -0.309461, -0.025787, -0.073582, -0.114571, -0.18949, -0.160794, -0.201557, 0.0, -0.021708, -0.021658, -0.041491, -0.032279, -0.071887, -0.095154, -0.014952, 0.0, 0.0, 0.0, 0.0, -0.014952, -0.017265, -0.024647, -0.021215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.065785, -0.049974, -0.014952, -0.048732, -0.173228, -0.177573, -0.091775, -0.050171, -0.114846, -0.3317, -0.408478, -0.263784, -0.139878, 0.0, -0.06989, -0.263975, 1.64051, -0.590402, -0.260113, -0.102253, 0.0, 0.0, -0.040796, -0.056023, -0.168633, -0.188284, -0.115671, -0.074486, 0.0, 0.0, 0.0, -0.014952, -0.014982, -0.040764, -0.042342, -0.027653, -0.014952, 0.0, 0.0, 0.0, 0.0, 0.0, -0.021732, -0.021365, -0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017556, -0.021126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018485, -0.01594, 0.0, 0.0, -0.043134, -0.029996, -0.09331, -0.098605, -0.071579, -0.017934, -0.063725, -0.182197, -0.255619, -0.391468, -0.187455, -0.077681, -0.03269, 0.0, -0.053085, -0.364504, 1.62745, -0.655993, -0.171326, -0.072849, -0.032187, 0.0, 0.0, -0.061848, -0.174962, -0.192699, -0.054582, -0.032023, 0.0, 0.0, 0.0, -0.015029, -0.021431, -0.018157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019294, -0.017308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.032096, -0.043388, -0.027179, 0.0, 0.0, 0.0, -0.099876, -0.135405, -0.183154, -0.106346, -0.037709, 0.0, 0.0, 0.0, -0.265739, -0.033434, -0.412626, -0.102856, -0.017565, 0.0, 0.0, 0.0, -0.049855, -0.114025, -0.078862, -0.021452, 0.0, 0.0, -0.014952, -0.021097, 1\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m data_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(raw_data, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m data_std  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(raw_data, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mdata_std\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_std\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalizer\u001b[39m(ex):\n\u001b[0;32m     10\u001b[0m     ex \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(ex)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# Load list of C4.5 Examples\n",
    "example_set = parse_c45('fox')\n",
    "\n",
    "# Get stats to normalize data\n",
    "raw_data = np.array(example_set.to_float())\n",
    "data_mean = np.average(raw_data, axis=0)\n",
    "data_std  = np.std(raw_data, axis=0)\n",
    "data_std[np.nonzero(data_std == 0.0)] = 1.0\n",
    "def normalizer(ex):\n",
    "    ex = np.array(ex)\n",
    "    normed = ((ex - data_mean) / data_std)\n",
    "    # The ...[:, 2:-1] removes first two columns and last column,\n",
    "    # which are the bag/instance ids and class label, as part of the\n",
    "    # normalization process\n",
    "    return normed[2:-1]\n",
    "\n",
    "\n",
    "# Group examples into bags\n",
    "bagset = bag_set(example_set)\n",
    "\n",
    "# Convert bags to NumPy arrays\n",
    "bags = [np.array(b.to_float(normalizer)) for b in bagset]\n",
    "labels = np.array([b.label for b in bagset], dtype=float)\n",
    "# Convert 0/1 labels to -1/1 labels\n",
    "labels = 2 * labels - 1\n",
    "\n",
    "# Spilt dataset arbitrarily to train/test sets\n",
    "train_bags = bags[10:]\n",
    "train_labels = labels[10:]\n",
    "test_bags = bags[:10]\n",
    "test_labels = labels[:10]\n",
    "\n",
    "# Construct classifiers\n",
    "classifiers = {}\n",
    "\n",
    "# MISVM   : the MI-SVM algorithm of Andrews, Tsochantaridis, & Hofmann (2002)\n",
    "# miSVM   : the mi-SVM algorithm of Andrews, Tsochantaridis, & Hofmann (2002)\n",
    "\n",
    "#  : the semi-supervised learning approach of Zhou & Xu (2007)\n",
    "#     : the MI classification algorithm of Mangasarian & Wild (2008)\n",
    "# sMIL    : sparse MIL (Bunescu & Mooney, 2007)\n",
    "# stMIL   : sparse, transductive  MIL (Bunescu & Mooney, 2007)\n",
    "\n",
    "classifiers['MissSVM'] = misvm.MissSVM(kernel='linear', C=1.0, max_iters=20)\n",
    "classifiers['sbMIL'] = misvm.sbMIL(kernel='linear', eta=0.1, C=1e2)\n",
    "classifiers['SIL'] = misvm.SIL(kernel='linear', C=1.0)\n",
    "classifiers['STK'] = misvm.STK(kernel='linear', C=1.0)\n",
    "classifiers['NSK'] = misvm.NSK(kernel='linear', C=1.0)\n",
    "classifiers['MICA'] = misvm.MICA(kernel='linear', C=1.0)\n",
    "\n",
    "# Train/Evaluate classifiers\n",
    "accuracies = {}\n",
    "for algorithm, classifier in classifiers.items():\n",
    "    classifier.fit(train_bags, train_labels)\n",
    "    predictions = classifier.predict(test_bags)\n",
    "    accuracies[algorithm] = np.average(test_labels == np.sign(predictions))\n",
    "\n",
    "for algorithm, accuracy in accuracies.items():\n",
    "    print('\\n%s Accuracy: %.f%%' % (algorithm, 100 * accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mi-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "_sDih52xsYL9",
    "outputId": "6e803c46-abda-4946-cbbe-6098086e21da"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Layer, Dropout\n",
    "\n",
    "from mil_nets.dataset import load_dataset\n",
    "from mil_nets.layer import Score_pooling\n",
    "from mil_nets.metrics import bag_accuracy\n",
    "from mil_nets.objectives import bag_loss\n",
    "from mil_nets.utils import convertToBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "F6FqGP2ss4nY"
   },
   "outputs": [],
   "source": [
    "def test_eval(model, test_set):\n",
    "    \"\"\"Evaluate on testing set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training mi-Net model.\n",
    "    test_set : list\n",
    "        A list of testing set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on testing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_test_batch = len(test_set)\n",
    "    test_loss = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    test_acc = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    for ibatch, batch in enumerate(test_set):\n",
    "        result = model.test_on_batch({'input':batch[0].astype(np.float32)}, {'sp':batch[1].astype(np.float32)})\n",
    "        test_loss[ibatch] = result[0]\n",
    "        test_acc[ibatch] = result[1]\n",
    "    return np.mean(test_loss), np.mean(test_acc)\n",
    "\n",
    "def train_eval(model, train_set):\n",
    "    \"\"\"Evaluate on training set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training mi-Net model.\n",
    "    train_set : list\n",
    "        A list of training set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on traing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_train_batch = len(train_set)\n",
    "    train_loss = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    train_acc = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    shuffle(train_set)\n",
    "    for ibatch, batch in enumerate(train_set):\n",
    "        result = model.train_on_batch({'input':batch[0].astype(np.float32)}, {'sp':batch[1].astype(np.float32)})\n",
    "        train_loss[ibatch] = result[0]\n",
    "        train_acc[ibatch] = result[1]\n",
    "    return np.mean(train_loss), np.mean(train_acc)\n",
    "\n",
    "def mi_Net(dataset):\n",
    "    weight_decay=0.005\n",
    "    init_lr=5e-4\n",
    "    pooling_mode='max'\n",
    "    momentum=0.9\n",
    "    max_epoch=50\n",
    "    \"\"\"Train and evaluate on mi-Net.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    dataset : dict\n",
    "        A dictionary contains all dataset information. We split train/test by keys.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_acc : float\n",
    "        Testing accuracy of mi-Net.\n",
    "    \"\"\"\n",
    "    # load data and convert type\n",
    "    train_bags = dataset['train']\n",
    "    test_bags = dataset['test']\n",
    "\n",
    "    # convert bag to batch\n",
    "    train_set = convertToBatch(train_bags)\n",
    "    test_set = convertToBatch(test_bags)\n",
    "    dimension = train_set[0][0].shape[1]\n",
    "\n",
    "    # data: instance feature, n*d, n = number of training instance\n",
    "    data_input = Input(shape=(dimension,), dtype='float32', name='input')\n",
    "\n",
    "    # fully-connected\n",
    "    fc1 = Dense(256, activation='relu', kernel_regularizer=l2(weight_decay))(data_input)\n",
    "    fc2 = Dense(128, activation='relu', kernel_regularizer=l2(weight_decay))(fc1)\n",
    "    fc3 = Dense(64, activation='relu', kernel_regularizer=l2(weight_decay))(fc2)\n",
    "\n",
    "    # dropout\n",
    "    dropout = Dropout(rate=0.5)(fc3)\n",
    "\n",
    "    # score pooling\n",
    "    sp = Score_pooling(output_dim=1, kernel_regularizer=l2(weight_decay), pooling_mode=pooling_mode, name='sp')(dropout)\n",
    "\n",
    "    model = Model(inputs=[data_input], outputs=[sp])\n",
    "    sgd = SGD(lr=init_lr, decay=1e-4, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss=bag_loss, optimizer=sgd, metrics=[bag_accuracy])\n",
    "\n",
    "    # train model\n",
    "    t1 = time.time()\n",
    "    num_batch = len(train_set)\n",
    "    for epoch in range(max_epoch):\n",
    "        train_loss, train_acc = train_eval(model, train_set)\n",
    "        test_loss, test_acc = test_eval(model, test_set)\n",
    "        print('epoch=', epoch, '  train_loss= {:.3f}'.format(train_loss), '  train_acc= {:.3f}'.format(train_acc), '  test_loss={:.3f}'.format(test_loss), '  test_acc= {:.3f}'.format(test_acc))\n",
    "    t2 = time.time()\n",
    "    print('run time:', (t2-t1) / 60.0, 'min')\n",
    "    print('test_acc={:.3f}'.format(test_acc))\n",
    "\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "7ahoLSg5vMxp",
    "outputId": "13625a5b-3823-421c-ee82-42db7607fa1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-211     \n",
      "MUSK-211     \n",
      "MUSK-212     \n",
      "MUSK-212     \n",
      "MUSK-212     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-219     \n",
      "MUSK-219     \n",
      "MUSK-224     \n",
      "MUSK-224     \n",
      "MUSK-227     \n",
      "MUSK-227     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-254     \n",
      "MUSK-254     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-272     \n",
      "MUSK-272     \n",
      "MUSK-272     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-311     \n",
      "MUSK-311     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-321     \n",
      "MUSK-321     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-344     \n",
      "MUSK-344     \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f159    \n",
      "MUSK-f159    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-j33     \n",
      "MUSK-j33     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-jf17    \n",
      "MUSK-jf17    \n",
      "MUSK-jf17    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-208 \n",
      "NON-MUSK-208 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-226 \n",
      "NON-MUSK-226 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-247 \n",
      "NON-MUSK-247 \n",
      "NON-MUSK-249 \n",
      "NON-MUSK-249 \n",
      "NON-MUSK-253 \n",
      "NON-MUSK-253 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-271 \n",
      "NON-MUSK-271 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-290 \n",
      "NON-MUSK-290 \n",
      "NON-MUSK-295 \n",
      "NON-MUSK-295 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-305 \n",
      "NON-MUSK-305 \n",
      "NON-MUSK-308 \n",
      "NON-MUSK-308 \n",
      "NON-MUSK-309 \n",
      "NON-MUSK-309 \n",
      "NON-MUSK-318 \n",
      "NON-MUSK-318 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-320 \n",
      "NON-MUSK-320 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-334 \n",
      "NON-MUSK-334 \n",
      "NON-MUSK-f150\n",
      "NON-MUSK-f150\n",
      "NON-MUSK-f161\n",
      "NON-MUSK-f161\n",
      "NON-MUSK-f164\n",
      "NON-MUSK-f164\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j130\n",
      "NON-MUSK-j130\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j148\n",
      "NON-MUSK-j148\n",
      "NON-MUSK-j81 \n",
      "NON-MUSK-j81 \n",
      "NON-MUSK-j83 \n",
      "NON-MUSK-j83 \n",
      "NON-MUSK-j84 \n",
      "NON-MUSK-j84 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "run= 0   fold= 0\n",
      "epoch= 0   train_loss= 2.921   train_acc= 0.659   test_loss=2.801   test_acc= 0.700\n",
      "epoch= 1   train_loss= 2.721   train_acc= 0.793   test_loss=2.770   test_acc= 0.800\n",
      "epoch= 2   train_loss= 2.618   train_acc= 0.854   test_loss=2.735   test_acc= 0.700\n",
      "epoch= 3   train_loss= 2.566   train_acc= 0.866   test_loss=2.652   test_acc= 0.700\n",
      "epoch= 4   train_loss= 2.512   train_acc= 0.915   test_loss=2.695   test_acc= 0.700\n",
      "epoch= 5   train_loss= 2.485   train_acc= 0.878   test_loss=2.584   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.416   train_acc= 0.915   test_loss=2.594   test_acc= 0.700\n",
      "epoch= 7   train_loss= 2.394   train_acc= 0.939   test_loss=2.572   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.348   train_acc= 0.939   test_loss=2.602   test_acc= 0.700\n",
      "epoch= 9   train_loss= 2.315   train_acc= 0.939   test_loss=2.580   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.317   train_acc= 0.939   test_loss=2.585   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.260   train_acc= 0.963   test_loss=2.540   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.221   train_acc= 0.988   test_loss=2.541   test_acc= 0.700\n",
      "epoch= 13   train_loss= 2.217   train_acc= 0.976   test_loss=2.498   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.159   train_acc= 0.988   test_loss=2.471   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.148   train_acc= 0.988   test_loss=2.448   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.116   train_acc= 0.988   test_loss=2.438   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.098   train_acc= 1.000   test_loss=2.463   test_acc= 0.900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 18   train_loss= 2.081   train_acc= 1.000   test_loss=2.426   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.059   train_acc= 1.000   test_loss=2.396   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.065   train_acc= 0.976   test_loss=2.386   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.039   train_acc= 0.976   test_loss=2.394   test_acc= 0.900\n",
      "epoch= 22   train_loss= 2.029   train_acc= 1.000   test_loss=2.429   test_acc= 0.900\n",
      "epoch= 23   train_loss= 2.010   train_acc= 0.976   test_loss=2.376   test_acc= 0.900\n",
      "epoch= 24   train_loss= 1.978   train_acc= 1.000   test_loss=2.356   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.962   train_acc= 1.000   test_loss=2.370   test_acc= 0.900\n",
      "epoch= 26   train_loss= 1.946   train_acc= 1.000   test_loss=2.366   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.941   train_acc= 1.000   test_loss=2.373   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.922   train_acc= 1.000   test_loss=2.357   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.907   train_acc= 1.000   test_loss=2.334   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.896   train_acc= 1.000   test_loss=2.325   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.883   train_acc= 1.000   test_loss=2.326   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.865   train_acc= 1.000   test_loss=2.311   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.856   train_acc= 1.000   test_loss=2.321   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.846   train_acc= 1.000   test_loss=2.299   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.847   train_acc= 1.000   test_loss=2.265   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.827   train_acc= 1.000   test_loss=2.242   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.814   train_acc= 1.000   test_loss=2.274   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.806   train_acc= 1.000   test_loss=2.266   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.796   train_acc= 1.000   test_loss=2.265   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.781   train_acc= 1.000   test_loss=2.259   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.770   train_acc= 1.000   test_loss=2.241   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.760   train_acc= 1.000   test_loss=2.231   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.746   train_acc= 1.000   test_loss=2.207   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.735   train_acc= 1.000   test_loss=2.209   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.738   train_acc= 1.000   test_loss=2.185   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.720   train_acc= 1.000   test_loss=2.194   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.707   train_acc= 1.000   test_loss=2.178   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.699   train_acc= 1.000   test_loss=2.154   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.689   train_acc= 1.000   test_loss=2.150   test_acc= 0.900\n",
      "run time: 0.7842020670572917 min\n",
      "test_acc=0.900\n",
      "run= 0   fold= 1\n",
      "epoch= 0   train_loss= 2.935   train_acc= 0.573   test_loss=2.803   test_acc= 0.800\n",
      "epoch= 1   train_loss= 2.785   train_acc= 0.768   test_loss=2.718   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.725   train_acc= 0.829   test_loss=2.671   test_acc= 0.700\n",
      "epoch= 3   train_loss= 2.651   train_acc= 0.841   test_loss=2.624   test_acc= 0.700\n",
      "epoch= 4   train_loss= 2.626   train_acc= 0.829   test_loss=2.599   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.537   train_acc= 0.866   test_loss=2.581   test_acc= 0.800\n",
      "epoch= 6   train_loss= 2.484   train_acc= 0.866   test_loss=2.574   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.431   train_acc= 0.915   test_loss=2.588   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.393   train_acc= 0.915   test_loss=2.587   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.345   train_acc= 0.915   test_loss=2.576   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.316   train_acc= 0.939   test_loss=2.559   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.292   train_acc= 0.951   test_loss=2.565   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.241   train_acc= 0.976   test_loss=2.567   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.208   train_acc= 0.976   test_loss=2.592   test_acc= 0.700\n",
      "epoch= 14   train_loss= 2.186   train_acc= 0.963   test_loss=2.607   test_acc= 0.700\n",
      "epoch= 15   train_loss= 2.191   train_acc= 0.976   test_loss=2.630   test_acc= 0.700\n",
      "epoch= 16   train_loss= 2.139   train_acc= 0.988   test_loss=2.610   test_acc= 0.700\n",
      "epoch= 17   train_loss= 2.120   train_acc= 0.988   test_loss=2.583   test_acc= 0.800\n",
      "epoch= 18   train_loss= 2.073   train_acc= 1.000   test_loss=2.581   test_acc= 0.700\n",
      "epoch= 19   train_loss= 2.072   train_acc= 0.988   test_loss=2.596   test_acc= 0.700\n",
      "epoch= 20   train_loss= 2.072   train_acc= 1.000   test_loss=2.589   test_acc= 0.700\n",
      "epoch= 21   train_loss= 2.049   train_acc= 0.988   test_loss=2.599   test_acc= 0.700\n",
      "epoch= 22   train_loss= 2.041   train_acc= 0.988   test_loss=2.547   test_acc= 0.800\n",
      "epoch= 23   train_loss= 2.015   train_acc= 1.000   test_loss=2.568   test_acc= 0.700\n",
      "epoch= 24   train_loss= 1.996   train_acc= 1.000   test_loss=2.616   test_acc= 0.700\n",
      "epoch= 25   train_loss= 1.982   train_acc= 0.976   test_loss=2.561   test_acc= 0.700\n",
      "epoch= 26   train_loss= 1.968   train_acc= 1.000   test_loss=2.591   test_acc= 0.700\n",
      "epoch= 27   train_loss= 1.947   train_acc= 1.000   test_loss=2.617   test_acc= 0.700\n",
      "epoch= 28   train_loss= 1.927   train_acc= 1.000   test_loss=2.596   test_acc= 0.700\n",
      "epoch= 29   train_loss= 1.921   train_acc= 1.000   test_loss=2.636   test_acc= 0.700\n",
      "epoch= 30   train_loss= 1.904   train_acc= 1.000   test_loss=2.601   test_acc= 0.700\n",
      "epoch= 31   train_loss= 1.886   train_acc= 1.000   test_loss=2.629   test_acc= 0.700\n",
      "epoch= 32   train_loss= 1.876   train_acc= 1.000   test_loss=2.547   test_acc= 0.700\n",
      "epoch= 33   train_loss= 1.856   train_acc= 1.000   test_loss=2.590   test_acc= 0.700\n",
      "epoch= 34   train_loss= 1.862   train_acc= 1.000   test_loss=2.594   test_acc= 0.700\n",
      "epoch= 35   train_loss= 1.840   train_acc= 1.000   test_loss=2.605   test_acc= 0.700\n",
      "epoch= 36   train_loss= 1.827   train_acc= 1.000   test_loss=2.565   test_acc= 0.700\n",
      "epoch= 37   train_loss= 1.815   train_acc= 1.000   test_loss=2.560   test_acc= 0.700\n",
      "epoch= 38   train_loss= 1.807   train_acc= 1.000   test_loss=2.555   test_acc= 0.700\n",
      "epoch= 39   train_loss= 1.797   train_acc= 1.000   test_loss=2.604   test_acc= 0.700\n",
      "epoch= 40   train_loss= 1.778   train_acc= 1.000   test_loss=2.574   test_acc= 0.700\n",
      "epoch= 41   train_loss= 1.770   train_acc= 1.000   test_loss=2.586   test_acc= 0.700\n",
      "epoch= 42   train_loss= 1.755   train_acc= 1.000   test_loss=2.543   test_acc= 0.700\n",
      "epoch= 43   train_loss= 1.752   train_acc= 1.000   test_loss=2.515   test_acc= 0.700\n",
      "epoch= 44   train_loss= 1.746   train_acc= 1.000   test_loss=2.550   test_acc= 0.700\n",
      "epoch= 45   train_loss= 1.727   train_acc= 1.000   test_loss=2.534   test_acc= 0.700\n",
      "epoch= 46   train_loss= 1.738   train_acc= 0.988   test_loss=2.560   test_acc= 0.700\n",
      "epoch= 47   train_loss= 1.721   train_acc= 1.000   test_loss=2.533   test_acc= 0.700\n",
      "epoch= 48   train_loss= 1.703   train_acc= 1.000   test_loss=2.467   test_acc= 0.700\n",
      "epoch= 49   train_loss= 1.692   train_acc= 1.000   test_loss=2.480   test_acc= 0.700\n",
      "run time: 0.9111469030380249 min\n",
      "test_acc=0.700\n",
      "run= 0   fold= 2\n",
      "epoch= 0   train_loss= 2.980   train_acc= 0.458   test_loss=2.928   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.814   train_acc= 0.723   test_loss=2.823   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.682   train_acc= 0.880   test_loss=2.784   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.614   train_acc= 0.880   test_loss=2.747   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.591   train_acc= 0.867   test_loss=2.711   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.535   train_acc= 0.843   test_loss=2.732   test_acc= 0.667\n",
      "epoch= 6   train_loss= 2.467   train_acc= 0.916   test_loss=2.664   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.456   train_acc= 0.892   test_loss=2.639   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.414   train_acc= 0.892   test_loss=2.581   test_acc= 0.667\n",
      "epoch= 9   train_loss= 2.350   train_acc= 0.916   test_loss=2.580   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.323   train_acc= 0.940   test_loss=2.641   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.284   train_acc= 0.940   test_loss=2.587   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.278   train_acc= 0.952   test_loss=2.538   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.229   train_acc= 0.976   test_loss=2.472   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 14   train_loss= 2.198   train_acc= 0.976   test_loss=2.442   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.180   train_acc= 0.976   test_loss=2.355   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.147   train_acc= 0.964   test_loss=2.329   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.141   train_acc= 0.988   test_loss=2.366   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.141   train_acc= 0.964   test_loss=2.397   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.089   train_acc= 0.988   test_loss=2.366   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.079   train_acc= 0.976   test_loss=2.333   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.063   train_acc= 0.976   test_loss=2.295   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.044   train_acc= 1.000   test_loss=2.254   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.030   train_acc= 0.976   test_loss=2.209   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.014   train_acc= 1.000   test_loss=2.221   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.978   train_acc= 1.000   test_loss=2.216   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.962   train_acc= 1.000   test_loss=2.191   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.948   train_acc= 0.988   test_loss=2.202   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.945   train_acc= 0.988   test_loss=2.276   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.927   train_acc= 0.988   test_loss=2.209   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.913   train_acc= 1.000   test_loss=2.286   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.897   train_acc= 1.000   test_loss=2.254   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.881   train_acc= 1.000   test_loss=2.214   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.868   train_acc= 1.000   test_loss=2.220   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.852   train_acc= 1.000   test_loss=2.211   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.840   train_acc= 1.000   test_loss=2.186   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.834   train_acc= 1.000   test_loss=2.176   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.819   train_acc= 1.000   test_loss=2.181   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.815   train_acc= 1.000   test_loss=2.180   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.794   train_acc= 1.000   test_loss=2.126   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.791   train_acc= 1.000   test_loss=2.098   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.774   train_acc= 1.000   test_loss=2.132   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.761   train_acc= 1.000   test_loss=2.130   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.753   train_acc= 1.000   test_loss=2.135   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.746   train_acc= 1.000   test_loss=2.125   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.734   train_acc= 1.000   test_loss=2.079   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.722   train_acc= 1.000   test_loss=2.074   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.714   train_acc= 1.000   test_loss=2.017   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.696   train_acc= 1.000   test_loss=2.043   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.697   train_acc= 1.000   test_loss=2.023   test_acc= 0.778\n",
      "run time: 0.7520058035850525 min\n",
      "test_acc=0.778\n",
      "run= 0   fold= 3\n",
      "epoch= 0   train_loss= 2.910   train_acc= 0.639   test_loss=2.895   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.709   train_acc= 0.819   test_loss=2.789   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.607   train_acc= 0.880   test_loss=2.683   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.527   train_acc= 0.843   test_loss=2.640   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.480   train_acc= 0.904   test_loss=2.583   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.418   train_acc= 0.928   test_loss=2.565   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.378   train_acc= 0.904   test_loss=2.521   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.351   train_acc= 0.940   test_loss=2.535   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.307   train_acc= 0.928   test_loss=2.476   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.275   train_acc= 0.940   test_loss=2.481   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.257   train_acc= 0.964   test_loss=2.437   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.235   train_acc= 0.988   test_loss=2.469   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.185   train_acc= 0.976   test_loss=2.464   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.156   train_acc= 0.988   test_loss=2.425   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.147   train_acc= 0.976   test_loss=2.439   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.096   train_acc= 1.000   test_loss=2.447   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.099   train_acc= 1.000   test_loss=2.468   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.103   train_acc= 0.988   test_loss=2.455   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.051   train_acc= 1.000   test_loss=2.460   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.051   train_acc= 1.000   test_loss=2.477   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.014   train_acc= 1.000   test_loss=2.442   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.003   train_acc= 1.000   test_loss=2.448   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.998   train_acc= 0.988   test_loss=2.438   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.986   train_acc= 1.000   test_loss=2.431   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.959   train_acc= 1.000   test_loss=2.441   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.955   train_acc= 1.000   test_loss=2.439   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.933   train_acc= 1.000   test_loss=2.425   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.933   train_acc= 1.000   test_loss=2.401   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.909   train_acc= 1.000   test_loss=2.423   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.892   train_acc= 1.000   test_loss=2.400   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.880   train_acc= 1.000   test_loss=2.419   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.867   train_acc= 1.000   test_loss=2.408   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.856   train_acc= 1.000   test_loss=2.347   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.839   train_acc= 1.000   test_loss=2.363   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.838   train_acc= 1.000   test_loss=2.398   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.821   train_acc= 1.000   test_loss=2.374   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.811   train_acc= 1.000   test_loss=2.373   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.798   train_acc= 1.000   test_loss=2.335   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.792   train_acc= 1.000   test_loss=2.334   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.775   train_acc= 1.000   test_loss=2.334   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.760   train_acc= 1.000   test_loss=2.319   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.751   train_acc= 1.000   test_loss=2.300   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.753   train_acc= 1.000   test_loss=2.300   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.735   train_acc= 1.000   test_loss=2.303   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.721   train_acc= 1.000   test_loss=2.294   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.718   train_acc= 1.000   test_loss=2.311   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.710   train_acc= 1.000   test_loss=2.267   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.698   train_acc= 1.000   test_loss=2.262   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.691   train_acc= 1.000   test_loss=2.283   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.676   train_acc= 1.000   test_loss=2.238   test_acc= 0.889\n",
      "run time: 0.7430750687917074 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 4\n",
      "epoch= 0   train_loss= 2.987   train_acc= 0.663   test_loss=2.713   test_acc= 1.000\n",
      "epoch= 1   train_loss= 2.855   train_acc= 0.711   test_loss=2.630   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.790   train_acc= 0.771   test_loss=2.570   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.632   train_acc= 0.892   test_loss=2.509   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.592   train_acc= 0.867   test_loss=2.454   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.530   train_acc= 0.904   test_loss=2.420   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.458   train_acc= 0.916   test_loss=2.376   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.462   train_acc= 0.916   test_loss=2.362   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.408   train_acc= 0.928   test_loss=2.350   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.361   train_acc= 0.916   test_loss=2.330   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 10   train_loss= 2.322   train_acc= 0.940   test_loss=2.306   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.300   train_acc= 0.952   test_loss=2.276   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.265   train_acc= 0.964   test_loss=2.326   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.245   train_acc= 0.976   test_loss=2.291   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.179   train_acc= 0.988   test_loss=2.261   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.181   train_acc= 0.952   test_loss=2.224   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.165   train_acc= 0.976   test_loss=2.276   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.138   train_acc= 0.976   test_loss=2.307   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.099   train_acc= 0.988   test_loss=2.221   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.072   train_acc= 1.000   test_loss=2.234   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.062   train_acc= 1.000   test_loss=2.244   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.043   train_acc= 0.988   test_loss=2.199   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.025   train_acc= 1.000   test_loss=2.191   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.004   train_acc= 1.000   test_loss=2.169   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.998   train_acc= 1.000   test_loss=2.177   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.978   train_acc= 1.000   test_loss=2.166   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.971   train_acc= 0.988   test_loss=2.171   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.947   train_acc= 1.000   test_loss=2.150   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.935   train_acc= 0.988   test_loss=2.147   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.917   train_acc= 1.000   test_loss=2.138   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.910   train_acc= 1.000   test_loss=2.134   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.899   train_acc= 1.000   test_loss=2.157   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.888   train_acc= 1.000   test_loss=2.177   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.867   train_acc= 1.000   test_loss=2.148   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.861   train_acc= 1.000   test_loss=2.141   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.841   train_acc= 1.000   test_loss=2.083   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.825   train_acc= 1.000   test_loss=2.075   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.819   train_acc= 1.000   test_loss=2.067   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.814   train_acc= 1.000   test_loss=2.105   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.801   train_acc= 1.000   test_loss=2.114   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.784   train_acc= 1.000   test_loss=2.055   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.789   train_acc= 0.988   test_loss=2.035   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.766   train_acc= 1.000   test_loss=2.044   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.756   train_acc= 1.000   test_loss=2.061   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.753   train_acc= 1.000   test_loss=2.019   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.735   train_acc= 1.000   test_loss=2.005   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.723   train_acc= 1.000   test_loss=2.007   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.713   train_acc= 1.000   test_loss=1.986   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.706   train_acc= 1.000   test_loss=2.016   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.698   train_acc= 1.000   test_loss=2.015   test_acc= 0.889\n",
      "run time: 0.7282952825228374 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 5\n",
      "epoch= 0   train_loss= 2.998   train_acc= 0.554   test_loss=2.934   test_acc= 0.444\n",
      "epoch= 1   train_loss= 2.806   train_acc= 0.711   test_loss=2.894   test_acc= 0.556\n",
      "epoch= 2   train_loss= 2.733   train_acc= 0.795   test_loss=2.890   test_acc= 0.444\n",
      "epoch= 3   train_loss= 2.686   train_acc= 0.747   test_loss=2.908   test_acc= 0.444\n",
      "epoch= 4   train_loss= 2.602   train_acc= 0.867   test_loss=2.711   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.542   train_acc= 0.855   test_loss=2.801   test_acc= 0.556\n",
      "epoch= 6   train_loss= 2.467   train_acc= 0.904   test_loss=2.738   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.451   train_acc= 0.916   test_loss=2.748   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.395   train_acc= 0.904   test_loss=2.648   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.379   train_acc= 0.904   test_loss=2.643   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.346   train_acc= 0.928   test_loss=2.828   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.294   train_acc= 0.976   test_loss=2.701   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.285   train_acc= 0.904   test_loss=2.716   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.219   train_acc= 0.976   test_loss=2.713   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.194   train_acc= 0.976   test_loss=2.703   test_acc= 0.667\n",
      "epoch= 15   train_loss= 2.176   train_acc= 0.976   test_loss=2.761   test_acc= 0.667\n",
      "epoch= 16   train_loss= 2.152   train_acc= 1.000   test_loss=2.728   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.135   train_acc= 0.964   test_loss=2.684   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.098   train_acc= 0.988   test_loss=2.706   test_acc= 0.667\n",
      "epoch= 19   train_loss= 2.076   train_acc= 1.000   test_loss=2.647   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.054   train_acc= 1.000   test_loss=2.616   test_acc= 0.667\n",
      "epoch= 21   train_loss= 2.037   train_acc= 1.000   test_loss=2.599   test_acc= 0.667\n",
      "epoch= 22   train_loss= 2.031   train_acc= 1.000   test_loss=2.665   test_acc= 0.667\n",
      "epoch= 23   train_loss= 2.011   train_acc= 1.000   test_loss=2.852   test_acc= 0.667\n",
      "epoch= 24   train_loss= 2.002   train_acc= 1.000   test_loss=2.673   test_acc= 0.667\n",
      "epoch= 25   train_loss= 1.967   train_acc= 1.000   test_loss=2.696   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.952   train_acc= 1.000   test_loss=2.667   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.936   train_acc= 1.000   test_loss=2.566   test_acc= 0.667\n",
      "epoch= 28   train_loss= 1.926   train_acc= 1.000   test_loss=2.665   test_acc= 0.667\n",
      "epoch= 29   train_loss= 1.906   train_acc= 1.000   test_loss=2.720   test_acc= 0.667\n",
      "epoch= 30   train_loss= 1.924   train_acc= 0.988   test_loss=2.561   test_acc= 0.667\n",
      "epoch= 31   train_loss= 1.887   train_acc= 1.000   test_loss=2.611   test_acc= 0.667\n",
      "epoch= 32   train_loss= 1.881   train_acc= 1.000   test_loss=2.639   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.875   train_acc= 1.000   test_loss=2.648   test_acc= 0.667\n",
      "epoch= 34   train_loss= 1.849   train_acc= 1.000   test_loss=2.627   test_acc= 0.667\n",
      "epoch= 35   train_loss= 1.841   train_acc= 1.000   test_loss=2.684   test_acc= 0.667\n",
      "epoch= 36   train_loss= 1.822   train_acc= 1.000   test_loss=2.730   test_acc= 0.667\n",
      "epoch= 37   train_loss= 1.811   train_acc= 1.000   test_loss=2.634   test_acc= 0.667\n",
      "epoch= 38   train_loss= 1.803   train_acc= 1.000   test_loss=2.725   test_acc= 0.667\n",
      "epoch= 39   train_loss= 1.792   train_acc= 1.000   test_loss=2.664   test_acc= 0.667\n",
      "epoch= 40   train_loss= 1.778   train_acc= 1.000   test_loss=2.717   test_acc= 0.667\n",
      "epoch= 41   train_loss= 1.770   train_acc= 1.000   test_loss=2.658   test_acc= 0.667\n",
      "epoch= 42   train_loss= 1.762   train_acc= 1.000   test_loss=2.684   test_acc= 0.667\n",
      "epoch= 43   train_loss= 1.758   train_acc= 0.988   test_loss=2.487   test_acc= 0.667\n",
      "epoch= 44   train_loss= 1.751   train_acc= 0.988   test_loss=2.503   test_acc= 0.667\n",
      "epoch= 45   train_loss= 1.733   train_acc= 1.000   test_loss=2.451   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.727   train_acc= 1.000   test_loss=2.488   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.711   train_acc= 1.000   test_loss=2.496   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.700   train_acc= 1.000   test_loss=2.498   test_acc= 0.667\n",
      "epoch= 49   train_loss= 1.688   train_acc= 1.000   test_loss=2.481   test_acc= 0.667\n",
      "run time: 0.6998331189155579 min\n",
      "test_acc=0.667\n",
      "run= 0   fold= 6\n",
      "epoch= 0   train_loss= 2.966   train_acc= 0.639   test_loss=2.807   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.793   train_acc= 0.783   test_loss=2.844   test_acc= 0.556\n",
      "epoch= 2   train_loss= 2.698   train_acc= 0.807   test_loss=2.643   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.640   train_acc= 0.819   test_loss=2.695   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.595   train_acc= 0.843   test_loss=2.489   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.523   train_acc= 0.904   test_loss=2.440   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 6   train_loss= 2.495   train_acc= 0.867   test_loss=2.423   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.424   train_acc= 0.928   test_loss=2.344   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.427   train_acc= 0.880   test_loss=2.349   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.384   train_acc= 0.940   test_loss=2.274   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.336   train_acc= 0.916   test_loss=2.262   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.274   train_acc= 0.964   test_loss=2.232   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.259   train_acc= 0.940   test_loss=2.262   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.225   train_acc= 0.964   test_loss=2.160   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.190   train_acc= 0.964   test_loss=2.307   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.192   train_acc= 0.952   test_loss=2.262   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.154   train_acc= 0.988   test_loss=2.142   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.122   train_acc= 0.988   test_loss=2.128   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.101   train_acc= 0.988   test_loss=2.115   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.075   train_acc= 1.000   test_loss=2.181   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.074   train_acc= 0.988   test_loss=2.204   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.059   train_acc= 0.988   test_loss=2.083   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.012   train_acc= 1.000   test_loss=2.063   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.004   train_acc= 1.000   test_loss=2.087   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.990   train_acc= 1.000   test_loss=2.057   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.978   train_acc= 1.000   test_loss=2.049   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.962   train_acc= 1.000   test_loss=2.037   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.967   train_acc= 1.000   test_loss=2.096   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.947   train_acc= 1.000   test_loss=2.055   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.911   train_acc= 1.000   test_loss=2.014   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.902   train_acc= 1.000   test_loss=2.014   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.893   train_acc= 1.000   test_loss=1.941   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.888   train_acc= 1.000   test_loss=1.958   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.872   train_acc= 1.000   test_loss=1.933   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.862   train_acc= 1.000   test_loss=1.944   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.846   train_acc= 1.000   test_loss=1.956   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.840   train_acc= 1.000   test_loss=1.971   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.828   train_acc= 1.000   test_loss=1.895   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.816   train_acc= 1.000   test_loss=1.933   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.807   train_acc= 1.000   test_loss=1.925   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.791   train_acc= 1.000   test_loss=1.873   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.779   train_acc= 1.000   test_loss=1.845   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.771   train_acc= 1.000   test_loss=1.829   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.760   train_acc= 1.000   test_loss=1.791   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.746   train_acc= 1.000   test_loss=1.823   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.737   train_acc= 1.000   test_loss=1.832   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.735   train_acc= 1.000   test_loss=1.813   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.713   train_acc= 1.000   test_loss=1.810   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.703   train_acc= 1.000   test_loss=1.785   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.698   train_acc= 1.000   test_loss=1.803   test_acc= 0.889\n",
      "run time: 0.7318286180496216 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 7\n",
      "epoch= 0   train_loss= 2.938   train_acc= 0.687   test_loss=2.827   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.817   train_acc= 0.735   test_loss=2.772   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.769   train_acc= 0.711   test_loss=2.706   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.672   train_acc= 0.831   test_loss=2.641   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.661   train_acc= 0.831   test_loss=2.613   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.563   train_acc= 0.880   test_loss=2.567   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.494   train_acc= 0.916   test_loss=2.530   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.503   train_acc= 0.880   test_loss=2.508   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.403   train_acc= 0.916   test_loss=2.507   test_acc= 0.667\n",
      "epoch= 9   train_loss= 2.424   train_acc= 0.880   test_loss=2.493   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.390   train_acc= 0.916   test_loss=2.409   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.380   train_acc= 0.892   test_loss=2.351   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.324   train_acc= 0.867   test_loss=2.333   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.291   train_acc= 0.940   test_loss=2.348   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.270   train_acc= 0.928   test_loss=2.345   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.214   train_acc= 0.976   test_loss=2.351   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.224   train_acc= 0.916   test_loss=2.250   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.181   train_acc= 0.988   test_loss=2.229   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.153   train_acc= 0.964   test_loss=2.331   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.148   train_acc= 0.952   test_loss=2.177   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.094   train_acc= 0.988   test_loss=2.163   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.087   train_acc= 0.964   test_loss=2.154   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.058   train_acc= 1.000   test_loss=2.113   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.047   train_acc= 0.988   test_loss=2.094   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.029   train_acc= 1.000   test_loss=2.091   test_acc= 0.889\n",
      "epoch= 25   train_loss= 2.017   train_acc= 1.000   test_loss=2.051   test_acc= 1.000\n",
      "epoch= 26   train_loss= 2.005   train_acc= 0.988   test_loss=2.084   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.986   train_acc= 0.976   test_loss=2.035   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.953   train_acc= 1.000   test_loss=2.047   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.943   train_acc= 0.988   test_loss=2.000   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.950   train_acc= 0.988   test_loss=2.030   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.905   train_acc= 1.000   test_loss=2.003   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.909   train_acc= 1.000   test_loss=2.028   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.893   train_acc= 0.976   test_loss=1.976   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.878   train_acc= 1.000   test_loss=2.034   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.855   train_acc= 1.000   test_loss=1.973   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.849   train_acc= 1.000   test_loss=2.005   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.828   train_acc= 1.000   test_loss=1.945   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.824   train_acc= 1.000   test_loss=1.950   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.812   train_acc= 1.000   test_loss=1.956   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.799   train_acc= 1.000   test_loss=1.986   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.807   train_acc= 1.000   test_loss=1.949   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.779   train_acc= 1.000   test_loss=1.976   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.764   train_acc= 1.000   test_loss=1.921   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.751   train_acc= 1.000   test_loss=1.917   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.746   train_acc= 1.000   test_loss=1.896   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.730   train_acc= 1.000   test_loss=1.876   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.739   train_acc= 1.000   test_loss=1.889   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.713   train_acc= 1.000   test_loss=1.885   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.698   train_acc= 1.000   test_loss=1.896   test_acc= 0.889\n",
      "run time: 0.7106761693954468 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 8\n",
      "epoch= 0   train_loss= 2.975   train_acc= 0.627   test_loss=2.972   test_acc= 0.444\n",
      "epoch= 1   train_loss= 2.780   train_acc= 0.819   test_loss=2.887   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2   train_loss= 2.695   train_acc= 0.807   test_loss=2.877   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.644   train_acc= 0.807   test_loss=2.733   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.545   train_acc= 0.867   test_loss=2.716   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.510   train_acc= 0.855   test_loss=2.643   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.435   train_acc= 0.904   test_loss=2.681   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.402   train_acc= 0.892   test_loss=2.661   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.375   train_acc= 0.916   test_loss=2.627   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.312   train_acc= 0.952   test_loss=2.609   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.306   train_acc= 0.940   test_loss=2.542   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.287   train_acc= 0.952   test_loss=2.620   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.260   train_acc= 0.952   test_loss=2.570   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.223   train_acc= 0.976   test_loss=2.541   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.206   train_acc= 0.964   test_loss=2.550   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.171   train_acc= 0.976   test_loss=2.574   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.168   train_acc= 0.964   test_loss=2.545   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.135   train_acc= 0.976   test_loss=2.478   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.085   train_acc= 0.976   test_loss=2.558   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.090   train_acc= 1.000   test_loss=2.427   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.068   train_acc= 0.988   test_loss=2.479   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.044   train_acc= 0.988   test_loss=2.436   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.033   train_acc= 1.000   test_loss=2.433   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.026   train_acc= 0.976   test_loss=2.360   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.987   train_acc= 1.000   test_loss=2.364   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.975   train_acc= 1.000   test_loss=2.288   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.953   train_acc= 1.000   test_loss=2.318   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.937   train_acc= 1.000   test_loss=2.364   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.936   train_acc= 1.000   test_loss=2.373   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.912   train_acc= 1.000   test_loss=2.290   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.905   train_acc= 1.000   test_loss=2.351   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.886   train_acc= 1.000   test_loss=2.308   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.877   train_acc= 1.000   test_loss=2.277   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.865   train_acc= 1.000   test_loss=2.315   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.856   train_acc= 1.000   test_loss=2.269   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.834   train_acc= 1.000   test_loss=2.242   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.819   train_acc= 1.000   test_loss=2.280   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.818   train_acc= 1.000   test_loss=2.311   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.801   train_acc= 1.000   test_loss=2.316   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.796   train_acc= 1.000   test_loss=2.280   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.784   train_acc= 1.000   test_loss=2.284   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.768   train_acc= 1.000   test_loss=2.306   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.759   train_acc= 1.000   test_loss=2.299   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.745   train_acc= 1.000   test_loss=2.255   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.731   train_acc= 1.000   test_loss=2.271   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.726   train_acc= 1.000   test_loss=2.254   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.719   train_acc= 1.000   test_loss=2.133   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.711   train_acc= 1.000   test_loss=2.211   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.700   train_acc= 1.000   test_loss=2.163   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.689   train_acc= 1.000   test_loss=2.091   test_acc= 0.889\n",
      "run time: 0.7443446675936382 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 9\n",
      "epoch= 0   train_loss= 3.059   train_acc= 0.590   test_loss=2.871   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.787   train_acc= 0.771   test_loss=2.814   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.726   train_acc= 0.783   test_loss=2.804   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.624   train_acc= 0.843   test_loss=2.765   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.603   train_acc= 0.807   test_loss=2.735   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.546   train_acc= 0.892   test_loss=2.687   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.484   train_acc= 0.892   test_loss=2.675   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.462   train_acc= 0.892   test_loss=2.655   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.431   train_acc= 0.916   test_loss=2.607   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.380   train_acc= 0.892   test_loss=2.587   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.342   train_acc= 0.928   test_loss=2.559   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.287   train_acc= 0.964   test_loss=2.497   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.295   train_acc= 0.928   test_loss=2.498   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.260   train_acc= 0.940   test_loss=2.472   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.222   train_acc= 0.940   test_loss=2.431   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.221   train_acc= 0.928   test_loss=2.403   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.188   train_acc= 0.964   test_loss=2.407   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.169   train_acc= 0.952   test_loss=2.455   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.141   train_acc= 0.976   test_loss=2.390   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.102   train_acc= 0.964   test_loss=2.355   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.099   train_acc= 0.976   test_loss=2.346   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.068   train_acc= 0.976   test_loss=2.302   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.065   train_acc= 0.976   test_loss=2.306   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.026   train_acc= 0.976   test_loss=2.284   test_acc= 0.778\n",
      "epoch= 24   train_loss= 2.008   train_acc= 0.988   test_loss=2.241   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.982   train_acc= 1.000   test_loss=2.243   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.987   train_acc= 0.964   test_loss=2.211   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.966   train_acc= 0.988   test_loss=2.190   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.949   train_acc= 1.000   test_loss=2.199   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.946   train_acc= 0.988   test_loss=2.152   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.911   train_acc= 1.000   test_loss=2.154   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.902   train_acc= 1.000   test_loss=2.141   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.882   train_acc= 1.000   test_loss=2.153   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.876   train_acc= 1.000   test_loss=2.123   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.866   train_acc= 1.000   test_loss=2.151   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.850   train_acc= 1.000   test_loss=2.107   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.841   train_acc= 1.000   test_loss=2.095   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.822   train_acc= 1.000   test_loss=2.088   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.815   train_acc= 1.000   test_loss=2.108   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.797   train_acc= 1.000   test_loss=2.073   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.800   train_acc= 1.000   test_loss=2.070   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.781   train_acc= 1.000   test_loss=2.051   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.776   train_acc= 1.000   test_loss=2.031   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.770   train_acc= 1.000   test_loss=1.993   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.754   train_acc= 1.000   test_loss=2.028   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.735   train_acc= 1.000   test_loss=2.013   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.726   train_acc= 1.000   test_loss=1.988   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.714   train_acc= 1.000   test_loss=1.964   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 48   train_loss= 1.708   train_acc= 1.000   test_loss=1.938   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.695   train_acc= 1.000   test_loss=1.952   test_acc= 0.889\n",
      "run time: 0.7161436160405477 min\n",
      "test_acc=0.889\n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-211     \n",
      "MUSK-211     \n",
      "MUSK-212     \n",
      "MUSK-212     \n",
      "MUSK-212     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-219     \n",
      "MUSK-219     \n",
      "MUSK-224     \n",
      "MUSK-224     \n",
      "MUSK-227     \n",
      "MUSK-227     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-254     \n",
      "MUSK-254     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-272     \n",
      "MUSK-272     \n",
      "MUSK-272     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-311     \n",
      "MUSK-311     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-321     \n",
      "MUSK-321     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-344     \n",
      "MUSK-344     \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f159    \n",
      "MUSK-f159    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-j33     \n",
      "MUSK-j33     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-jf17    \n",
      "MUSK-jf17    \n",
      "MUSK-jf17    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-208 \n",
      "NON-MUSK-208 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-226 \n",
      "NON-MUSK-226 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-247 \n",
      "NON-MUSK-247 \n",
      "NON-MUSK-249 \n",
      "NON-MUSK-249 \n",
      "NON-MUSK-253 \n",
      "NON-MUSK-253 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-271 \n",
      "NON-MUSK-271 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-290 \n",
      "NON-MUSK-290 \n",
      "NON-MUSK-295 \n",
      "NON-MUSK-295 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-305 \n",
      "NON-MUSK-305 \n",
      "NON-MUSK-308 \n",
      "NON-MUSK-308 \n",
      "NON-MUSK-309 \n",
      "NON-MUSK-309 \n",
      "NON-MUSK-318 \n",
      "NON-MUSK-318 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-320 \n",
      "NON-MUSK-320 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-334 \n",
      "NON-MUSK-334 \n",
      "NON-MUSK-f150\n",
      "NON-MUSK-f150\n",
      "NON-MUSK-f161\n",
      "NON-MUSK-f161\n",
      "NON-MUSK-f164\n",
      "NON-MUSK-f164\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j130\n",
      "NON-MUSK-j130\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j148\n",
      "NON-MUSK-j148\n",
      "NON-MUSK-j81 \n",
      "NON-MUSK-j81 \n",
      "NON-MUSK-j83 \n",
      "NON-MUSK-j83 \n",
      "NON-MUSK-j84 \n",
      "NON-MUSK-j84 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "run= 1   fold= 0\n",
      "epoch= 0   train_loss= 2.943   train_acc= 0.720   test_loss=2.948   test_acc= 0.600\n",
      "epoch= 1   train_loss= 2.791   train_acc= 0.756   test_loss=2.813   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.678   train_acc= 0.854   test_loss=2.756   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.697   train_acc= 0.829   test_loss=2.728   test_acc= 0.800\n",
      "epoch= 4   train_loss= 2.578   train_acc= 0.866   test_loss=2.657   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.512   train_acc= 0.878   test_loss=2.592   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.476   train_acc= 0.915   test_loss=2.573   test_acc= 0.800\n",
      "epoch= 7   train_loss= 2.445   train_acc= 0.902   test_loss=2.482   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.425   train_acc= 0.915   test_loss=2.543   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.343   train_acc= 0.963   test_loss=2.434   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.353   train_acc= 0.927   test_loss=2.459   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.305   train_acc= 0.963   test_loss=2.543   test_acc= 0.700\n",
      "epoch= 12   train_loss= 2.285   train_acc= 0.976   test_loss=2.405   test_acc= 0.800\n",
      "epoch= 13   train_loss= 2.242   train_acc= 0.963   test_loss=2.385   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.208   train_acc= 0.988   test_loss=2.308   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.198   train_acc= 0.988   test_loss=2.383   test_acc= 0.900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 16   train_loss= 2.172   train_acc= 0.976   test_loss=2.296   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.149   train_acc= 0.988   test_loss=2.268   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.139   train_acc= 0.988   test_loss=2.238   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.085   train_acc= 1.000   test_loss=2.233   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.083   train_acc= 0.988   test_loss=2.249   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.067   train_acc= 1.000   test_loss=2.214   test_acc= 0.900\n",
      "epoch= 22   train_loss= 2.048   train_acc= 0.988   test_loss=2.228   test_acc= 0.900\n",
      "epoch= 23   train_loss= 2.028   train_acc= 1.000   test_loss=2.135   test_acc= 0.900\n",
      "epoch= 24   train_loss= 2.021   train_acc= 1.000   test_loss=2.209   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.982   train_acc= 1.000   test_loss=2.106   test_acc= 0.900\n",
      "epoch= 26   train_loss= 1.982   train_acc= 1.000   test_loss=2.142   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.974   train_acc= 1.000   test_loss=2.054   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.953   train_acc= 1.000   test_loss=2.096   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.939   train_acc= 1.000   test_loss=2.069   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.937   train_acc= 1.000   test_loss=2.127   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.916   train_acc= 1.000   test_loss=2.033   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.902   train_acc= 1.000   test_loss=2.015   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.881   train_acc= 1.000   test_loss=2.026   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.875   train_acc= 1.000   test_loss=2.046   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.858   train_acc= 1.000   test_loss=1.978   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.850   train_acc= 1.000   test_loss=1.981   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.835   train_acc= 1.000   test_loss=1.966   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.827   train_acc= 1.000   test_loss=1.974   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.812   train_acc= 1.000   test_loss=1.954   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.809   train_acc= 1.000   test_loss=1.984   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.797   train_acc= 1.000   test_loss=1.947   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.784   train_acc= 1.000   test_loss=1.963   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.776   train_acc= 1.000   test_loss=1.898   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.764   train_acc= 1.000   test_loss=1.889   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.749   train_acc= 1.000   test_loss=1.897   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.742   train_acc= 1.000   test_loss=1.900   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.737   train_acc= 1.000   test_loss=1.866   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.724   train_acc= 1.000   test_loss=1.855   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.711   train_acc= 1.000   test_loss=1.869   test_acc= 0.900\n",
      "run time: 0.7124800682067871 min\n",
      "test_acc=0.900\n",
      "run= 1   fold= 1\n",
      "epoch= 0   train_loss= 2.925   train_acc= 0.683   test_loss=2.959   test_acc= 0.600\n",
      "epoch= 1   train_loss= 2.826   train_acc= 0.744   test_loss=2.868   test_acc= 0.800\n",
      "epoch= 2   train_loss= 2.661   train_acc= 0.854   test_loss=2.853   test_acc= 0.800\n",
      "epoch= 3   train_loss= 2.612   train_acc= 0.878   test_loss=2.817   test_acc= 0.700\n",
      "epoch= 4   train_loss= 2.506   train_acc= 0.915   test_loss=2.813   test_acc= 0.700\n",
      "epoch= 5   train_loss= 2.470   train_acc= 0.915   test_loss=2.798   test_acc= 0.700\n",
      "epoch= 6   train_loss= 2.442   train_acc= 0.927   test_loss=2.769   test_acc= 0.700\n",
      "epoch= 7   train_loss= 2.374   train_acc= 0.951   test_loss=2.766   test_acc= 0.700\n",
      "epoch= 8   train_loss= 2.352   train_acc= 0.963   test_loss=2.900   test_acc= 0.700\n",
      "epoch= 9   train_loss= 2.317   train_acc= 0.951   test_loss=2.959   test_acc= 0.700\n",
      "epoch= 10   train_loss= 2.329   train_acc= 0.902   test_loss=2.800   test_acc= 0.700\n",
      "epoch= 11   train_loss= 2.261   train_acc= 0.963   test_loss=2.941   test_acc= 0.800\n",
      "epoch= 12   train_loss= 2.230   train_acc= 0.976   test_loss=2.913   test_acc= 0.800\n",
      "epoch= 13   train_loss= 2.193   train_acc= 0.988   test_loss=2.780   test_acc= 0.700\n",
      "epoch= 14   train_loss= 2.184   train_acc= 0.976   test_loss=2.787   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.166   train_acc= 0.988   test_loss=2.876   test_acc= 0.700\n",
      "epoch= 16   train_loss= 2.147   train_acc= 0.988   test_loss=2.946   test_acc= 0.700\n",
      "epoch= 17   train_loss= 2.107   train_acc= 1.000   test_loss=2.922   test_acc= 0.800\n",
      "epoch= 18   train_loss= 2.103   train_acc= 1.000   test_loss=2.764   test_acc= 0.700\n",
      "epoch= 19   train_loss= 2.075   train_acc= 1.000   test_loss=2.835   test_acc= 0.700\n",
      "epoch= 20   train_loss= 2.067   train_acc= 1.000   test_loss=2.865   test_acc= 0.800\n",
      "epoch= 21   train_loss= 2.034   train_acc= 1.000   test_loss=2.842   test_acc= 0.800\n",
      "epoch= 22   train_loss= 2.032   train_acc= 1.000   test_loss=2.853   test_acc= 0.800\n",
      "epoch= 23   train_loss= 2.019   train_acc= 1.000   test_loss=2.872   test_acc= 0.700\n",
      "epoch= 24   train_loss= 1.999   train_acc= 1.000   test_loss=2.855   test_acc= 0.700\n",
      "epoch= 25   train_loss= 1.987   train_acc= 1.000   test_loss=2.882   test_acc= 0.800\n",
      "epoch= 26   train_loss= 1.987   train_acc= 0.988   test_loss=2.764   test_acc= 0.700\n",
      "epoch= 27   train_loss= 1.953   train_acc= 1.000   test_loss=2.821   test_acc= 0.800\n",
      "epoch= 28   train_loss= 1.945   train_acc= 1.000   test_loss=2.874   test_acc= 0.800\n",
      "epoch= 29   train_loss= 1.940   train_acc= 0.988   test_loss=2.778   test_acc= 0.800\n",
      "epoch= 30   train_loss= 1.910   train_acc= 1.000   test_loss=2.820   test_acc= 0.800\n",
      "epoch= 31   train_loss= 1.911   train_acc= 1.000   test_loss=2.694   test_acc= 0.800\n",
      "epoch= 32   train_loss= 1.897   train_acc= 1.000   test_loss=2.810   test_acc= 0.800\n",
      "epoch= 33   train_loss= 1.878   train_acc= 1.000   test_loss=2.724   test_acc= 0.800\n",
      "epoch= 34   train_loss= 1.866   train_acc= 0.988   test_loss=2.760   test_acc= 0.800\n",
      "epoch= 35   train_loss= 1.854   train_acc= 1.000   test_loss=2.729   test_acc= 0.800\n",
      "epoch= 36   train_loss= 1.836   train_acc= 1.000   test_loss=2.751   test_acc= 0.800\n",
      "epoch= 37   train_loss= 1.828   train_acc= 1.000   test_loss=2.733   test_acc= 0.800\n",
      "epoch= 38   train_loss= 1.812   train_acc= 1.000   test_loss=2.743   test_acc= 0.800\n",
      "epoch= 39   train_loss= 1.815   train_acc= 1.000   test_loss=2.764   test_acc= 0.800\n",
      "epoch= 40   train_loss= 1.793   train_acc= 1.000   test_loss=2.690   test_acc= 0.800\n",
      "epoch= 41   train_loss= 1.785   train_acc= 1.000   test_loss=2.691   test_acc= 0.800\n",
      "epoch= 42   train_loss= 1.771   train_acc= 1.000   test_loss=2.664   test_acc= 0.800\n",
      "epoch= 43   train_loss= 1.764   train_acc= 1.000   test_loss=2.650   test_acc= 0.800\n",
      "epoch= 44   train_loss= 1.750   train_acc= 1.000   test_loss=2.633   test_acc= 0.800\n",
      "epoch= 45   train_loss= 1.751   train_acc= 1.000   test_loss=2.605   test_acc= 0.800\n",
      "epoch= 46   train_loss= 1.738   train_acc= 1.000   test_loss=2.639   test_acc= 0.800\n",
      "epoch= 47   train_loss= 1.725   train_acc= 1.000   test_loss=2.612   test_acc= 0.800\n",
      "epoch= 48   train_loss= 1.712   train_acc= 1.000   test_loss=2.615   test_acc= 0.800\n",
      "epoch= 49   train_loss= 1.707   train_acc= 1.000   test_loss=2.627   test_acc= 0.800\n",
      "run time: 0.7190778652826945 min\n",
      "test_acc=0.800\n",
      "run= 1   fold= 2\n",
      "epoch= 0   train_loss= 2.951   train_acc= 0.518   test_loss=2.795   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.811   train_acc= 0.735   test_loss=2.691   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.725   train_acc= 0.771   test_loss=2.594   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.652   train_acc= 0.795   test_loss=2.513   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.585   train_acc= 0.855   test_loss=2.445   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.510   train_acc= 0.867   test_loss=2.389   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.433   train_acc= 0.904   test_loss=2.320   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.432   train_acc= 0.855   test_loss=2.298   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.377   train_acc= 0.940   test_loss=2.292   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.314   train_acc= 0.928   test_loss=2.269   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.302   train_acc= 0.940   test_loss=2.264   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.270   train_acc= 0.952   test_loss=2.217   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 12   train_loss= 2.254   train_acc= 0.952   test_loss=2.191   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.217   train_acc= 0.952   test_loss=2.187   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.203   train_acc= 0.928   test_loss=2.151   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.185   train_acc= 0.964   test_loss=2.155   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.133   train_acc= 0.976   test_loss=2.147   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.118   train_acc= 0.988   test_loss=2.114   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.088   train_acc= 0.988   test_loss=2.100   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.047   train_acc= 1.000   test_loss=2.069   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.049   train_acc= 1.000   test_loss=2.084   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.024   train_acc= 0.988   test_loss=2.051   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.003   train_acc= 1.000   test_loss=2.037   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.001   train_acc= 0.988   test_loss=2.009   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.989   train_acc= 0.988   test_loss=1.993   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.987   train_acc= 0.988   test_loss=1.987   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.951   train_acc= 1.000   test_loss=2.012   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.934   train_acc= 1.000   test_loss=1.993   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.910   train_acc= 1.000   test_loss=1.979   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.909   train_acc= 1.000   test_loss=1.948   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.880   train_acc= 1.000   test_loss=1.930   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.879   train_acc= 1.000   test_loss=1.904   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.874   train_acc= 1.000   test_loss=1.879   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.859   train_acc= 1.000   test_loss=1.876   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.844   train_acc= 1.000   test_loss=1.875   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.831   train_acc= 1.000   test_loss=1.861   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.823   train_acc= 1.000   test_loss=1.854   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.808   train_acc= 1.000   test_loss=1.839   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.790   train_acc= 1.000   test_loss=1.828   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.792   train_acc= 1.000   test_loss=1.834   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.778   train_acc= 1.000   test_loss=1.792   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.765   train_acc= 1.000   test_loss=1.785   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.759   train_acc= 1.000   test_loss=1.788   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.743   train_acc= 1.000   test_loss=1.779   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.732   train_acc= 1.000   test_loss=1.766   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.726   train_acc= 1.000   test_loss=1.740   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.714   train_acc= 1.000   test_loss=1.729   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.708   train_acc= 1.000   test_loss=1.744   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.702   train_acc= 1.000   test_loss=1.726   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.682   train_acc= 1.000   test_loss=1.718   test_acc= 1.000\n",
      "run time: 0.7207656701405843 min\n",
      "test_acc=1.000\n",
      "run= 1   fold= 3\n",
      "epoch= 0   train_loss= 3.071   train_acc= 0.506   test_loss=2.838   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.854   train_acc= 0.735   test_loss=2.747   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.695   train_acc= 0.843   test_loss=2.672   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.638   train_acc= 0.819   test_loss=2.644   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.583   train_acc= 0.880   test_loss=2.536   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.528   train_acc= 0.843   test_loss=2.546   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.506   train_acc= 0.904   test_loss=2.486   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.445   train_acc= 0.904   test_loss=2.542   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.416   train_acc= 0.940   test_loss=2.466   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.351   train_acc= 0.928   test_loss=2.444   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.331   train_acc= 0.952   test_loss=2.441   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.304   train_acc= 0.952   test_loss=2.370   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.272   train_acc= 0.976   test_loss=2.339   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.260   train_acc= 0.976   test_loss=2.408   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.213   train_acc= 0.976   test_loss=2.331   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.210   train_acc= 0.952   test_loss=2.300   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.156   train_acc= 0.976   test_loss=2.365   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.135   train_acc= 0.988   test_loss=2.267   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.118   train_acc= 0.988   test_loss=2.274   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.099   train_acc= 0.988   test_loss=2.302   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.063   train_acc= 1.000   test_loss=2.209   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.070   train_acc= 0.988   test_loss=2.224   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.044   train_acc= 0.988   test_loss=2.234   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.025   train_acc= 0.988   test_loss=2.136   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.007   train_acc= 1.000   test_loss=2.130   test_acc= 0.889\n",
      "epoch= 25   train_loss= 2.006   train_acc= 0.976   test_loss=2.157   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.977   train_acc= 1.000   test_loss=2.150   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.958   train_acc= 1.000   test_loss=2.117   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.951   train_acc= 1.000   test_loss=2.155   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.931   train_acc= 1.000   test_loss=2.102   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.935   train_acc= 0.988   test_loss=2.194   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.900   train_acc= 1.000   test_loss=2.121   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.896   train_acc= 1.000   test_loss=2.076   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.872   train_acc= 1.000   test_loss=2.037   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.871   train_acc= 1.000   test_loss=2.050   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.866   train_acc= 1.000   test_loss=2.010   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.846   train_acc= 1.000   test_loss=2.032   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.834   train_acc= 1.000   test_loss=2.027   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.814   train_acc= 1.000   test_loss=2.012   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.814   train_acc= 1.000   test_loss=2.000   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.793   train_acc= 1.000   test_loss=2.025   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.784   train_acc= 1.000   test_loss=1.997   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.773   train_acc= 1.000   test_loss=2.030   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.762   train_acc= 1.000   test_loss=2.006   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.757   train_acc= 1.000   test_loss=2.036   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.754   train_acc= 1.000   test_loss=2.001   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.727   train_acc= 1.000   test_loss=1.930   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.718   train_acc= 1.000   test_loss=1.951   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.719   train_acc= 1.000   test_loss=1.913   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.698   train_acc= 1.000   test_loss=1.908   test_acc= 0.889\n",
      "run time: 0.7046672185262044 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 4\n",
      "epoch= 0   train_loss= 2.993   train_acc= 0.614   test_loss=2.857   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.773   train_acc= 0.819   test_loss=2.766   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.681   train_acc= 0.807   test_loss=2.687   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.614   train_acc= 0.843   test_loss=2.648   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.543   train_acc= 0.843   test_loss=2.621   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.529   train_acc= 0.855   test_loss=2.598   test_acc= 0.667\n",
      "epoch= 6   train_loss= 2.492   train_acc= 0.880   test_loss=2.571   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.424   train_acc= 0.940   test_loss=2.550   test_acc= 0.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 8   train_loss= 2.398   train_acc= 0.904   test_loss=2.525   test_acc= 0.667\n",
      "epoch= 9   train_loss= 2.349   train_acc= 0.940   test_loss=2.508   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.317   train_acc= 0.904   test_loss=2.488   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.301   train_acc= 0.952   test_loss=2.496   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.273   train_acc= 0.940   test_loss=2.470   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.228   train_acc= 0.964   test_loss=2.466   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.198   train_acc= 0.964   test_loss=2.444   test_acc= 0.667\n",
      "epoch= 15   train_loss= 2.188   train_acc= 0.964   test_loss=2.460   test_acc= 0.667\n",
      "epoch= 16   train_loss= 2.169   train_acc= 0.976   test_loss=2.442   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.117   train_acc= 0.988   test_loss=2.356   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.099   train_acc= 1.000   test_loss=2.365   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.091   train_acc= 0.976   test_loss=2.345   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.075   train_acc= 0.976   test_loss=2.347   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.062   train_acc= 1.000   test_loss=2.344   test_acc= 0.667\n",
      "epoch= 22   train_loss= 2.030   train_acc= 1.000   test_loss=2.327   test_acc= 0.667\n",
      "epoch= 23   train_loss= 2.000   train_acc= 1.000   test_loss=2.323   test_acc= 0.667\n",
      "epoch= 24   train_loss= 1.998   train_acc= 1.000   test_loss=2.301   test_acc= 0.667\n",
      "epoch= 25   train_loss= 1.992   train_acc= 0.988   test_loss=2.322   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.969   train_acc= 1.000   test_loss=2.285   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.948   train_acc= 1.000   test_loss=2.261   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.945   train_acc= 1.000   test_loss=2.251   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.931   train_acc= 0.988   test_loss=2.261   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.916   train_acc= 1.000   test_loss=2.227   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.902   train_acc= 1.000   test_loss=2.210   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.883   train_acc= 1.000   test_loss=2.216   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.872   train_acc= 1.000   test_loss=2.193   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.861   train_acc= 1.000   test_loss=2.190   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.847   train_acc= 1.000   test_loss=2.197   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.834   train_acc= 1.000   test_loss=2.172   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.824   train_acc= 1.000   test_loss=2.177   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.814   train_acc= 1.000   test_loss=2.188   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.808   train_acc= 1.000   test_loss=2.185   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.787   train_acc= 1.000   test_loss=2.148   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.773   train_acc= 1.000   test_loss=2.142   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.771   train_acc= 1.000   test_loss=2.120   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.755   train_acc= 1.000   test_loss=2.115   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.749   train_acc= 1.000   test_loss=2.104   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.739   train_acc= 1.000   test_loss=2.119   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.733   train_acc= 1.000   test_loss=2.110   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.721   train_acc= 1.000   test_loss=2.078   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.711   train_acc= 1.000   test_loss=2.042   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.702   train_acc= 1.000   test_loss=2.042   test_acc= 0.778\n",
      "run time: 0.7140041828155518 min\n",
      "test_acc=0.778\n",
      "run= 1   fold= 5\n",
      "epoch= 0   train_loss= 2.963   train_acc= 0.518   test_loss=2.759   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.739   train_acc= 0.807   test_loss=2.678   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.652   train_acc= 0.795   test_loss=2.697   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.547   train_acc= 0.843   test_loss=2.655   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.512   train_acc= 0.855   test_loss=2.692   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.468   train_acc= 0.867   test_loss=2.698   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.424   train_acc= 0.928   test_loss=2.755   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.348   train_acc= 0.928   test_loss=2.711   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.331   train_acc= 0.952   test_loss=2.714   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.274   train_acc= 0.940   test_loss=2.737   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.284   train_acc= 0.928   test_loss=2.774   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.216   train_acc= 0.988   test_loss=2.831   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.197   train_acc= 0.964   test_loss=2.790   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.153   train_acc= 1.000   test_loss=2.791   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.168   train_acc= 0.964   test_loss=2.831   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.127   train_acc= 0.988   test_loss=2.858   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.115   train_acc= 1.000   test_loss=2.903   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.070   train_acc= 1.000   test_loss=2.932   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.061   train_acc= 1.000   test_loss=2.897   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.066   train_acc= 0.988   test_loss=2.906   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.029   train_acc= 1.000   test_loss=2.952   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.006   train_acc= 1.000   test_loss=2.950   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.995   train_acc= 1.000   test_loss=2.975   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.969   train_acc= 1.000   test_loss=2.980   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.962   train_acc= 1.000   test_loss=2.987   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.950   train_acc= 1.000   test_loss=2.986   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.935   train_acc= 1.000   test_loss=2.960   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.928   train_acc= 1.000   test_loss=2.944   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.912   train_acc= 1.000   test_loss=2.984   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.890   train_acc= 1.000   test_loss=2.970   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.879   train_acc= 1.000   test_loss=2.985   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.863   train_acc= 1.000   test_loss=2.965   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.856   train_acc= 1.000   test_loss=2.987   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.845   train_acc= 1.000   test_loss=2.991   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.834   train_acc= 1.000   test_loss=3.008   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.821   train_acc= 1.000   test_loss=2.990   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.817   train_acc= 1.000   test_loss=2.975   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.809   train_acc= 1.000   test_loss=2.956   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.792   train_acc= 1.000   test_loss=2.946   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.775   train_acc= 1.000   test_loss=2.942   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.766   train_acc= 1.000   test_loss=2.943   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.755   train_acc= 1.000   test_loss=2.941   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.748   train_acc= 1.000   test_loss=2.973   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.735   train_acc= 1.000   test_loss=2.968   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.720   train_acc= 1.000   test_loss=2.950   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.716   train_acc= 1.000   test_loss=2.899   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.709   train_acc= 1.000   test_loss=2.922   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.690   train_acc= 1.000   test_loss=2.939   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.686   train_acc= 1.000   test_loss=2.964   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.671   train_acc= 1.000   test_loss=2.960   test_acc= 0.778\n",
      "run time: 0.7140609463055928 min\n",
      "test_acc=0.778\n",
      "run= 1   fold= 6\n",
      "epoch= 0   train_loss= 2.952   train_acc= 0.602   test_loss=2.925   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.806   train_acc= 0.723   test_loss=2.758   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.691   train_acc= 0.867   test_loss=2.706   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.621   train_acc= 0.819   test_loss=2.618   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 4   train_loss= 2.566   train_acc= 0.819   test_loss=2.527   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.515   train_acc= 0.892   test_loss=2.493   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.461   train_acc= 0.855   test_loss=2.476   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.432   train_acc= 0.867   test_loss=2.466   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.382   train_acc= 0.940   test_loss=2.371   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.340   train_acc= 0.964   test_loss=2.344   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.322   train_acc= 0.928   test_loss=2.434   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.314   train_acc= 0.928   test_loss=2.303   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.275   train_acc= 0.940   test_loss=2.311   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.228   train_acc= 0.964   test_loss=2.313   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.227   train_acc= 0.964   test_loss=2.255   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.179   train_acc= 0.964   test_loss=2.251   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.156   train_acc= 0.964   test_loss=2.230   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.139   train_acc= 0.988   test_loss=2.157   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.107   train_acc= 0.976   test_loss=2.279   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.111   train_acc= 0.976   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.058   train_acc= 1.000   test_loss=2.180   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.054   train_acc= 1.000   test_loss=2.118   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.035   train_acc= 0.988   test_loss=2.174   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.008   train_acc= 1.000   test_loss=2.079   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.998   train_acc= 1.000   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.987   train_acc= 1.000   test_loss=2.184   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.973   train_acc= 0.988   test_loss=2.055   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.958   train_acc= 0.988   test_loss=2.062   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.933   train_acc= 1.000   test_loss=2.049   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.920   train_acc= 1.000   test_loss=2.109   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.906   train_acc= 1.000   test_loss=2.062   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.891   train_acc= 1.000   test_loss=2.035   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.882   train_acc= 1.000   test_loss=2.055   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.878   train_acc= 1.000   test_loss=2.011   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.857   train_acc= 1.000   test_loss=2.010   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.849   train_acc= 1.000   test_loss=1.959   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.828   train_acc= 1.000   test_loss=2.021   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.814   train_acc= 1.000   test_loss=2.012   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.821   train_acc= 1.000   test_loss=1.964   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.801   train_acc= 1.000   test_loss=1.935   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.783   train_acc= 1.000   test_loss=1.893   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.779   train_acc= 1.000   test_loss=1.921   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.761   train_acc= 1.000   test_loss=1.921   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.755   train_acc= 1.000   test_loss=1.870   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.748   train_acc= 1.000   test_loss=1.855   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.739   train_acc= 0.988   test_loss=1.871   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.731   train_acc= 1.000   test_loss=1.854   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.713   train_acc= 1.000   test_loss=1.834   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.709   train_acc= 1.000   test_loss=1.829   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.692   train_acc= 1.000   test_loss=1.817   test_acc= 0.889\n",
      "run time: 0.7208602984746297 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 7\n",
      "epoch= 0   train_loss= 2.972   train_acc= 0.578   test_loss=2.940   test_acc= 0.444\n",
      "epoch= 1   train_loss= 2.803   train_acc= 0.771   test_loss=2.814   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.672   train_acc= 0.819   test_loss=2.800   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.634   train_acc= 0.855   test_loss=2.684   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.601   train_acc= 0.867   test_loss=2.645   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.508   train_acc= 0.867   test_loss=2.616   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.489   train_acc= 0.880   test_loss=2.713   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.425   train_acc= 0.952   test_loss=2.540   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.396   train_acc= 0.916   test_loss=2.548   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.370   train_acc= 0.904   test_loss=2.529   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.322   train_acc= 0.928   test_loss=2.612   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.306   train_acc= 0.928   test_loss=2.577   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.266   train_acc= 0.940   test_loss=2.454   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.240   train_acc= 0.940   test_loss=2.421   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.205   train_acc= 0.964   test_loss=2.413   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.188   train_acc= 0.964   test_loss=2.436   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.144   train_acc= 0.964   test_loss=2.411   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.122   train_acc= 0.988   test_loss=2.411   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.105   train_acc= 0.988   test_loss=2.287   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.106   train_acc= 0.976   test_loss=2.301   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.068   train_acc= 1.000   test_loss=2.295   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.047   train_acc= 0.988   test_loss=2.405   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.016   train_acc= 1.000   test_loss=2.284   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.018   train_acc= 1.000   test_loss=2.202   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.985   train_acc= 1.000   test_loss=2.262   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.977   train_acc= 1.000   test_loss=2.323   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.979   train_acc= 0.988   test_loss=2.430   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.965   train_acc= 1.000   test_loss=2.302   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.927   train_acc= 1.000   test_loss=2.224   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.923   train_acc= 1.000   test_loss=2.212   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.905   train_acc= 1.000   test_loss=2.168   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.896   train_acc= 1.000   test_loss=2.182   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.881   train_acc= 1.000   test_loss=2.251   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.883   train_acc= 1.000   test_loss=2.116   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.857   train_acc= 1.000   test_loss=2.157   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.844   train_acc= 1.000   test_loss=2.227   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.829   train_acc= 1.000   test_loss=2.140   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.818   train_acc= 1.000   test_loss=2.160   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.806   train_acc= 1.000   test_loss=2.082   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.797   train_acc= 1.000   test_loss=2.180   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.792   train_acc= 1.000   test_loss=2.098   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.777   train_acc= 1.000   test_loss=2.125   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.766   train_acc= 1.000   test_loss=2.175   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.755   train_acc= 1.000   test_loss=2.074   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.746   train_acc= 1.000   test_loss=2.147   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.730   train_acc= 1.000   test_loss=2.089   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.718   train_acc= 1.000   test_loss=2.130   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.712   train_acc= 1.000   test_loss=2.077   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.708   train_acc= 1.000   test_loss=2.096   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.694   train_acc= 1.000   test_loss=2.077   test_acc= 0.778\n",
      "run time: 0.7286408702532451 min\n",
      "test_acc=0.778\n",
      "run= 1   fold= 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0   train_loss= 2.983   train_acc= 0.494   test_loss=2.813   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.831   train_acc= 0.747   test_loss=2.784   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.733   train_acc= 0.807   test_loss=2.728   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.621   train_acc= 0.880   test_loss=2.720   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.600   train_acc= 0.843   test_loss=2.652   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.504   train_acc= 0.904   test_loss=2.699   test_acc= 0.667\n",
      "epoch= 6   train_loss= 2.455   train_acc= 0.904   test_loss=2.692   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.403   train_acc= 0.940   test_loss=2.718   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.382   train_acc= 0.916   test_loss=2.717   test_acc= 0.667\n",
      "epoch= 9   train_loss= 2.358   train_acc= 0.964   test_loss=2.503   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.349   train_acc= 0.952   test_loss=2.584   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.278   train_acc= 0.952   test_loss=2.596   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.237   train_acc= 0.976   test_loss=2.607   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.219   train_acc= 0.940   test_loss=2.411   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.206   train_acc= 0.964   test_loss=2.784   test_acc= 0.667\n",
      "epoch= 15   train_loss= 2.186   train_acc= 0.964   test_loss=2.782   test_acc= 0.667\n",
      "epoch= 16   train_loss= 2.142   train_acc= 0.988   test_loss=2.795   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.140   train_acc= 0.988   test_loss=2.719   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.107   train_acc= 0.976   test_loss=2.795   test_acc= 0.667\n",
      "epoch= 19   train_loss= 2.095   train_acc= 0.964   test_loss=2.769   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.065   train_acc= 0.988   test_loss=2.692   test_acc= 0.667\n",
      "epoch= 21   train_loss= 2.046   train_acc= 1.000   test_loss=2.786   test_acc= 0.667\n",
      "epoch= 22   train_loss= 2.026   train_acc= 1.000   test_loss=2.817   test_acc= 0.667\n",
      "epoch= 23   train_loss= 2.021   train_acc= 1.000   test_loss=2.820   test_acc= 0.667\n",
      "epoch= 24   train_loss= 1.992   train_acc= 1.000   test_loss=2.924   test_acc= 0.667\n",
      "epoch= 25   train_loss= 1.971   train_acc= 1.000   test_loss=2.929   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.976   train_acc= 0.976   test_loss=2.902   test_acc= 0.667\n",
      "epoch= 27   train_loss= 1.975   train_acc= 0.976   test_loss=2.940   test_acc= 0.667\n",
      "epoch= 28   train_loss= 1.934   train_acc= 1.000   test_loss=2.987   test_acc= 0.667\n",
      "epoch= 29   train_loss= 1.927   train_acc= 1.000   test_loss=2.773   test_acc= 0.667\n",
      "epoch= 30   train_loss= 1.910   train_acc= 1.000   test_loss=2.890   test_acc= 0.667\n",
      "epoch= 31   train_loss= 1.901   train_acc= 1.000   test_loss=2.944   test_acc= 0.667\n",
      "epoch= 32   train_loss= 1.884   train_acc= 0.988   test_loss=2.718   test_acc= 0.667\n",
      "epoch= 33   train_loss= 1.876   train_acc= 1.000   test_loss=2.774   test_acc= 0.667\n",
      "epoch= 34   train_loss= 1.860   train_acc= 1.000   test_loss=2.907   test_acc= 0.667\n",
      "epoch= 35   train_loss= 1.845   train_acc= 1.000   test_loss=2.905   test_acc= 0.667\n",
      "epoch= 36   train_loss= 1.835   train_acc= 1.000   test_loss=2.945   test_acc= 0.667\n",
      "epoch= 37   train_loss= 1.825   train_acc= 1.000   test_loss=2.843   test_acc= 0.667\n",
      "epoch= 38   train_loss= 1.824   train_acc= 1.000   test_loss=3.055   test_acc= 0.667\n",
      "epoch= 39   train_loss= 1.804   train_acc= 1.000   test_loss=2.873   test_acc= 0.667\n",
      "epoch= 40   train_loss= 1.790   train_acc= 1.000   test_loss=2.870   test_acc= 0.667\n",
      "epoch= 41   train_loss= 1.777   train_acc= 1.000   test_loss=2.862   test_acc= 0.667\n",
      "epoch= 42   train_loss= 1.778   train_acc= 1.000   test_loss=2.922   test_acc= 0.667\n",
      "epoch= 43   train_loss= 1.757   train_acc= 1.000   test_loss=2.865   test_acc= 0.667\n",
      "epoch= 44   train_loss= 1.750   train_acc= 1.000   test_loss=2.842   test_acc= 0.667\n",
      "epoch= 45   train_loss= 1.740   train_acc= 1.000   test_loss=2.845   test_acc= 0.667\n",
      "epoch= 46   train_loss= 1.726   train_acc= 1.000   test_loss=2.904   test_acc= 0.667\n",
      "epoch= 47   train_loss= 1.720   train_acc= 1.000   test_loss=2.867   test_acc= 0.667\n",
      "epoch= 48   train_loss= 1.718   train_acc= 1.000   test_loss=2.917   test_acc= 0.667\n",
      "epoch= 49   train_loss= 1.701   train_acc= 1.000   test_loss=2.928   test_acc= 0.667\n",
      "run time: 0.7140230019887288 min\n",
      "test_acc=0.667\n",
      "run= 1   fold= 9\n",
      "epoch= 0   train_loss= 3.054   train_acc= 0.554   test_loss=2.728   test_acc= 1.000\n",
      "epoch= 1   train_loss= 2.812   train_acc= 0.771   test_loss=2.619   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.685   train_acc= 0.843   test_loss=2.535   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.626   train_acc= 0.831   test_loss=2.516   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.534   train_acc= 0.867   test_loss=2.442   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.479   train_acc= 0.892   test_loss=2.428   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.418   train_acc= 0.940   test_loss=2.405   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.441   train_acc= 0.916   test_loss=2.373   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.345   train_acc= 0.952   test_loss=2.337   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.340   train_acc= 0.916   test_loss=2.320   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.306   train_acc= 0.940   test_loss=2.311   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.271   train_acc= 0.952   test_loss=2.294   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.231   train_acc= 0.964   test_loss=2.277   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.222   train_acc= 0.964   test_loss=2.264   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.181   train_acc= 0.976   test_loss=2.274   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.161   train_acc= 0.988   test_loss=2.278   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.138   train_acc= 0.988   test_loss=2.250   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.124   train_acc= 0.976   test_loss=2.178   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.090   train_acc= 1.000   test_loss=2.173   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.062   train_acc= 1.000   test_loss=2.142   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.061   train_acc= 0.988   test_loss=2.103   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.038   train_acc= 0.988   test_loss=2.104   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.050   train_acc= 0.976   test_loss=2.120   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.010   train_acc= 0.988   test_loss=2.118   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.996   train_acc= 1.000   test_loss=2.106   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.976   train_acc= 1.000   test_loss=2.101   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.965   train_acc= 1.000   test_loss=2.095   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.941   train_acc= 1.000   test_loss=2.082   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.930   train_acc= 1.000   test_loss=2.058   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.912   train_acc= 1.000   test_loss=2.054   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.901   train_acc= 1.000   test_loss=2.031   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.894   train_acc= 1.000   test_loss=2.027   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.888   train_acc= 1.000   test_loss=1.988   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.870   train_acc= 1.000   test_loss=1.978   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.858   train_acc= 1.000   test_loss=1.983   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.845   train_acc= 1.000   test_loss=1.970   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.834   train_acc= 1.000   test_loss=1.943   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.828   train_acc= 1.000   test_loss=1.966   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.809   train_acc= 1.000   test_loss=1.957   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.802   train_acc= 1.000   test_loss=1.937   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.790   train_acc= 1.000   test_loss=1.919   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.785   train_acc= 1.000   test_loss=1.928   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.768   train_acc= 1.000   test_loss=1.897   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.758   train_acc= 1.000   test_loss=1.873   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.746   train_acc= 1.000   test_loss=1.876   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.733   train_acc= 1.000   test_loss=1.870   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 46   train_loss= 1.727   train_acc= 1.000   test_loss=1.860   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.719   train_acc= 1.000   test_loss=1.839   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.702   train_acc= 1.000   test_loss=1.826   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.701   train_acc= 1.000   test_loss=1.820   test_acc= 0.889\n",
      "run time: 0.7510839025179545 min\n",
      "test_acc=0.889\n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-211     \n",
      "MUSK-211     \n",
      "MUSK-212     \n",
      "MUSK-212     \n",
      "MUSK-212     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-219     \n",
      "MUSK-219     \n",
      "MUSK-224     \n",
      "MUSK-224     \n",
      "MUSK-227     \n",
      "MUSK-227     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-254     \n",
      "MUSK-254     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-272     \n",
      "MUSK-272     \n",
      "MUSK-272     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-311     \n",
      "MUSK-311     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-321     \n",
      "MUSK-321     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-344     \n",
      "MUSK-344     \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f159    \n",
      "MUSK-f159    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-j33     \n",
      "MUSK-j33     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-jf17    \n",
      "MUSK-jf17    \n",
      "MUSK-jf17    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-208 \n",
      "NON-MUSK-208 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-226 \n",
      "NON-MUSK-226 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-247 \n",
      "NON-MUSK-247 \n",
      "NON-MUSK-249 \n",
      "NON-MUSK-249 \n",
      "NON-MUSK-253 \n",
      "NON-MUSK-253 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-271 \n",
      "NON-MUSK-271 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-290 \n",
      "NON-MUSK-290 \n",
      "NON-MUSK-295 \n",
      "NON-MUSK-295 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-305 \n",
      "NON-MUSK-305 \n",
      "NON-MUSK-308 \n",
      "NON-MUSK-308 \n",
      "NON-MUSK-309 \n",
      "NON-MUSK-309 \n",
      "NON-MUSK-318 \n",
      "NON-MUSK-318 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-320 \n",
      "NON-MUSK-320 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-334 \n",
      "NON-MUSK-334 \n",
      "NON-MUSK-f150\n",
      "NON-MUSK-f150\n",
      "NON-MUSK-f161\n",
      "NON-MUSK-f161\n",
      "NON-MUSK-f164\n",
      "NON-MUSK-f164\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j130\n",
      "NON-MUSK-j130\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j148\n",
      "NON-MUSK-j148\n",
      "NON-MUSK-j81 \n",
      "NON-MUSK-j81 \n",
      "NON-MUSK-j83 \n",
      "NON-MUSK-j83 \n",
      "NON-MUSK-j84 \n",
      "NON-MUSK-j84 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "run= 2   fold= 0\n",
      "epoch= 0   train_loss= 2.880   train_acc= 0.744   test_loss=3.003   test_acc= 0.400\n",
      "epoch= 1   train_loss= 2.779   train_acc= 0.780   test_loss=2.828   test_acc= 0.700\n",
      "epoch= 2   train_loss= 2.682   train_acc= 0.817   test_loss=2.756   test_acc= 0.800\n",
      "epoch= 3   train_loss= 2.542   train_acc= 0.878   test_loss=2.711   test_acc= 0.800\n",
      "epoch= 4   train_loss= 2.511   train_acc= 0.915   test_loss=2.672   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.471   train_acc= 0.890   test_loss=2.689   test_acc= 0.700\n",
      "epoch= 6   train_loss= 2.428   train_acc= 0.927   test_loss=2.631   test_acc= 0.700\n",
      "epoch= 7   train_loss= 2.384   train_acc= 0.927   test_loss=2.578   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.374   train_acc= 0.890   test_loss=2.591   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.342   train_acc= 0.927   test_loss=2.581   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.298   train_acc= 0.951   test_loss=2.547   test_acc= 0.800\n",
      "epoch= 11   train_loss= 2.259   train_acc= 0.976   test_loss=2.493   test_acc= 0.800\n",
      "epoch= 12   train_loss= 2.262   train_acc= 0.963   test_loss=2.499   test_acc= 0.800\n",
      "epoch= 13   train_loss= 2.210   train_acc= 0.976   test_loss=2.470   test_acc= 0.800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 14   train_loss= 2.185   train_acc= 0.988   test_loss=2.416   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.162   train_acc= 0.976   test_loss=2.437   test_acc= 0.800\n",
      "epoch= 16   train_loss= 2.176   train_acc= 0.951   test_loss=2.446   test_acc= 0.800\n",
      "epoch= 17   train_loss= 2.132   train_acc= 0.988   test_loss=2.364   test_acc= 0.800\n",
      "epoch= 18   train_loss= 2.106   train_acc= 0.988   test_loss=2.342   test_acc= 0.800\n",
      "epoch= 19   train_loss= 2.096   train_acc= 1.000   test_loss=2.393   test_acc= 0.800\n",
      "epoch= 20   train_loss= 2.060   train_acc= 1.000   test_loss=2.354   test_acc= 0.800\n",
      "epoch= 21   train_loss= 2.046   train_acc= 0.988   test_loss=2.327   test_acc= 0.800\n",
      "epoch= 22   train_loss= 2.054   train_acc= 0.976   test_loss=2.312   test_acc= 0.800\n",
      "epoch= 23   train_loss= 2.029   train_acc= 1.000   test_loss=2.321   test_acc= 0.800\n",
      "epoch= 24   train_loss= 2.003   train_acc= 1.000   test_loss=2.302   test_acc= 0.800\n",
      "epoch= 25   train_loss= 1.981   train_acc= 1.000   test_loss=2.251   test_acc= 0.800\n",
      "epoch= 26   train_loss= 1.978   train_acc= 1.000   test_loss=2.184   test_acc= 0.800\n",
      "epoch= 27   train_loss= 1.958   train_acc= 1.000   test_loss=2.225   test_acc= 0.800\n",
      "epoch= 28   train_loss= 1.938   train_acc= 1.000   test_loss=2.224   test_acc= 0.800\n",
      "epoch= 29   train_loss= 1.923   train_acc= 1.000   test_loss=2.180   test_acc= 0.800\n",
      "epoch= 30   train_loss= 1.925   train_acc= 1.000   test_loss=2.158   test_acc= 0.800\n",
      "epoch= 31   train_loss= 1.904   train_acc= 1.000   test_loss=2.139   test_acc= 0.800\n",
      "epoch= 32   train_loss= 1.897   train_acc= 1.000   test_loss=2.195   test_acc= 0.800\n",
      "epoch= 33   train_loss= 1.890   train_acc= 0.988   test_loss=2.076   test_acc= 0.800\n",
      "epoch= 34   train_loss= 1.876   train_acc= 1.000   test_loss=2.129   test_acc= 0.800\n",
      "epoch= 35   train_loss= 1.853   train_acc= 1.000   test_loss=2.134   test_acc= 0.800\n",
      "epoch= 36   train_loss= 1.841   train_acc= 1.000   test_loss=2.130   test_acc= 0.800\n",
      "epoch= 37   train_loss= 1.826   train_acc= 1.000   test_loss=2.108   test_acc= 0.800\n",
      "epoch= 38   train_loss= 1.824   train_acc= 1.000   test_loss=2.164   test_acc= 0.800\n",
      "epoch= 39   train_loss= 1.806   train_acc= 1.000   test_loss=2.112   test_acc= 0.800\n",
      "epoch= 40   train_loss= 1.806   train_acc= 1.000   test_loss=2.081   test_acc= 0.800\n",
      "epoch= 41   train_loss= 1.780   train_acc= 1.000   test_loss=2.101   test_acc= 0.800\n",
      "epoch= 42   train_loss= 1.798   train_acc= 1.000   test_loss=2.168   test_acc= 0.800\n",
      "epoch= 43   train_loss= 1.765   train_acc= 1.000   test_loss=2.135   test_acc= 0.800\n",
      "epoch= 44   train_loss= 1.750   train_acc= 1.000   test_loss=2.136   test_acc= 0.800\n",
      "epoch= 45   train_loss= 1.743   train_acc= 1.000   test_loss=2.119   test_acc= 0.800\n",
      "epoch= 46   train_loss= 1.733   train_acc= 1.000   test_loss=2.065   test_acc= 0.800\n",
      "epoch= 47   train_loss= 1.722   train_acc= 1.000   test_loss=2.022   test_acc= 0.800\n",
      "epoch= 48   train_loss= 1.715   train_acc= 1.000   test_loss=2.051   test_acc= 0.800\n",
      "epoch= 49   train_loss= 1.703   train_acc= 1.000   test_loss=2.006   test_acc= 0.800\n",
      "run time: 0.7527872363726298 min\n",
      "test_acc=0.800\n",
      "run= 2   fold= 1\n",
      "epoch= 0   train_loss= 2.996   train_acc= 0.537   test_loss=2.898   test_acc= 0.600\n",
      "epoch= 1   train_loss= 2.853   train_acc= 0.683   test_loss=2.786   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.747   train_acc= 0.768   test_loss=2.658   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.667   train_acc= 0.829   test_loss=2.575   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.631   train_acc= 0.829   test_loss=2.523   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.545   train_acc= 0.890   test_loss=2.477   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.481   train_acc= 0.902   test_loss=2.436   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.451   train_acc= 0.890   test_loss=2.458   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.386   train_acc= 0.927   test_loss=2.423   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.361   train_acc= 0.927   test_loss=2.391   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.348   train_acc= 0.902   test_loss=2.317   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.312   train_acc= 0.939   test_loss=2.297   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.281   train_acc= 0.927   test_loss=2.293   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.272   train_acc= 0.951   test_loss=2.272   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.180   train_acc= 1.000   test_loss=2.231   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.157   train_acc= 0.988   test_loss=2.227   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.144   train_acc= 1.000   test_loss=2.237   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.152   train_acc= 0.976   test_loss=2.223   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.107   train_acc= 0.988   test_loss=2.187   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.082   train_acc= 1.000   test_loss=2.171   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.075   train_acc= 0.976   test_loss=2.197   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.061   train_acc= 0.988   test_loss=2.150   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.042   train_acc= 1.000   test_loss=2.151   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.019   train_acc= 1.000   test_loss=2.133   test_acc= 0.900\n",
      "epoch= 24   train_loss= 1.993   train_acc= 1.000   test_loss=2.103   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.994   train_acc= 1.000   test_loss=2.056   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.979   train_acc= 1.000   test_loss=2.058   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.961   train_acc= 1.000   test_loss=2.027   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.941   train_acc= 1.000   test_loss=2.050   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.925   train_acc= 1.000   test_loss=2.031   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.938   train_acc= 0.988   test_loss=2.020   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.909   train_acc= 1.000   test_loss=1.989   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.886   train_acc= 1.000   test_loss=1.979   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.875   train_acc= 1.000   test_loss=1.964   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.863   train_acc= 1.000   test_loss=1.961   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.846   train_acc= 1.000   test_loss=1.943   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.844   train_acc= 1.000   test_loss=1.928   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.833   train_acc= 1.000   test_loss=1.924   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.817   train_acc= 1.000   test_loss=1.910   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.804   train_acc= 1.000   test_loss=1.885   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.799   train_acc= 1.000   test_loss=1.891   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.787   train_acc= 1.000   test_loss=1.870   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.774   train_acc= 1.000   test_loss=1.857   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.762   train_acc= 1.000   test_loss=1.869   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.750   train_acc= 1.000   test_loss=1.867   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.744   train_acc= 1.000   test_loss=1.846   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.738   train_acc= 1.000   test_loss=1.856   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.727   train_acc= 1.000   test_loss=1.845   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.718   train_acc= 1.000   test_loss=1.840   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.709   train_acc= 1.000   test_loss=1.823   test_acc= 0.900\n",
      "run time: 0.7212483167648316 min\n",
      "test_acc=0.900\n",
      "run= 2   fold= 2\n",
      "epoch= 0   train_loss= 3.011   train_acc= 0.518   test_loss=2.866   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.894   train_acc= 0.711   test_loss=2.923   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.790   train_acc= 0.723   test_loss=2.825   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.671   train_acc= 0.855   test_loss=2.758   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.603   train_acc= 0.855   test_loss=2.660   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.524   train_acc= 0.880   test_loss=2.630   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.447   train_acc= 0.892   test_loss=2.495   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.468   train_acc= 0.843   test_loss=2.563   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.435   train_acc= 0.904   test_loss=2.542   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.385   train_acc= 0.928   test_loss=2.411   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 10   train_loss= 2.314   train_acc= 0.940   test_loss=2.432   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.304   train_acc= 0.928   test_loss=2.452   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.285   train_acc= 0.964   test_loss=2.276   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.273   train_acc= 0.940   test_loss=2.422   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.232   train_acc= 0.952   test_loss=2.380   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.204   train_acc= 0.988   test_loss=2.254   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.167   train_acc= 0.976   test_loss=2.231   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.131   train_acc= 0.988   test_loss=2.248   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.124   train_acc= 0.976   test_loss=2.183   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.078   train_acc= 0.988   test_loss=2.280   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.076   train_acc= 0.988   test_loss=2.184   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.050   train_acc= 0.988   test_loss=2.203   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.050   train_acc= 1.000   test_loss=2.156   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.019   train_acc= 1.000   test_loss=2.104   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.004   train_acc= 1.000   test_loss=2.076   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.984   train_acc= 1.000   test_loss=2.027   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.977   train_acc= 1.000   test_loss=2.062   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.950   train_acc= 1.000   test_loss=2.037   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.938   train_acc= 1.000   test_loss=2.013   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.925   train_acc= 1.000   test_loss=1.983   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.911   train_acc= 1.000   test_loss=1.996   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.909   train_acc= 1.000   test_loss=1.951   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.904   train_acc= 1.000   test_loss=1.944   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.876   train_acc= 1.000   test_loss=1.931   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.867   train_acc= 1.000   test_loss=1.934   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.842   train_acc= 1.000   test_loss=1.907   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.840   train_acc= 1.000   test_loss=1.881   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.822   train_acc= 1.000   test_loss=1.872   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.815   train_acc= 1.000   test_loss=1.865   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.801   train_acc= 1.000   test_loss=1.854   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.807   train_acc= 1.000   test_loss=1.876   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.786   train_acc= 1.000   test_loss=1.842   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.768   train_acc= 1.000   test_loss=1.846   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.769   train_acc= 1.000   test_loss=1.807   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.747   train_acc= 1.000   test_loss=1.796   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.746   train_acc= 1.000   test_loss=1.787   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.732   train_acc= 1.000   test_loss=1.794   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.731   train_acc= 1.000   test_loss=1.751   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.708   train_acc= 1.000   test_loss=1.745   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.704   train_acc= 1.000   test_loss=1.744   test_acc= 1.000\n",
      "run time: 0.705838966369629 min\n",
      "test_acc=1.000\n",
      "run= 2   fold= 3\n",
      "epoch= 0   train_loss= 2.902   train_acc= 0.711   test_loss=2.739   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.758   train_acc= 0.807   test_loss=2.667   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.688   train_acc= 0.783   test_loss=2.590   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.648   train_acc= 0.807   test_loss=2.521   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.558   train_acc= 0.831   test_loss=2.508   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.495   train_acc= 0.904   test_loss=2.451   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.457   train_acc= 0.904   test_loss=2.420   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.399   train_acc= 0.940   test_loss=2.381   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.369   train_acc= 0.928   test_loss=2.393   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.333   train_acc= 0.916   test_loss=2.318   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.328   train_acc= 0.952   test_loss=2.366   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.303   train_acc= 0.928   test_loss=2.359   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.244   train_acc= 0.964   test_loss=2.352   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.235   train_acc= 0.964   test_loss=2.320   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.199   train_acc= 0.964   test_loss=2.310   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.195   train_acc= 0.976   test_loss=2.308   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.158   train_acc= 0.964   test_loss=2.287   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.112   train_acc= 1.000   test_loss=2.292   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.100   train_acc= 1.000   test_loss=2.298   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.083   train_acc= 0.988   test_loss=2.213   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.058   train_acc= 1.000   test_loss=2.234   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.062   train_acc= 0.988   test_loss=2.249   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.028   train_acc= 1.000   test_loss=2.232   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.029   train_acc= 0.988   test_loss=2.272   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.008   train_acc= 1.000   test_loss=2.275   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.992   train_acc= 1.000   test_loss=2.282   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.970   train_acc= 1.000   test_loss=2.202   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.956   train_acc= 1.000   test_loss=2.211   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.949   train_acc= 1.000   test_loss=2.216   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.931   train_acc= 1.000   test_loss=2.242   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.921   train_acc= 1.000   test_loss=2.199   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.907   train_acc= 1.000   test_loss=2.204   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.881   train_acc= 1.000   test_loss=2.161   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.871   train_acc= 1.000   test_loss=2.175   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.871   train_acc= 0.988   test_loss=2.177   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.847   train_acc= 1.000   test_loss=2.176   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.833   train_acc= 1.000   test_loss=2.162   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.830   train_acc= 1.000   test_loss=2.167   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.826   train_acc= 1.000   test_loss=2.134   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.815   train_acc= 1.000   test_loss=2.127   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.797   train_acc= 1.000   test_loss=2.113   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.780   train_acc= 1.000   test_loss=2.130   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.782   train_acc= 1.000   test_loss=2.106   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.762   train_acc= 1.000   test_loss=2.092   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.757   train_acc= 1.000   test_loss=2.106   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.751   train_acc= 1.000   test_loss=2.087   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.738   train_acc= 1.000   test_loss=2.063   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.725   train_acc= 0.988   test_loss=2.076   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.715   train_acc= 1.000   test_loss=2.105   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.702   train_acc= 1.000   test_loss=2.097   test_acc= 0.889\n",
      "run time: 0.7704657475153606 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 4\n",
      "epoch= 0   train_loss= 2.956   train_acc= 0.578   test_loss=2.857   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.817   train_acc= 0.735   test_loss=2.786   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.739   train_acc= 0.771   test_loss=2.680   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.657   train_acc= 0.831   test_loss=2.629   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.613   train_acc= 0.843   test_loss=2.605   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.556   train_acc= 0.843   test_loss=2.553   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 6   train_loss= 2.492   train_acc= 0.916   test_loss=2.532   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.495   train_acc= 0.855   test_loss=2.493   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.400   train_acc= 0.916   test_loss=2.439   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.384   train_acc= 0.928   test_loss=2.439   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.335   train_acc= 0.928   test_loss=2.394   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.305   train_acc= 0.904   test_loss=2.370   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.300   train_acc= 0.916   test_loss=2.401   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.258   train_acc= 0.916   test_loss=2.325   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.214   train_acc= 0.988   test_loss=2.317   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.180   train_acc= 0.964   test_loss=2.300   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.189   train_acc= 0.928   test_loss=2.345   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.158   train_acc= 0.952   test_loss=2.332   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.144   train_acc= 0.976   test_loss=2.350   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.105   train_acc= 0.976   test_loss=2.295   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.065   train_acc= 1.000   test_loss=2.292   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.054   train_acc= 1.000   test_loss=2.311   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.037   train_acc= 1.000   test_loss=2.256   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.011   train_acc= 1.000   test_loss=2.241   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.009   train_acc= 0.988   test_loss=2.250   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.997   train_acc= 1.000   test_loss=2.220   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.975   train_acc= 1.000   test_loss=2.240   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.965   train_acc= 1.000   test_loss=2.208   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.936   train_acc= 1.000   test_loss=2.216   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.943   train_acc= 0.988   test_loss=2.186   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.916   train_acc= 1.000   test_loss=2.222   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.898   train_acc= 1.000   test_loss=2.160   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.879   train_acc= 1.000   test_loss=2.173   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.869   train_acc= 1.000   test_loss=2.184   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.879   train_acc= 1.000   test_loss=2.226   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.847   train_acc= 1.000   test_loss=2.226   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.829   train_acc= 1.000   test_loss=2.193   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.826   train_acc= 1.000   test_loss=2.182   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.807   train_acc= 1.000   test_loss=2.179   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.801   train_acc= 1.000   test_loss=2.164   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.788   train_acc= 1.000   test_loss=2.191   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.783   train_acc= 1.000   test_loss=2.122   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.769   train_acc= 1.000   test_loss=2.130   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.766   train_acc= 1.000   test_loss=2.154   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.747   train_acc= 1.000   test_loss=2.141   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.732   train_acc= 1.000   test_loss=2.128   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.725   train_acc= 1.000   test_loss=2.155   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.718   train_acc= 1.000   test_loss=2.139   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.705   train_acc= 1.000   test_loss=2.135   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.701   train_acc= 1.000   test_loss=2.102   test_acc= 0.889\n",
      "run time: 0.9660339633623759 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 5\n",
      "epoch= 0   train_loss= 3.031   train_acc= 0.542   test_loss=2.923   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.833   train_acc= 0.723   test_loss=2.808   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.721   train_acc= 0.807   test_loss=2.723   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.634   train_acc= 0.855   test_loss=2.693   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.540   train_acc= 0.880   test_loss=2.692   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.529   train_acc= 0.855   test_loss=2.724   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.473   train_acc= 0.892   test_loss=2.782   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.463   train_acc= 0.892   test_loss=2.706   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.394   train_acc= 0.928   test_loss=2.712   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.363   train_acc= 0.916   test_loss=2.774   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.330   train_acc= 0.940   test_loss=2.740   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.310   train_acc= 0.904   test_loss=2.705   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.250   train_acc= 0.988   test_loss=2.738   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.209   train_acc= 0.976   test_loss=2.751   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.200   train_acc= 0.964   test_loss=2.722   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.182   train_acc= 0.976   test_loss=2.712   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.141   train_acc= 0.988   test_loss=2.778   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.116   train_acc= 1.000   test_loss=2.821   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.098   train_acc= 0.976   test_loss=2.822   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.077   train_acc= 1.000   test_loss=2.830   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.051   train_acc= 0.988   test_loss=2.832   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.038   train_acc= 0.988   test_loss=2.872   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.020   train_acc= 1.000   test_loss=2.922   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.995   train_acc= 1.000   test_loss=2.897   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.989   train_acc= 1.000   test_loss=2.885   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.979   train_acc= 1.000   test_loss=2.913   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.955   train_acc= 1.000   test_loss=2.930   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.950   train_acc= 0.988   test_loss=2.901   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.930   train_acc= 1.000   test_loss=2.899   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.920   train_acc= 1.000   test_loss=2.900   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.903   train_acc= 1.000   test_loss=2.899   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.886   train_acc= 1.000   test_loss=2.928   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.875   train_acc= 1.000   test_loss=2.908   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.866   train_acc= 1.000   test_loss=2.927   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.852   train_acc= 1.000   test_loss=2.909   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.848   train_acc= 0.988   test_loss=2.830   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.834   train_acc= 1.000   test_loss=2.859   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.821   train_acc= 1.000   test_loss=2.856   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.807   train_acc= 1.000   test_loss=2.880   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.797   train_acc= 1.000   test_loss=2.843   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.787   train_acc= 1.000   test_loss=2.869   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.772   train_acc= 1.000   test_loss=2.881   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.772   train_acc= 1.000   test_loss=2.899   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.750   train_acc= 1.000   test_loss=2.890   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.740   train_acc= 1.000   test_loss=2.900   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.738   train_acc= 1.000   test_loss=2.871   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.721   train_acc= 1.000   test_loss=2.889   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.717   train_acc= 1.000   test_loss=2.872   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.707   train_acc= 1.000   test_loss=2.870   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.697   train_acc= 1.000   test_loss=2.864   test_acc= 0.778\n",
      "run time: 0.7112614552179972 min\n",
      "test_acc=0.778\n",
      "run= 2   fold= 6\n",
      "epoch= 0   train_loss= 3.025   train_acc= 0.578   test_loss=2.938   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.825   train_acc= 0.747   test_loss=2.857   test_acc= 0.556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2   train_loss= 2.714   train_acc= 0.819   test_loss=2.747   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.641   train_acc= 0.831   test_loss=2.730   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.620   train_acc= 0.843   test_loss=2.707   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.530   train_acc= 0.880   test_loss=2.662   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.487   train_acc= 0.892   test_loss=2.677   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.434   train_acc= 0.916   test_loss=2.646   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.397   train_acc= 0.952   test_loss=2.646   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.371   train_acc= 0.940   test_loss=2.606   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.354   train_acc= 0.952   test_loss=2.586   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.316   train_acc= 0.940   test_loss=2.547   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.262   train_acc= 0.952   test_loss=2.547   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.246   train_acc= 0.940   test_loss=2.548   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.233   train_acc= 0.928   test_loss=2.524   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.172   train_acc= 0.988   test_loss=2.565   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.171   train_acc= 0.964   test_loss=2.534   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.155   train_acc= 0.976   test_loss=2.506   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.130   train_acc= 0.988   test_loss=2.535   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.110   train_acc= 0.988   test_loss=2.491   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.077   train_acc= 0.988   test_loss=2.542   test_acc= 0.667\n",
      "epoch= 21   train_loss= 2.059   train_acc= 1.000   test_loss=2.526   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.055   train_acc= 0.988   test_loss=2.531   test_acc= 0.667\n",
      "epoch= 23   train_loss= 2.019   train_acc= 1.000   test_loss=2.460   test_acc= 0.778\n",
      "epoch= 24   train_loss= 2.003   train_acc= 1.000   test_loss=2.445   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.995   train_acc= 0.988   test_loss=2.556   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.977   train_acc= 1.000   test_loss=2.514   test_acc= 0.667\n",
      "epoch= 27   train_loss= 1.971   train_acc= 1.000   test_loss=2.494   test_acc= 0.667\n",
      "epoch= 28   train_loss= 1.949   train_acc= 1.000   test_loss=2.468   test_acc= 0.667\n",
      "epoch= 29   train_loss= 1.935   train_acc= 1.000   test_loss=2.501   test_acc= 0.667\n",
      "epoch= 30   train_loss= 1.918   train_acc= 1.000   test_loss=2.526   test_acc= 0.667\n",
      "epoch= 31   train_loss= 1.909   train_acc= 1.000   test_loss=2.539   test_acc= 0.667\n",
      "epoch= 32   train_loss= 1.896   train_acc= 1.000   test_loss=2.541   test_acc= 0.667\n",
      "epoch= 33   train_loss= 1.888   train_acc= 1.000   test_loss=2.492   test_acc= 0.667\n",
      "epoch= 34   train_loss= 1.873   train_acc= 1.000   test_loss=2.523   test_acc= 0.667\n",
      "epoch= 35   train_loss= 1.846   train_acc= 1.000   test_loss=2.492   test_acc= 0.667\n",
      "epoch= 36   train_loss= 1.844   train_acc= 1.000   test_loss=2.477   test_acc= 0.667\n",
      "epoch= 37   train_loss= 1.832   train_acc= 1.000   test_loss=2.471   test_acc= 0.667\n",
      "epoch= 38   train_loss= 1.829   train_acc= 1.000   test_loss=2.434   test_acc= 0.667\n",
      "epoch= 39   train_loss= 1.812   train_acc= 1.000   test_loss=2.377   test_acc= 0.667\n",
      "epoch= 40   train_loss= 1.793   train_acc= 1.000   test_loss=2.417   test_acc= 0.667\n",
      "epoch= 41   train_loss= 1.793   train_acc= 1.000   test_loss=2.428   test_acc= 0.667\n",
      "epoch= 42   train_loss= 1.779   train_acc= 1.000   test_loss=2.363   test_acc= 0.667\n",
      "epoch= 43   train_loss= 1.768   train_acc= 1.000   test_loss=2.350   test_acc= 0.667\n",
      "epoch= 44   train_loss= 1.752   train_acc= 1.000   test_loss=2.415   test_acc= 0.667\n",
      "epoch= 45   train_loss= 1.752   train_acc= 1.000   test_loss=2.347   test_acc= 0.667\n",
      "epoch= 46   train_loss= 1.735   train_acc= 1.000   test_loss=2.370   test_acc= 0.667\n",
      "epoch= 47   train_loss= 1.726   train_acc= 1.000   test_loss=2.356   test_acc= 0.667\n",
      "epoch= 48   train_loss= 1.717   train_acc= 1.000   test_loss=2.341   test_acc= 0.667\n",
      "epoch= 49   train_loss= 1.708   train_acc= 1.000   test_loss=2.348   test_acc= 0.667\n",
      "run time: 0.72613951365153 min\n",
      "test_acc=0.667\n",
      "run= 2   fold= 7\n",
      "epoch= 0   train_loss= 3.041   train_acc= 0.602   test_loss=2.784   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.778   train_acc= 0.759   test_loss=2.771   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.695   train_acc= 0.795   test_loss=2.746   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.625   train_acc= 0.783   test_loss=2.686   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.540   train_acc= 0.867   test_loss=2.645   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.471   train_acc= 0.892   test_loss=2.584   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.451   train_acc= 0.880   test_loss=2.590   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.393   train_acc= 0.940   test_loss=2.671   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.374   train_acc= 0.928   test_loss=2.432   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.341   train_acc= 0.952   test_loss=2.473   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.287   train_acc= 0.952   test_loss=2.483   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.237   train_acc= 0.988   test_loss=2.510   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.210   train_acc= 0.964   test_loss=2.448   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.173   train_acc= 0.988   test_loss=2.455   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.148   train_acc= 1.000   test_loss=2.547   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.158   train_acc= 0.964   test_loss=2.625   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.115   train_acc= 1.000   test_loss=2.450   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.096   train_acc= 1.000   test_loss=2.469   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.080   train_acc= 1.000   test_loss=2.396   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.060   train_acc= 0.988   test_loss=2.354   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.036   train_acc= 1.000   test_loss=2.530   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.032   train_acc= 1.000   test_loss=2.389   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.009   train_acc= 1.000   test_loss=2.370   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.994   train_acc= 1.000   test_loss=2.375   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.983   train_acc= 0.988   test_loss=2.441   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.964   train_acc= 1.000   test_loss=2.516   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.951   train_acc= 1.000   test_loss=2.469   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.933   train_acc= 1.000   test_loss=2.374   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.916   train_acc= 1.000   test_loss=2.387   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.905   train_acc= 1.000   test_loss=2.417   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.896   train_acc= 1.000   test_loss=2.409   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.883   train_acc= 1.000   test_loss=2.450   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.869   train_acc= 1.000   test_loss=2.333   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.861   train_acc= 0.988   test_loss=2.437   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.845   train_acc= 1.000   test_loss=2.486   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.838   train_acc= 1.000   test_loss=2.469   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.820   train_acc= 1.000   test_loss=2.386   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.805   train_acc= 1.000   test_loss=2.375   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.790   train_acc= 1.000   test_loss=2.421   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.787   train_acc= 1.000   test_loss=2.369   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.776   train_acc= 1.000   test_loss=2.368   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.764   train_acc= 1.000   test_loss=2.342   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.746   train_acc= 1.000   test_loss=2.258   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.740   train_acc= 1.000   test_loss=2.313   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.733   train_acc= 1.000   test_loss=2.276   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.723   train_acc= 1.000   test_loss=2.373   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.711   train_acc= 1.000   test_loss=2.280   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.700   train_acc= 1.000   test_loss=2.270   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 48   train_loss= 1.697   train_acc= 1.000   test_loss=2.287   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.684   train_acc= 1.000   test_loss=2.285   test_acc= 0.778\n",
      "run time: 0.756281320254008 min\n",
      "test_acc=0.778\n",
      "run= 2   fold= 8\n",
      "epoch= 0   train_loss= 2.909   train_acc= 0.687   test_loss=2.735   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.803   train_acc= 0.771   test_loss=2.616   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.685   train_acc= 0.819   test_loss=2.572   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.583   train_acc= 0.867   test_loss=2.544   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.558   train_acc= 0.855   test_loss=2.549   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.516   train_acc= 0.892   test_loss=2.549   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.433   train_acc= 0.904   test_loss=2.527   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.389   train_acc= 0.904   test_loss=2.517   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.373   train_acc= 0.916   test_loss=2.564   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.356   train_acc= 0.916   test_loss=2.544   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.280   train_acc= 0.976   test_loss=2.558   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.241   train_acc= 0.988   test_loss=2.574   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.211   train_acc= 0.988   test_loss=2.648   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.201   train_acc= 0.988   test_loss=2.646   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.172   train_acc= 0.976   test_loss=2.666   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.152   train_acc= 1.000   test_loss=2.665   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.132   train_acc= 1.000   test_loss=2.654   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.110   train_acc= 0.976   test_loss=2.646   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.078   train_acc= 1.000   test_loss=2.648   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.077   train_acc= 0.988   test_loss=2.665   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.048   train_acc= 1.000   test_loss=2.683   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.044   train_acc= 0.988   test_loss=2.693   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.014   train_acc= 1.000   test_loss=2.700   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.998   train_acc= 1.000   test_loss=2.697   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.983   train_acc= 1.000   test_loss=2.698   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.978   train_acc= 1.000   test_loss=2.680   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.959   train_acc= 1.000   test_loss=2.679   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.946   train_acc= 1.000   test_loss=2.679   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.935   train_acc= 1.000   test_loss=2.655   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.912   train_acc= 1.000   test_loss=2.638   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.912   train_acc= 1.000   test_loss=2.677   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.899   train_acc= 1.000   test_loss=2.668   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.875   train_acc= 1.000   test_loss=2.669   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.872   train_acc= 1.000   test_loss=2.643   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.855   train_acc= 1.000   test_loss=2.651   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.841   train_acc= 1.000   test_loss=2.648   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.828   train_acc= 1.000   test_loss=2.656   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.820   train_acc= 1.000   test_loss=2.660   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.808   train_acc= 1.000   test_loss=2.639   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.802   train_acc= 1.000   test_loss=2.646   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.785   train_acc= 1.000   test_loss=2.624   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.776   train_acc= 1.000   test_loss=2.605   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.767   train_acc= 1.000   test_loss=2.604   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.751   train_acc= 1.000   test_loss=2.597   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.739   train_acc= 1.000   test_loss=2.591   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.742   train_acc= 1.000   test_loss=2.571   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.723   train_acc= 1.000   test_loss=2.569   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.714   train_acc= 1.000   test_loss=2.567   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.703   train_acc= 1.000   test_loss=2.575   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.693   train_acc= 1.000   test_loss=2.570   test_acc= 0.778\n",
      "run time: 0.7927567998568217 min\n",
      "test_acc=0.778\n",
      "run= 2   fold= 9\n",
      "epoch= 0   train_loss= 2.969   train_acc= 0.590   test_loss=2.971   test_acc= 0.333\n",
      "epoch= 1   train_loss= 2.769   train_acc= 0.735   test_loss=2.994   test_acc= 0.444\n",
      "epoch= 2   train_loss= 2.696   train_acc= 0.831   test_loss=2.928   test_acc= 0.556\n",
      "epoch= 3   train_loss= 2.595   train_acc= 0.880   test_loss=2.851   test_acc= 0.556\n",
      "epoch= 4   train_loss= 2.538   train_acc= 0.892   test_loss=2.897   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.486   train_acc= 0.904   test_loss=3.021   test_acc= 0.667\n",
      "epoch= 6   train_loss= 2.472   train_acc= 0.892   test_loss=2.895   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.396   train_acc= 0.940   test_loss=3.081   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.359   train_acc= 0.892   test_loss=3.009   test_acc= 0.667\n",
      "epoch= 9   train_loss= 2.317   train_acc= 0.952   test_loss=3.051   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.276   train_acc= 0.988   test_loss=3.220   test_acc= 0.556\n",
      "epoch= 11   train_loss= 2.256   train_acc= 0.952   test_loss=3.110   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.246   train_acc= 0.940   test_loss=3.138   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.210   train_acc= 0.964   test_loss=3.243   test_acc= 0.556\n",
      "epoch= 14   train_loss= 2.172   train_acc= 0.988   test_loss=3.440   test_acc= 0.556\n",
      "epoch= 15   train_loss= 2.161   train_acc= 0.976   test_loss=3.391   test_acc= 0.556\n",
      "epoch= 16   train_loss= 2.140   train_acc= 0.988   test_loss=3.347   test_acc= 0.556\n",
      "epoch= 17   train_loss= 2.145   train_acc= 0.952   test_loss=3.463   test_acc= 0.556\n",
      "epoch= 18   train_loss= 2.093   train_acc= 1.000   test_loss=3.354   test_acc= 0.556\n",
      "epoch= 19   train_loss= 2.071   train_acc= 1.000   test_loss=3.450   test_acc= 0.556\n",
      "epoch= 20   train_loss= 2.044   train_acc= 1.000   test_loss=3.366   test_acc= 0.556\n",
      "epoch= 21   train_loss= 2.042   train_acc= 0.988   test_loss=3.472   test_acc= 0.556\n",
      "epoch= 22   train_loss= 2.021   train_acc= 1.000   test_loss=3.612   test_acc= 0.556\n",
      "epoch= 23   train_loss= 2.016   train_acc= 1.000   test_loss=3.497   test_acc= 0.556\n",
      "epoch= 24   train_loss= 1.995   train_acc= 1.000   test_loss=3.688   test_acc= 0.556\n",
      "epoch= 25   train_loss= 1.983   train_acc= 0.988   test_loss=3.382   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.965   train_acc= 1.000   test_loss=3.699   test_acc= 0.556\n",
      "epoch= 27   train_loss= 1.942   train_acc= 1.000   test_loss=3.562   test_acc= 0.556\n",
      "epoch= 28   train_loss= 1.938   train_acc= 1.000   test_loss=3.725   test_acc= 0.556\n",
      "epoch= 29   train_loss= 1.926   train_acc= 0.988   test_loss=3.677   test_acc= 0.556\n",
      "epoch= 30   train_loss= 1.915   train_acc= 0.988   test_loss=3.755   test_acc= 0.556\n",
      "epoch= 31   train_loss= 1.892   train_acc= 1.000   test_loss=3.672   test_acc= 0.556\n",
      "epoch= 32   train_loss= 1.882   train_acc= 1.000   test_loss=3.708   test_acc= 0.556\n",
      "epoch= 33   train_loss= 1.870   train_acc= 1.000   test_loss=3.642   test_acc= 0.556\n",
      "epoch= 34   train_loss= 1.876   train_acc= 0.988   test_loss=3.498   test_acc= 0.556\n",
      "epoch= 35   train_loss= 1.856   train_acc= 1.000   test_loss=3.501   test_acc= 0.556\n",
      "epoch= 36   train_loss= 1.842   train_acc= 0.988   test_loss=4.011   test_acc= 0.556\n",
      "epoch= 37   train_loss= 1.820   train_acc= 1.000   test_loss=3.907   test_acc= 0.556\n",
      "epoch= 38   train_loss= 1.816   train_acc= 1.000   test_loss=3.833   test_acc= 0.556\n",
      "epoch= 39   train_loss= 1.799   train_acc= 1.000   test_loss=3.733   test_acc= 0.556\n",
      "epoch= 40   train_loss= 1.796   train_acc= 1.000   test_loss=3.752   test_acc= 0.556\n",
      "epoch= 41   train_loss= 1.777   train_acc= 1.000   test_loss=3.752   test_acc= 0.556\n",
      "epoch= 42   train_loss= 1.777   train_acc= 1.000   test_loss=3.694   test_acc= 0.556\n",
      "epoch= 43   train_loss= 1.756   train_acc= 1.000   test_loss=3.651   test_acc= 0.556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 44   train_loss= 1.745   train_acc= 1.000   test_loss=3.650   test_acc= 0.556\n",
      "epoch= 45   train_loss= 1.737   train_acc= 1.000   test_loss=3.719   test_acc= 0.556\n",
      "epoch= 46   train_loss= 1.724   train_acc= 1.000   test_loss=3.693   test_acc= 0.556\n",
      "epoch= 47   train_loss= 1.716   train_acc= 1.000   test_loss=3.753   test_acc= 0.556\n",
      "epoch= 48   train_loss= 1.709   train_acc= 1.000   test_loss=3.771   test_acc= 0.556\n",
      "epoch= 49   train_loss= 1.694   train_acc= 1.000   test_loss=3.698   test_acc= 0.556\n",
      "run time: 0.6951592485109965 min\n",
      "test_acc=0.556\n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-211     \n",
      "MUSK-211     \n",
      "MUSK-212     \n",
      "MUSK-212     \n",
      "MUSK-212     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-219     \n",
      "MUSK-219     \n",
      "MUSK-224     \n",
      "MUSK-224     \n",
      "MUSK-227     \n",
      "MUSK-227     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-254     \n",
      "MUSK-254     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-272     \n",
      "MUSK-272     \n",
      "MUSK-272     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-311     \n",
      "MUSK-311     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-321     \n",
      "MUSK-321     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-344     \n",
      "MUSK-344     \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f159    \n",
      "MUSK-f159    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-j33     \n",
      "MUSK-j33     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-jf17    \n",
      "MUSK-jf17    \n",
      "MUSK-jf17    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-208 \n",
      "NON-MUSK-208 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-226 \n",
      "NON-MUSK-226 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-247 \n",
      "NON-MUSK-247 \n",
      "NON-MUSK-249 \n",
      "NON-MUSK-249 \n",
      "NON-MUSK-253 \n",
      "NON-MUSK-253 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-271 \n",
      "NON-MUSK-271 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-290 \n",
      "NON-MUSK-290 \n",
      "NON-MUSK-295 \n",
      "NON-MUSK-295 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-305 \n",
      "NON-MUSK-305 \n",
      "NON-MUSK-308 \n",
      "NON-MUSK-308 \n",
      "NON-MUSK-309 \n",
      "NON-MUSK-309 \n",
      "NON-MUSK-318 \n",
      "NON-MUSK-318 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-320 \n",
      "NON-MUSK-320 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-334 \n",
      "NON-MUSK-334 \n",
      "NON-MUSK-f150\n",
      "NON-MUSK-f150\n",
      "NON-MUSK-f161\n",
      "NON-MUSK-f161\n",
      "NON-MUSK-f164\n",
      "NON-MUSK-f164\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j130\n",
      "NON-MUSK-j130\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j148\n",
      "NON-MUSK-j148\n",
      "NON-MUSK-j81 \n",
      "NON-MUSK-j81 \n",
      "NON-MUSK-j83 \n",
      "NON-MUSK-j83 \n",
      "NON-MUSK-j84 \n",
      "NON-MUSK-j84 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "run= 3   fold= 0\n",
      "epoch= 0   train_loss= 3.039   train_acc= 0.598   test_loss=2.980   test_acc= 0.300\n",
      "epoch= 1   train_loss= 2.852   train_acc= 0.695   test_loss=2.888   test_acc= 0.600\n",
      "epoch= 2   train_loss= 2.781   train_acc= 0.756   test_loss=2.774   test_acc= 0.700\n",
      "epoch= 3   train_loss= 2.658   train_acc= 0.805   test_loss=2.639   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.585   train_acc= 0.829   test_loss=2.596   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.536   train_acc= 0.902   test_loss=2.552   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.526   train_acc= 0.866   test_loss=2.538   test_acc= 0.800\n",
      "epoch= 7   train_loss= 2.441   train_acc= 0.915   test_loss=2.552   test_acc= 0.700\n",
      "epoch= 8   train_loss= 2.421   train_acc= 0.939   test_loss=2.391   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.343   train_acc= 0.927   test_loss=2.435   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.334   train_acc= 0.902   test_loss=2.398   test_acc= 0.800\n",
      "epoch= 11   train_loss= 2.288   train_acc= 0.927   test_loss=2.388   test_acc= 0.800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 12   train_loss= 2.257   train_acc= 0.963   test_loss=2.399   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.257   train_acc= 0.939   test_loss=2.362   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.197   train_acc= 0.951   test_loss=2.368   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.177   train_acc= 0.976   test_loss=2.264   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.154   train_acc= 0.976   test_loss=2.277   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.143   train_acc= 0.988   test_loss=2.308   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.115   train_acc= 0.988   test_loss=2.269   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.106   train_acc= 0.963   test_loss=2.256   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.072   train_acc= 0.988   test_loss=2.196   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.054   train_acc= 0.988   test_loss=2.151   test_acc= 0.900\n",
      "epoch= 22   train_loss= 2.023   train_acc= 1.000   test_loss=2.194   test_acc= 0.900\n",
      "epoch= 23   train_loss= 2.014   train_acc= 0.988   test_loss=2.183   test_acc= 0.800\n",
      "epoch= 24   train_loss= 2.004   train_acc= 0.988   test_loss=2.205   test_acc= 0.800\n",
      "epoch= 25   train_loss= 1.985   train_acc= 1.000   test_loss=2.181   test_acc= 0.800\n",
      "epoch= 26   train_loss= 1.968   train_acc= 1.000   test_loss=2.157   test_acc= 0.800\n",
      "epoch= 27   train_loss= 1.940   train_acc= 1.000   test_loss=2.138   test_acc= 0.800\n",
      "epoch= 28   train_loss= 1.924   train_acc= 1.000   test_loss=2.075   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.929   train_acc= 1.000   test_loss=2.119   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.902   train_acc= 1.000   test_loss=2.089   test_acc= 0.800\n",
      "epoch= 31   train_loss= 1.896   train_acc= 1.000   test_loss=2.101   test_acc= 0.800\n",
      "epoch= 32   train_loss= 1.873   train_acc= 1.000   test_loss=2.059   test_acc= 0.800\n",
      "epoch= 33   train_loss= 1.862   train_acc= 1.000   test_loss=2.043   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.850   train_acc= 1.000   test_loss=2.065   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.859   train_acc= 0.988   test_loss=2.082   test_acc= 0.800\n",
      "epoch= 36   train_loss= 1.833   train_acc= 1.000   test_loss=2.036   test_acc= 0.800\n",
      "epoch= 37   train_loss= 1.823   train_acc= 1.000   test_loss=2.051   test_acc= 0.800\n",
      "epoch= 38   train_loss= 1.816   train_acc= 1.000   test_loss=2.022   test_acc= 0.800\n",
      "epoch= 39   train_loss= 1.802   train_acc= 1.000   test_loss=2.009   test_acc= 0.800\n",
      "epoch= 40   train_loss= 1.782   train_acc= 1.000   test_loss=1.961   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.781   train_acc= 1.000   test_loss=1.951   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.766   train_acc= 1.000   test_loss=1.924   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.754   train_acc= 1.000   test_loss=1.891   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.751   train_acc= 1.000   test_loss=1.874   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.734   train_acc= 1.000   test_loss=1.901   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.725   train_acc= 1.000   test_loss=1.877   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.709   train_acc= 1.000   test_loss=1.884   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.719   train_acc= 1.000   test_loss=1.862   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.700   train_acc= 1.000   test_loss=1.849   test_acc= 0.900\n",
      "run time: 0.7128431836764018 min\n",
      "test_acc=0.900\n",
      "run= 3   fold= 1\n",
      "epoch= 0   train_loss= 3.025   train_acc= 0.610   test_loss=2.871   test_acc= 0.600\n",
      "epoch= 1   train_loss= 2.824   train_acc= 0.793   test_loss=2.768   test_acc= 0.600\n",
      "epoch= 2   train_loss= 2.744   train_acc= 0.756   test_loss=2.705   test_acc= 0.800\n",
      "epoch= 3   train_loss= 2.654   train_acc= 0.817   test_loss=2.672   test_acc= 0.700\n",
      "epoch= 4   train_loss= 2.602   train_acc= 0.841   test_loss=2.617   test_acc= 0.800\n",
      "epoch= 5   train_loss= 2.503   train_acc= 0.866   test_loss=2.587   test_acc= 0.800\n",
      "epoch= 6   train_loss= 2.495   train_acc= 0.890   test_loss=2.599   test_acc= 0.700\n",
      "epoch= 7   train_loss= 2.411   train_acc= 0.902   test_loss=2.544   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.416   train_acc= 0.878   test_loss=2.528   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.356   train_acc= 0.927   test_loss=2.488   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.352   train_acc= 0.939   test_loss=2.456   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.299   train_acc= 0.939   test_loss=2.481   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.250   train_acc= 0.963   test_loss=2.441   test_acc= 0.800\n",
      "epoch= 13   train_loss= 2.237   train_acc= 0.951   test_loss=2.408   test_acc= 0.800\n",
      "epoch= 14   train_loss= 2.202   train_acc= 0.976   test_loss=2.436   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.174   train_acc= 0.976   test_loss=2.455   test_acc= 0.800\n",
      "epoch= 16   train_loss= 2.132   train_acc= 1.000   test_loss=2.416   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.127   train_acc= 1.000   test_loss=2.374   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.115   train_acc= 0.988   test_loss=2.374   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.090   train_acc= 0.988   test_loss=2.346   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.061   train_acc= 1.000   test_loss=2.355   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.062   train_acc= 1.000   test_loss=2.343   test_acc= 0.900\n",
      "epoch= 22   train_loss= 2.045   train_acc= 0.988   test_loss=2.343   test_acc= 0.900\n",
      "epoch= 23   train_loss= 2.016   train_acc= 1.000   test_loss=2.356   test_acc= 0.900\n",
      "epoch= 24   train_loss= 1.995   train_acc= 1.000   test_loss=2.353   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.991   train_acc= 1.000   test_loss=2.396   test_acc= 0.900\n",
      "epoch= 26   train_loss= 1.966   train_acc= 1.000   test_loss=2.358   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.951   train_acc= 1.000   test_loss=2.364   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.935   train_acc= 1.000   test_loss=2.360   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.930   train_acc= 1.000   test_loss=2.329   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.917   train_acc= 1.000   test_loss=2.347   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.909   train_acc= 1.000   test_loss=2.309   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.891   train_acc= 1.000   test_loss=2.317   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.876   train_acc= 1.000   test_loss=2.337   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.869   train_acc= 0.988   test_loss=2.323   test_acc= 0.800\n",
      "epoch= 35   train_loss= 1.854   train_acc= 1.000   test_loss=2.308   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.843   train_acc= 1.000   test_loss=2.316   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.830   train_acc= 1.000   test_loss=2.308   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.820   train_acc= 1.000   test_loss=2.296   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.806   train_acc= 1.000   test_loss=2.286   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.796   train_acc= 1.000   test_loss=2.272   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.781   train_acc= 1.000   test_loss=2.262   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.775   train_acc= 1.000   test_loss=2.279   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.766   train_acc= 1.000   test_loss=2.228   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.747   train_acc= 1.000   test_loss=2.223   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.750   train_acc= 1.000   test_loss=2.241   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.729   train_acc= 1.000   test_loss=2.219   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.722   train_acc= 1.000   test_loss=2.223   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.710   train_acc= 1.000   test_loss=2.194   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.706   train_acc= 1.000   test_loss=2.191   test_acc= 0.900\n",
      "run time: 0.6972539822260538 min\n",
      "test_acc=0.900\n",
      "run= 3   fold= 2\n",
      "epoch= 0   train_loss= 3.032   train_acc= 0.554   test_loss=3.001   test_acc= 0.444\n",
      "epoch= 1   train_loss= 2.810   train_acc= 0.783   test_loss=2.991   test_acc= 0.444\n",
      "epoch= 2   train_loss= 2.754   train_acc= 0.795   test_loss=2.966   test_acc= 0.556\n",
      "epoch= 3   train_loss= 2.669   train_acc= 0.843   test_loss=2.893   test_acc= 0.556\n",
      "epoch= 4   train_loss= 2.606   train_acc= 0.855   test_loss=2.864   test_acc= 0.556\n",
      "epoch= 5   train_loss= 2.552   train_acc= 0.880   test_loss=2.828   test_acc= 0.556\n",
      "epoch= 6   train_loss= 2.465   train_acc= 0.916   test_loss=2.789   test_acc= 0.556\n",
      "epoch= 7   train_loss= 2.445   train_acc= 0.892   test_loss=2.770   test_acc= 0.556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 8   train_loss= 2.404   train_acc= 0.928   test_loss=2.718   test_acc= 0.667\n",
      "epoch= 9   train_loss= 2.348   train_acc= 0.892   test_loss=2.681   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.303   train_acc= 0.976   test_loss=2.643   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.317   train_acc= 0.904   test_loss=2.613   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.255   train_acc= 0.964   test_loss=2.570   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.262   train_acc= 0.952   test_loss=2.571   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.194   train_acc= 0.964   test_loss=2.502   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.161   train_acc= 0.964   test_loss=2.505   test_acc= 0.667\n",
      "epoch= 16   train_loss= 2.152   train_acc= 0.988   test_loss=2.468   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.117   train_acc= 1.000   test_loss=2.440   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.106   train_acc= 0.976   test_loss=2.419   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.079   train_acc= 1.000   test_loss=2.373   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.078   train_acc= 0.976   test_loss=2.317   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.042   train_acc= 0.988   test_loss=2.302   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.022   train_acc= 1.000   test_loss=2.267   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.003   train_acc= 1.000   test_loss=2.235   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.984   train_acc= 1.000   test_loss=2.221   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.976   train_acc= 1.000   test_loss=2.235   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.952   train_acc= 1.000   test_loss=2.234   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.933   train_acc= 1.000   test_loss=2.208   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.926   train_acc= 1.000   test_loss=2.185   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.906   train_acc= 1.000   test_loss=2.170   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.917   train_acc= 0.988   test_loss=2.175   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.896   train_acc= 1.000   test_loss=2.161   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.869   train_acc= 1.000   test_loss=2.148   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.863   train_acc= 1.000   test_loss=2.127   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.848   train_acc= 1.000   test_loss=2.092   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.838   train_acc= 1.000   test_loss=2.079   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.832   train_acc= 1.000   test_loss=2.072   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.813   train_acc= 1.000   test_loss=2.049   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.803   train_acc= 1.000   test_loss=2.022   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.790   train_acc= 1.000   test_loss=2.015   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.778   train_acc= 1.000   test_loss=2.002   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.772   train_acc= 1.000   test_loss=2.004   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.765   train_acc= 1.000   test_loss=1.998   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.749   train_acc= 1.000   test_loss=1.988   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.746   train_acc= 1.000   test_loss=1.978   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.731   train_acc= 1.000   test_loss=1.951   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.715   train_acc= 1.000   test_loss=1.941   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.704   train_acc= 1.000   test_loss=1.937   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.697   train_acc= 1.000   test_loss=1.922   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.682   train_acc= 1.000   test_loss=1.912   test_acc= 0.889\n",
      "run time: 0.742206319173177 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 3\n",
      "epoch= 0   train_loss= 2.945   train_acc= 0.627   test_loss=2.950   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.806   train_acc= 0.723   test_loss=2.757   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.680   train_acc= 0.831   test_loss=2.646   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.629   train_acc= 0.843   test_loss=2.611   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.556   train_acc= 0.855   test_loss=2.601   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.495   train_acc= 0.880   test_loss=2.569   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.475   train_acc= 0.867   test_loss=2.595   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.427   train_acc= 0.892   test_loss=2.553   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.363   train_acc= 0.952   test_loss=2.593   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.350   train_acc= 0.916   test_loss=2.576   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.308   train_acc= 0.976   test_loss=2.523   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.274   train_acc= 0.964   test_loss=2.460   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.242   train_acc= 0.964   test_loss=2.487   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.243   train_acc= 0.964   test_loss=2.486   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.202   train_acc= 0.988   test_loss=2.512   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.168   train_acc= 0.976   test_loss=2.547   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.142   train_acc= 0.988   test_loss=2.554   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.122   train_acc= 1.000   test_loss=2.534   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.124   train_acc= 0.988   test_loss=2.487   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.094   train_acc= 0.976   test_loss=2.501   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.072   train_acc= 0.988   test_loss=2.463   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.038   train_acc= 1.000   test_loss=2.431   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.043   train_acc= 0.976   test_loss=2.407   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.008   train_acc= 1.000   test_loss=2.519   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.013   train_acc= 0.988   test_loss=2.485   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.985   train_acc= 1.000   test_loss=2.407   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.974   train_acc= 1.000   test_loss=2.454   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.953   train_acc= 1.000   test_loss=2.427   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.941   train_acc= 1.000   test_loss=2.442   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.944   train_acc= 0.988   test_loss=2.431   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.922   train_acc= 1.000   test_loss=2.416   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.907   train_acc= 1.000   test_loss=2.392   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.895   train_acc= 0.988   test_loss=2.400   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.886   train_acc= 1.000   test_loss=2.416   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.863   train_acc= 1.000   test_loss=2.411   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.848   train_acc= 1.000   test_loss=2.389   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.835   train_acc= 1.000   test_loss=2.367   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.833   train_acc= 1.000   test_loss=2.360   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.813   train_acc= 1.000   test_loss=2.353   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.807   train_acc= 1.000   test_loss=2.368   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.800   train_acc= 1.000   test_loss=2.318   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.795   train_acc= 1.000   test_loss=2.354   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.778   train_acc= 1.000   test_loss=2.354   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.759   train_acc= 1.000   test_loss=2.364   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.754   train_acc= 1.000   test_loss=2.334   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.743   train_acc= 1.000   test_loss=2.302   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.729   train_acc= 1.000   test_loss=2.312   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.722   train_acc= 1.000   test_loss=2.302   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.711   train_acc= 1.000   test_loss=2.302   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.706   train_acc= 1.000   test_loss=2.309   test_acc= 0.889\n",
      "run time: 0.7086130142211914 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 4\n",
      "epoch= 0   train_loss= 2.999   train_acc= 0.627   test_loss=2.971   test_acc= 0.333\n",
      "epoch= 1   train_loss= 2.806   train_acc= 0.759   test_loss=2.798   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.706   train_acc= 0.783   test_loss=2.770   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.605   train_acc= 0.843   test_loss=2.780   test_acc= 0.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 4   train_loss= 2.532   train_acc= 0.831   test_loss=2.841   test_acc= 0.444\n",
      "epoch= 5   train_loss= 2.494   train_acc= 0.880   test_loss=2.851   test_acc= 0.444\n",
      "epoch= 6   train_loss= 2.440   train_acc= 0.916   test_loss=2.886   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.364   train_acc= 0.952   test_loss=2.902   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.319   train_acc= 0.952   test_loss=2.915   test_acc= 0.444\n",
      "epoch= 9   train_loss= 2.318   train_acc= 0.940   test_loss=2.897   test_acc= 0.556\n",
      "epoch= 10   train_loss= 2.280   train_acc= 0.976   test_loss=2.946   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.242   train_acc= 0.952   test_loss=2.987   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.209   train_acc= 0.976   test_loss=2.935   test_acc= 0.556\n",
      "epoch= 13   train_loss= 2.192   train_acc= 0.964   test_loss=2.996   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.164   train_acc= 0.976   test_loss=3.012   test_acc= 0.556\n",
      "epoch= 15   train_loss= 2.123   train_acc= 1.000   test_loss=3.049   test_acc= 0.667\n",
      "epoch= 16   train_loss= 2.121   train_acc= 0.976   test_loss=3.025   test_acc= 0.556\n",
      "epoch= 17   train_loss= 2.093   train_acc= 0.988   test_loss=3.109   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.068   train_acc= 1.000   test_loss=3.165   test_acc= 0.667\n",
      "epoch= 19   train_loss= 2.055   train_acc= 0.988   test_loss=3.159   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.047   train_acc= 0.988   test_loss=3.082   test_acc= 0.556\n",
      "epoch= 21   train_loss= 2.034   train_acc= 1.000   test_loss=3.151   test_acc= 0.667\n",
      "epoch= 22   train_loss= 1.998   train_acc= 1.000   test_loss=3.172   test_acc= 0.667\n",
      "epoch= 23   train_loss= 1.993   train_acc= 1.000   test_loss=3.152   test_acc= 0.556\n",
      "epoch= 24   train_loss= 1.982   train_acc= 1.000   test_loss=3.193   test_acc= 0.667\n",
      "epoch= 25   train_loss= 1.961   train_acc= 1.000   test_loss=3.179   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.947   train_acc= 1.000   test_loss=3.179   test_acc= 0.667\n",
      "epoch= 27   train_loss= 1.930   train_acc= 1.000   test_loss=3.185   test_acc= 0.667\n",
      "epoch= 28   train_loss= 1.932   train_acc= 1.000   test_loss=3.148   test_acc= 0.667\n",
      "epoch= 29   train_loss= 1.908   train_acc= 1.000   test_loss=3.208   test_acc= 0.667\n",
      "epoch= 30   train_loss= 1.887   train_acc= 1.000   test_loss=3.209   test_acc= 0.667\n",
      "epoch= 31   train_loss= 1.877   train_acc= 1.000   test_loss=3.216   test_acc= 0.667\n",
      "epoch= 32   train_loss= 1.869   train_acc= 1.000   test_loss=3.216   test_acc= 0.667\n",
      "epoch= 33   train_loss= 1.853   train_acc= 1.000   test_loss=3.253   test_acc= 0.667\n",
      "epoch= 34   train_loss= 1.840   train_acc= 1.000   test_loss=3.240   test_acc= 0.667\n",
      "epoch= 35   train_loss= 1.830   train_acc= 1.000   test_loss=3.242   test_acc= 0.667\n",
      "epoch= 36   train_loss= 1.826   train_acc= 1.000   test_loss=3.182   test_acc= 0.667\n",
      "epoch= 37   train_loss= 1.810   train_acc= 1.000   test_loss=3.234   test_acc= 0.667\n",
      "epoch= 38   train_loss= 1.798   train_acc= 1.000   test_loss=3.208   test_acc= 0.667\n",
      "epoch= 39   train_loss= 1.786   train_acc= 1.000   test_loss=3.215   test_acc= 0.667\n",
      "epoch= 40   train_loss= 1.780   train_acc= 1.000   test_loss=3.218   test_acc= 0.667\n",
      "epoch= 41   train_loss= 1.761   train_acc= 1.000   test_loss=3.218   test_acc= 0.667\n",
      "epoch= 42   train_loss= 1.763   train_acc= 0.988   test_loss=3.214   test_acc= 0.667\n",
      "epoch= 43   train_loss= 1.747   train_acc= 1.000   test_loss=3.202   test_acc= 0.667\n",
      "epoch= 44   train_loss= 1.736   train_acc= 1.000   test_loss=3.179   test_acc= 0.667\n",
      "epoch= 45   train_loss= 1.724   train_acc= 1.000   test_loss=3.194   test_acc= 0.667\n",
      "epoch= 46   train_loss= 1.715   train_acc= 1.000   test_loss=3.201   test_acc= 0.667\n",
      "epoch= 47   train_loss= 1.711   train_acc= 1.000   test_loss=3.115   test_acc= 0.667\n",
      "epoch= 48   train_loss= 1.696   train_acc= 1.000   test_loss=3.115   test_acc= 0.667\n",
      "epoch= 49   train_loss= 1.688   train_acc= 1.000   test_loss=3.076   test_acc= 0.667\n",
      "run time: 0.7124061346054077 min\n",
      "test_acc=0.667\n",
      "run= 3   fold= 5\n",
      "epoch= 0   train_loss= 2.954   train_acc= 0.566   test_loss=2.911   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.779   train_acc= 0.723   test_loss=2.866   test_acc= 0.556\n",
      "epoch= 2   train_loss= 2.751   train_acc= 0.771   test_loss=2.803   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.654   train_acc= 0.807   test_loss=2.724   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.610   train_acc= 0.855   test_loss=2.673   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.551   train_acc= 0.807   test_loss=2.696   test_acc= 0.556\n",
      "epoch= 6   train_loss= 2.476   train_acc= 0.904   test_loss=2.635   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.464   train_acc= 0.867   test_loss=2.634   test_acc= 0.556\n",
      "epoch= 8   train_loss= 2.416   train_acc= 0.904   test_loss=2.582   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.365   train_acc= 0.928   test_loss=2.549   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.348   train_acc= 0.892   test_loss=2.582   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.269   train_acc= 0.988   test_loss=2.547   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.288   train_acc= 0.928   test_loss=2.554   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.273   train_acc= 0.928   test_loss=2.569   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.189   train_acc= 0.976   test_loss=2.588   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.198   train_acc= 0.976   test_loss=2.541   test_acc= 0.667\n",
      "epoch= 16   train_loss= 2.150   train_acc= 0.976   test_loss=2.530   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.136   train_acc= 0.976   test_loss=2.509   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.112   train_acc= 0.988   test_loss=2.505   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.074   train_acc= 1.000   test_loss=2.493   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.065   train_acc= 0.976   test_loss=2.486   test_acc= 0.667\n",
      "epoch= 21   train_loss= 2.054   train_acc= 0.988   test_loss=2.515   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.046   train_acc= 0.976   test_loss=2.482   test_acc= 0.667\n",
      "epoch= 23   train_loss= 2.024   train_acc= 0.988   test_loss=2.522   test_acc= 0.667\n",
      "epoch= 24   train_loss= 1.991   train_acc= 1.000   test_loss=2.558   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.973   train_acc= 1.000   test_loss=2.509   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.961   train_acc= 1.000   test_loss=2.553   test_acc= 0.667\n",
      "epoch= 27   train_loss= 1.944   train_acc= 1.000   test_loss=2.513   test_acc= 0.667\n",
      "epoch= 28   train_loss= 1.932   train_acc= 1.000   test_loss=2.515   test_acc= 0.667\n",
      "epoch= 29   train_loss= 1.920   train_acc= 1.000   test_loss=2.516   test_acc= 0.667\n",
      "epoch= 30   train_loss= 1.897   train_acc= 1.000   test_loss=2.479   test_acc= 0.667\n",
      "epoch= 31   train_loss= 1.890   train_acc= 1.000   test_loss=2.464   test_acc= 0.667\n",
      "epoch= 32   train_loss= 1.886   train_acc= 1.000   test_loss=2.466   test_acc= 0.667\n",
      "epoch= 33   train_loss= 1.866   train_acc= 1.000   test_loss=2.416   test_acc= 0.667\n",
      "epoch= 34   train_loss= 1.850   train_acc= 1.000   test_loss=2.423   test_acc= 0.667\n",
      "epoch= 35   train_loss= 1.841   train_acc= 1.000   test_loss=2.428   test_acc= 0.667\n",
      "epoch= 36   train_loss= 1.827   train_acc= 1.000   test_loss=2.415   test_acc= 0.667\n",
      "epoch= 37   train_loss= 1.812   train_acc= 1.000   test_loss=2.415   test_acc= 0.667\n",
      "epoch= 38   train_loss= 1.795   train_acc= 1.000   test_loss=2.395   test_acc= 0.667\n",
      "epoch= 39   train_loss= 1.792   train_acc= 1.000   test_loss=2.379   test_acc= 0.667\n",
      "epoch= 40   train_loss= 1.783   train_acc= 1.000   test_loss=2.396   test_acc= 0.667\n",
      "epoch= 41   train_loss= 1.770   train_acc= 1.000   test_loss=2.403   test_acc= 0.667\n",
      "epoch= 42   train_loss= 1.759   train_acc= 1.000   test_loss=2.366   test_acc= 0.667\n",
      "epoch= 43   train_loss= 1.752   train_acc= 1.000   test_loss=2.381   test_acc= 0.667\n",
      "epoch= 44   train_loss= 1.737   train_acc= 1.000   test_loss=2.365   test_acc= 0.667\n",
      "epoch= 45   train_loss= 1.734   train_acc= 1.000   test_loss=2.354   test_acc= 0.667\n",
      "epoch= 46   train_loss= 1.712   train_acc= 1.000   test_loss=2.362   test_acc= 0.667\n",
      "epoch= 47   train_loss= 1.707   train_acc= 1.000   test_loss=2.362   test_acc= 0.667\n",
      "epoch= 48   train_loss= 1.703   train_acc= 1.000   test_loss=2.372   test_acc= 0.667\n",
      "epoch= 49   train_loss= 1.685   train_acc= 1.000   test_loss=2.392   test_acc= 0.667\n",
      "run time: 0.736993916829427 min\n",
      "test_acc=0.667\n",
      "run= 3   fold= 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0   train_loss= 3.049   train_acc= 0.482   test_loss=3.040   test_acc= 0.444\n",
      "epoch= 1   train_loss= 2.833   train_acc= 0.687   test_loss=3.046   test_acc= 0.444\n",
      "epoch= 2   train_loss= 2.743   train_acc= 0.771   test_loss=2.874   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.654   train_acc= 0.855   test_loss=2.879   test_acc= 0.333\n",
      "epoch= 4   train_loss= 2.599   train_acc= 0.831   test_loss=2.803   test_acc= 0.556\n",
      "epoch= 5   train_loss= 2.522   train_acc= 0.880   test_loss=2.843   test_acc= 0.444\n",
      "epoch= 6   train_loss= 2.479   train_acc= 0.880   test_loss=2.738   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.457   train_acc= 0.892   test_loss=2.904   test_acc= 0.444\n",
      "epoch= 8   train_loss= 2.395   train_acc= 0.940   test_loss=2.790   test_acc= 0.556\n",
      "epoch= 9   train_loss= 2.357   train_acc= 0.928   test_loss=2.880   test_acc= 0.444\n",
      "epoch= 10   train_loss= 2.322   train_acc= 0.964   test_loss=2.827   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.259   train_acc= 0.976   test_loss=2.780   test_acc= 0.556\n",
      "epoch= 12   train_loss= 2.246   train_acc= 0.964   test_loss=2.634   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.218   train_acc= 0.964   test_loss=2.667   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.180   train_acc= 0.988   test_loss=2.646   test_acc= 0.667\n",
      "epoch= 15   train_loss= 2.175   train_acc= 0.976   test_loss=2.616   test_acc= 0.667\n",
      "epoch= 16   train_loss= 2.171   train_acc= 0.964   test_loss=2.824   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.119   train_acc= 1.000   test_loss=2.547   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.119   train_acc= 0.988   test_loss=2.540   test_acc= 0.667\n",
      "epoch= 19   train_loss= 2.090   train_acc= 0.976   test_loss=2.616   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.082   train_acc= 0.988   test_loss=2.824   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.041   train_acc= 0.988   test_loss=2.550   test_acc= 0.667\n",
      "epoch= 22   train_loss= 2.018   train_acc= 1.000   test_loss=2.602   test_acc= 0.667\n",
      "epoch= 23   train_loss= 2.013   train_acc= 1.000   test_loss=2.564   test_acc= 0.667\n",
      "epoch= 24   train_loss= 2.003   train_acc= 1.000   test_loss=2.652   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.998   train_acc= 1.000   test_loss=2.716   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.967   train_acc= 1.000   test_loss=2.615   test_acc= 0.667\n",
      "epoch= 27   train_loss= 1.942   train_acc= 1.000   test_loss=2.695   test_acc= 0.667\n",
      "epoch= 28   train_loss= 1.939   train_acc= 1.000   test_loss=2.598   test_acc= 0.667\n",
      "epoch= 29   train_loss= 1.938   train_acc= 1.000   test_loss=2.660   test_acc= 0.667\n",
      "epoch= 30   train_loss= 1.922   train_acc= 0.988   test_loss=2.660   test_acc= 0.667\n",
      "epoch= 31   train_loss= 1.913   train_acc= 1.000   test_loss=2.717   test_acc= 0.667\n",
      "epoch= 32   train_loss= 1.891   train_acc= 1.000   test_loss=2.672   test_acc= 0.667\n",
      "epoch= 33   train_loss= 1.879   train_acc= 0.988   test_loss=2.725   test_acc= 0.667\n",
      "epoch= 34   train_loss= 1.872   train_acc= 1.000   test_loss=2.635   test_acc= 0.667\n",
      "epoch= 35   train_loss= 1.854   train_acc= 1.000   test_loss=2.695   test_acc= 0.667\n",
      "epoch= 36   train_loss= 1.839   train_acc= 1.000   test_loss=2.685   test_acc= 0.667\n",
      "epoch= 37   train_loss= 1.822   train_acc= 1.000   test_loss=2.600   test_acc= 0.667\n",
      "epoch= 38   train_loss= 1.816   train_acc= 1.000   test_loss=2.684   test_acc= 0.667\n",
      "epoch= 39   train_loss= 1.804   train_acc= 1.000   test_loss=2.594   test_acc= 0.667\n",
      "epoch= 40   train_loss= 1.794   train_acc= 1.000   test_loss=2.582   test_acc= 0.667\n",
      "epoch= 41   train_loss= 1.781   train_acc= 1.000   test_loss=2.561   test_acc= 0.667\n",
      "epoch= 42   train_loss= 1.772   train_acc= 1.000   test_loss=2.625   test_acc= 0.667\n",
      "epoch= 43   train_loss= 1.760   train_acc= 1.000   test_loss=2.670   test_acc= 0.667\n",
      "epoch= 44   train_loss= 1.753   train_acc= 1.000   test_loss=2.658   test_acc= 0.667\n",
      "epoch= 45   train_loss= 1.743   train_acc= 1.000   test_loss=2.515   test_acc= 0.667\n",
      "epoch= 46   train_loss= 1.732   train_acc= 1.000   test_loss=2.555   test_acc= 0.667\n",
      "epoch= 47   train_loss= 1.720   train_acc= 1.000   test_loss=2.590   test_acc= 0.667\n",
      "epoch= 48   train_loss= 1.708   train_acc= 1.000   test_loss=2.523   test_acc= 0.667\n",
      "epoch= 49   train_loss= 1.702   train_acc= 1.000   test_loss=2.617   test_acc= 0.667\n",
      "run time: 0.7086019198099772 min\n",
      "test_acc=0.667\n",
      "run= 3   fold= 7\n",
      "epoch= 0   train_loss= 2.999   train_acc= 0.578   test_loss=2.872   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.827   train_acc= 0.771   test_loss=2.820   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.748   train_acc= 0.771   test_loss=2.787   test_acc= 0.556\n",
      "epoch= 3   train_loss= 2.688   train_acc= 0.843   test_loss=2.743   test_acc= 0.556\n",
      "epoch= 4   train_loss= 2.580   train_acc= 0.880   test_loss=2.786   test_acc= 0.556\n",
      "epoch= 5   train_loss= 2.528   train_acc= 0.880   test_loss=2.769   test_acc= 0.556\n",
      "epoch= 6   train_loss= 2.503   train_acc= 0.892   test_loss=2.613   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.445   train_acc= 0.916   test_loss=2.729   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.404   train_acc= 0.916   test_loss=2.904   test_acc= 0.556\n",
      "epoch= 9   train_loss= 2.403   train_acc= 0.892   test_loss=2.871   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.328   train_acc= 0.940   test_loss=2.772   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.301   train_acc= 0.952   test_loss=2.816   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.291   train_acc= 0.940   test_loss=2.673   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.244   train_acc= 0.952   test_loss=2.739   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.222   train_acc= 0.940   test_loss=2.805   test_acc= 0.667\n",
      "epoch= 15   train_loss= 2.202   train_acc= 0.964   test_loss=2.565   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.162   train_acc= 0.976   test_loss=2.786   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.157   train_acc= 0.988   test_loss=2.683   test_acc= 0.667\n",
      "epoch= 18   train_loss= 2.116   train_acc= 0.988   test_loss=2.742   test_acc= 0.667\n",
      "epoch= 19   train_loss= 2.098   train_acc= 0.988   test_loss=2.785   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.070   train_acc= 0.988   test_loss=2.794   test_acc= 0.667\n",
      "epoch= 21   train_loss= 2.060   train_acc= 1.000   test_loss=2.839   test_acc= 0.667\n",
      "epoch= 22   train_loss= 2.039   train_acc= 1.000   test_loss=2.781   test_acc= 0.667\n",
      "epoch= 23   train_loss= 2.011   train_acc= 1.000   test_loss=2.975   test_acc= 0.667\n",
      "epoch= 24   train_loss= 1.995   train_acc= 1.000   test_loss=2.929   test_acc= 0.667\n",
      "epoch= 25   train_loss= 1.986   train_acc= 1.000   test_loss=2.961   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.974   train_acc= 1.000   test_loss=2.839   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.960   train_acc= 1.000   test_loss=2.800   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.942   train_acc= 1.000   test_loss=2.889   test_acc= 0.667\n",
      "epoch= 29   train_loss= 1.934   train_acc= 1.000   test_loss=2.804   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.920   train_acc= 1.000   test_loss=2.881   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.902   train_acc= 1.000   test_loss=2.898   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.882   train_acc= 1.000   test_loss=2.961   test_acc= 0.667\n",
      "epoch= 33   train_loss= 1.877   train_acc= 1.000   test_loss=3.007   test_acc= 0.667\n",
      "epoch= 34   train_loss= 1.864   train_acc= 1.000   test_loss=2.925   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.846   train_acc= 1.000   test_loss=2.923   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.840   train_acc= 1.000   test_loss=3.071   test_acc= 0.667\n",
      "epoch= 37   train_loss= 1.821   train_acc= 1.000   test_loss=2.942   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.815   train_acc= 1.000   test_loss=2.983   test_acc= 0.667\n",
      "epoch= 39   train_loss= 1.808   train_acc= 1.000   test_loss=3.010   test_acc= 0.667\n",
      "epoch= 40   train_loss= 1.793   train_acc= 1.000   test_loss=2.944   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.783   train_acc= 1.000   test_loss=2.836   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.773   train_acc= 1.000   test_loss=2.803   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.764   train_acc= 1.000   test_loss=2.830   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.754   train_acc= 1.000   test_loss=2.922   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.739   train_acc= 1.000   test_loss=2.877   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 46   train_loss= 1.728   train_acc= 1.000   test_loss=2.918   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.719   train_acc= 1.000   test_loss=2.979   test_acc= 0.667\n",
      "epoch= 48   train_loss= 1.709   train_acc= 1.000   test_loss=2.961   test_acc= 0.667\n",
      "epoch= 49   train_loss= 1.705   train_acc= 1.000   test_loss=2.930   test_acc= 0.667\n",
      "run time: 0.7117309292157491 min\n",
      "test_acc=0.667\n",
      "run= 3   fold= 8\n",
      "epoch= 0   train_loss= 2.948   train_acc= 0.602   test_loss=2.559   test_acc= 1.000\n",
      "epoch= 1   train_loss= 2.809   train_acc= 0.747   test_loss=2.499   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.718   train_acc= 0.819   test_loss=2.432   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.653   train_acc= 0.807   test_loss=2.355   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.622   train_acc= 0.831   test_loss=2.349   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.551   train_acc= 0.795   test_loss=2.275   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.495   train_acc= 0.892   test_loss=2.251   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.398   train_acc= 0.928   test_loss=2.215   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.368   train_acc= 0.964   test_loss=2.194   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.379   train_acc= 0.928   test_loss=2.179   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.371   train_acc= 0.892   test_loss=2.172   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.303   train_acc= 0.964   test_loss=2.144   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.263   train_acc= 0.940   test_loss=2.128   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.230   train_acc= 0.952   test_loss=2.107   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.196   train_acc= 0.988   test_loss=2.095   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.214   train_acc= 0.940   test_loss=2.086   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.157   train_acc= 0.964   test_loss=2.065   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.135   train_acc= 0.964   test_loss=2.047   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.121   train_acc= 0.988   test_loss=2.032   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.112   train_acc= 0.976   test_loss=2.020   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.083   train_acc= 0.988   test_loss=2.007   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.058   train_acc= 1.000   test_loss=1.990   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.021   train_acc= 1.000   test_loss=1.977   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.024   train_acc= 1.000   test_loss=1.964   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.015   train_acc= 0.976   test_loss=1.951   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.981   train_acc= 1.000   test_loss=1.938   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.977   train_acc= 0.988   test_loss=1.927   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.960   train_acc= 1.000   test_loss=1.913   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.941   train_acc= 1.000   test_loss=1.901   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.935   train_acc= 0.988   test_loss=1.889   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.908   train_acc= 1.000   test_loss=1.877   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.917   train_acc= 0.988   test_loss=1.866   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.896   train_acc= 1.000   test_loss=1.855   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.893   train_acc= 0.988   test_loss=1.844   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.870   train_acc= 1.000   test_loss=1.837   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.853   train_acc= 1.000   test_loss=1.823   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.845   train_acc= 1.000   test_loss=1.811   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.828   train_acc= 1.000   test_loss=1.800   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.812   train_acc= 1.000   test_loss=1.789   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.801   train_acc= 1.000   test_loss=1.779   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.795   train_acc= 1.000   test_loss=1.769   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.781   train_acc= 1.000   test_loss=1.758   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.772   train_acc= 1.000   test_loss=1.748   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.762   train_acc= 1.000   test_loss=1.738   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.750   train_acc= 1.000   test_loss=1.728   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.740   train_acc= 1.000   test_loss=1.719   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.730   train_acc= 1.000   test_loss=1.709   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.724   train_acc= 1.000   test_loss=1.699   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.715   train_acc= 1.000   test_loss=1.689   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.697   train_acc= 1.000   test_loss=1.680   test_acc= 1.000\n",
      "run time: 0.7234262665112813 min\n",
      "test_acc=1.000\n",
      "run= 3   fold= 9\n",
      "epoch= 0   train_loss= 2.999   train_acc= 0.554   test_loss=2.836   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.871   train_acc= 0.711   test_loss=2.724   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.741   train_acc= 0.795   test_loss=2.635   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.695   train_acc= 0.831   test_loss=2.557   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.652   train_acc= 0.819   test_loss=2.476   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.605   train_acc= 0.855   test_loss=2.429   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.506   train_acc= 0.880   test_loss=2.376   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.494   train_acc= 0.843   test_loss=2.334   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.470   train_acc= 0.892   test_loss=2.295   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.397   train_acc= 0.892   test_loss=2.261   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.350   train_acc= 0.904   test_loss=2.225   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.323   train_acc= 0.928   test_loss=2.189   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.308   train_acc= 0.940   test_loss=2.170   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.265   train_acc= 0.952   test_loss=2.144   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.260   train_acc= 0.928   test_loss=2.129   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.195   train_acc= 0.976   test_loss=2.106   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.216   train_acc= 0.928   test_loss=2.091   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.155   train_acc= 0.964   test_loss=2.072   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.142   train_acc= 0.988   test_loss=2.055   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.114   train_acc= 0.988   test_loss=2.039   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.100   train_acc= 0.964   test_loss=2.022   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.064   train_acc= 0.988   test_loss=2.005   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.046   train_acc= 0.988   test_loss=1.994   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.033   train_acc= 0.988   test_loss=1.974   test_acc= 1.000\n",
      "epoch= 24   train_loss= 2.032   train_acc= 0.976   test_loss=1.963   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.001   train_acc= 1.000   test_loss=1.949   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.983   train_acc= 0.988   test_loss=1.933   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.961   train_acc= 1.000   test_loss=1.922   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.951   train_acc= 1.000   test_loss=1.908   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.947   train_acc= 0.988   test_loss=1.898   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.904   train_acc= 1.000   test_loss=1.884   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.904   train_acc= 1.000   test_loss=1.872   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.895   train_acc= 1.000   test_loss=1.861   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.884   train_acc= 1.000   test_loss=1.850   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.861   train_acc= 1.000   test_loss=1.838   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.857   train_acc= 1.000   test_loss=1.830   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.837   train_acc= 1.000   test_loss=1.817   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.822   train_acc= 1.000   test_loss=1.806   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.810   train_acc= 1.000   test_loss=1.796   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.807   train_acc= 0.988   test_loss=1.785   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.795   train_acc= 1.000   test_loss=1.775   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.783   train_acc= 1.000   test_loss=1.765   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 42   train_loss= 1.779   train_acc= 1.000   test_loss=1.757   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.757   train_acc= 1.000   test_loss=1.745   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.752   train_acc= 1.000   test_loss=1.737   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.736   train_acc= 1.000   test_loss=1.727   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.730   train_acc= 1.000   test_loss=1.719   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.717   train_acc= 1.000   test_loss=1.710   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.705   train_acc= 1.000   test_loss=1.698   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.709   train_acc= 1.000   test_loss=1.691   test_acc= 1.000\n",
      "run time: 0.7496605674425761 min\n",
      "test_acc=1.000\n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-188     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-190     \n",
      "MUSK-211     \n",
      "MUSK-211     \n",
      "MUSK-212     \n",
      "MUSK-212     \n",
      "MUSK-212     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-213     \n",
      "MUSK-219     \n",
      "MUSK-219     \n",
      "MUSK-224     \n",
      "MUSK-224     \n",
      "MUSK-227     \n",
      "MUSK-227     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-228     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-236     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-238     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-240     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-246     \n",
      "MUSK-254     \n",
      "MUSK-254     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-256     \n",
      "MUSK-272     \n",
      "MUSK-272     \n",
      "MUSK-272     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-273     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-284     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-285     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-292     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-293     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-301     \n",
      "MUSK-311     \n",
      "MUSK-311     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-314     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-315     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-316     \n",
      "MUSK-321     \n",
      "MUSK-321     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-322     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-323     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-330     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-331     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-333     \n",
      "MUSK-344     \n",
      "MUSK-344     \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f152    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f158    \n",
      "MUSK-f159    \n",
      "MUSK-f159    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f184    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-f205    \n",
      "MUSK-j33     \n",
      "MUSK-j33     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-j51     \n",
      "MUSK-jf17    \n",
      "MUSK-jf17    \n",
      "MUSK-jf17    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf46    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf47    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf58    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf59    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf67    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "MUSK-jf78    \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-199 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-200 \n",
      "NON-MUSK-208 \n",
      "NON-MUSK-208 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-220 \n",
      "NON-MUSK-226 \n",
      "NON-MUSK-226 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-232 \n",
      "NON-MUSK-247 \n",
      "NON-MUSK-247 \n",
      "NON-MUSK-249 \n",
      "NON-MUSK-249 \n",
      "NON-MUSK-253 \n",
      "NON-MUSK-253 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-257 \n",
      "NON-MUSK-271 \n",
      "NON-MUSK-271 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-286 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-288 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-289 \n",
      "NON-MUSK-290 \n",
      "NON-MUSK-290 \n",
      "NON-MUSK-295 \n",
      "NON-MUSK-295 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-296 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-297 \n",
      "NON-MUSK-305 \n",
      "NON-MUSK-305 \n",
      "NON-MUSK-308 \n",
      "NON-MUSK-308 \n",
      "NON-MUSK-309 \n",
      "NON-MUSK-309 \n",
      "NON-MUSK-318 \n",
      "NON-MUSK-318 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-319 \n",
      "NON-MUSK-320 \n",
      "NON-MUSK-320 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-327 \n",
      "NON-MUSK-334 \n",
      "NON-MUSK-334 \n",
      "NON-MUSK-f150\n",
      "NON-MUSK-f150\n",
      "NON-MUSK-f161\n",
      "NON-MUSK-f161\n",
      "NON-MUSK-f164\n",
      "NON-MUSK-f164\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-f209\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j100\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j129\n",
      "NON-MUSK-j130\n",
      "NON-MUSK-j130\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j146\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j147\n",
      "NON-MUSK-j148\n",
      "NON-MUSK-j148\n",
      "NON-MUSK-j81 \n",
      "NON-MUSK-j81 \n",
      "NON-MUSK-j83 \n",
      "NON-MUSK-j83 \n",
      "NON-MUSK-j84 \n",
      "NON-MUSK-j84 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j90 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j93 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j96 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-j97 \n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp10\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "NON-MUSK-jp13\n",
      "run= 4   fold= 0\n",
      "epoch= 0   train_loss= 3.033   train_acc= 0.439   test_loss=3.011   test_acc= 0.500\n",
      "epoch= 1   train_loss= 2.791   train_acc= 0.720   test_loss=2.960   test_acc= 0.600\n",
      "epoch= 2   train_loss= 2.706   train_acc= 0.805   test_loss=2.792   test_acc= 0.600\n",
      "epoch= 3   train_loss= 2.575   train_acc= 0.866   test_loss=2.778   test_acc= 0.600\n",
      "epoch= 4   train_loss= 2.562   train_acc= 0.866   test_loss=2.638   test_acc= 0.800\n",
      "epoch= 5   train_loss= 2.508   train_acc= 0.890   test_loss=2.710   test_acc= 0.700\n",
      "epoch= 6   train_loss= 2.448   train_acc= 0.915   test_loss=2.551   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.426   train_acc= 0.915   test_loss=2.646   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.353   train_acc= 0.939   test_loss=2.568   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.362   train_acc= 0.890   test_loss=2.625   test_acc= 0.800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 10   train_loss= 2.334   train_acc= 0.939   test_loss=2.647   test_acc= 0.700\n",
      "epoch= 11   train_loss= 2.286   train_acc= 0.976   test_loss=2.456   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.259   train_acc= 0.963   test_loss=2.568   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.196   train_acc= 1.000   test_loss=2.513   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.179   train_acc= 0.976   test_loss=2.522   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.154   train_acc= 1.000   test_loss=2.554   test_acc= 0.800\n",
      "epoch= 16   train_loss= 2.150   train_acc= 0.963   test_loss=2.380   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.145   train_acc= 0.963   test_loss=2.469   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.115   train_acc= 0.988   test_loss=2.392   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.089   train_acc= 0.988   test_loss=2.497   test_acc= 0.800\n",
      "epoch= 20   train_loss= 2.081   train_acc= 0.976   test_loss=2.403   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.052   train_acc= 1.000   test_loss=2.420   test_acc= 0.900\n",
      "epoch= 22   train_loss= 2.020   train_acc= 1.000   test_loss=2.509   test_acc= 0.800\n",
      "epoch= 23   train_loss= 2.016   train_acc= 1.000   test_loss=2.308   test_acc= 0.900\n",
      "epoch= 24   train_loss= 1.992   train_acc= 1.000   test_loss=2.397   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.974   train_acc= 1.000   test_loss=2.388   test_acc= 0.900\n",
      "epoch= 26   train_loss= 1.955   train_acc= 1.000   test_loss=2.402   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.950   train_acc= 1.000   test_loss=2.409   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.936   train_acc= 1.000   test_loss=2.366   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.917   train_acc= 1.000   test_loss=2.363   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.905   train_acc= 1.000   test_loss=2.676   test_acc= 0.700\n",
      "epoch= 31   train_loss= 1.891   train_acc= 1.000   test_loss=2.545   test_acc= 0.700\n",
      "epoch= 32   train_loss= 1.883   train_acc= 1.000   test_loss=2.440   test_acc= 0.800\n",
      "epoch= 33   train_loss= 1.869   train_acc= 1.000   test_loss=2.415   test_acc= 0.800\n",
      "epoch= 34   train_loss= 1.872   train_acc= 0.988   test_loss=2.369   test_acc= 0.800\n",
      "epoch= 35   train_loss= 1.849   train_acc= 1.000   test_loss=2.431   test_acc= 0.800\n",
      "epoch= 36   train_loss= 1.830   train_acc= 1.000   test_loss=2.357   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.820   train_acc= 1.000   test_loss=2.345   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.812   train_acc= 1.000   test_loss=2.345   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.798   train_acc= 1.000   test_loss=2.338   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.787   train_acc= 1.000   test_loss=2.336   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.778   train_acc= 1.000   test_loss=2.294   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.772   train_acc= 1.000   test_loss=2.333   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.755   train_acc= 1.000   test_loss=2.320   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.748   train_acc= 1.000   test_loss=2.315   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.741   train_acc= 1.000   test_loss=2.336   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.730   train_acc= 1.000   test_loss=2.379   test_acc= 0.700\n",
      "epoch= 47   train_loss= 1.721   train_acc= 1.000   test_loss=2.337   test_acc= 0.800\n",
      "epoch= 48   train_loss= 1.706   train_acc= 1.000   test_loss=2.328   test_acc= 0.800\n",
      "epoch= 49   train_loss= 1.699   train_acc= 1.000   test_loss=2.305   test_acc= 0.800\n",
      "run time: 0.7414915323257446 min\n",
      "test_acc=0.800\n",
      "run= 4   fold= 1\n",
      "epoch= 0   train_loss= 2.948   train_acc= 0.622   test_loss=2.866   test_acc= 0.600\n",
      "epoch= 1   train_loss= 2.772   train_acc= 0.768   test_loss=2.782   test_acc= 0.700\n",
      "epoch= 2   train_loss= 2.677   train_acc= 0.829   test_loss=2.770   test_acc= 0.800\n",
      "epoch= 3   train_loss= 2.600   train_acc= 0.854   test_loss=2.710   test_acc= 0.800\n",
      "epoch= 4   train_loss= 2.552   train_acc= 0.890   test_loss=2.705   test_acc= 0.800\n",
      "epoch= 5   train_loss= 2.505   train_acc= 0.890   test_loss=2.669   test_acc= 0.800\n",
      "epoch= 6   train_loss= 2.467   train_acc= 0.890   test_loss=2.597   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.432   train_acc= 0.902   test_loss=2.636   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.367   train_acc= 0.939   test_loss=2.622   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.347   train_acc= 0.927   test_loss=2.553   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.329   train_acc= 0.939   test_loss=2.610   test_acc= 0.800\n",
      "epoch= 11   train_loss= 2.282   train_acc= 0.939   test_loss=2.617   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.252   train_acc= 0.976   test_loss=2.539   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.228   train_acc= 0.951   test_loss=2.665   test_acc= 0.800\n",
      "epoch= 14   train_loss= 2.181   train_acc= 0.988   test_loss=2.660   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.192   train_acc= 0.963   test_loss=2.544   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.146   train_acc= 0.976   test_loss=2.564   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.126   train_acc= 0.976   test_loss=2.561   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.095   train_acc= 0.988   test_loss=2.521   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.083   train_acc= 0.988   test_loss=2.546   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.054   train_acc= 1.000   test_loss=2.521   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.047   train_acc= 1.000   test_loss=2.553   test_acc= 0.900\n",
      "epoch= 22   train_loss= 2.022   train_acc= 1.000   test_loss=2.570   test_acc= 0.900\n",
      "epoch= 23   train_loss= 2.017   train_acc= 1.000   test_loss=2.526   test_acc= 0.900\n",
      "epoch= 24   train_loss= 2.000   train_acc= 1.000   test_loss=2.552   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.978   train_acc= 1.000   test_loss=2.507   test_acc= 0.900\n",
      "epoch= 26   train_loss= 1.971   train_acc= 1.000   test_loss=2.509   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.951   train_acc= 1.000   test_loss=2.487   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.935   train_acc= 1.000   test_loss=2.548   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.916   train_acc= 1.000   test_loss=2.507   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.915   train_acc= 1.000   test_loss=2.483   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.887   train_acc= 1.000   test_loss=2.485   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.882   train_acc= 1.000   test_loss=2.444   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.869   train_acc= 1.000   test_loss=2.446   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.854   train_acc= 1.000   test_loss=2.448   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.843   train_acc= 1.000   test_loss=2.447   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.833   train_acc= 1.000   test_loss=2.441   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.828   train_acc= 1.000   test_loss=2.454   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.809   train_acc= 1.000   test_loss=2.429   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.799   train_acc= 1.000   test_loss=2.433   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.791   train_acc= 1.000   test_loss=2.441   test_acc= 0.800\n",
      "epoch= 41   train_loss= 1.781   train_acc= 1.000   test_loss=2.441   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.773   train_acc= 1.000   test_loss=2.441   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.760   train_acc= 1.000   test_loss=2.409   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.738   train_acc= 1.000   test_loss=2.409   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.743   train_acc= 1.000   test_loss=2.392   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.723   train_acc= 1.000   test_loss=2.377   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.721   train_acc= 1.000   test_loss=2.383   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.703   train_acc= 1.000   test_loss=2.394   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.694   train_acc= 1.000   test_loss=2.355   test_acc= 0.900\n",
      "run time: 0.7068909804026285 min\n",
      "test_acc=0.900\n",
      "run= 4   fold= 2\n",
      "epoch= 0   train_loss= 2.904   train_acc= 0.663   test_loss=2.883   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.757   train_acc= 0.759   test_loss=2.792   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.709   train_acc= 0.783   test_loss=2.691   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.629   train_acc= 0.831   test_loss=2.643   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.574   train_acc= 0.855   test_loss=2.580   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.501   train_acc= 0.880   test_loss=2.522   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 6   train_loss= 2.474   train_acc= 0.928   test_loss=2.486   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.408   train_acc= 0.916   test_loss=2.483   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.386   train_acc= 0.916   test_loss=2.438   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.366   train_acc= 0.916   test_loss=2.483   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.306   train_acc= 0.952   test_loss=2.484   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.306   train_acc= 0.940   test_loss=2.488   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.229   train_acc= 0.976   test_loss=2.457   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.238   train_acc= 0.940   test_loss=2.411   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.187   train_acc= 0.988   test_loss=2.457   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.175   train_acc= 0.964   test_loss=2.387   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.158   train_acc= 0.988   test_loss=2.475   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.160   train_acc= 0.952   test_loss=2.373   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.101   train_acc= 0.988   test_loss=2.356   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.074   train_acc= 1.000   test_loss=2.415   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.075   train_acc= 0.988   test_loss=2.433   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.041   train_acc= 0.988   test_loss=2.320   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.019   train_acc= 1.000   test_loss=2.287   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.001   train_acc= 0.988   test_loss=2.254   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.997   train_acc= 1.000   test_loss=2.271   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.976   train_acc= 1.000   test_loss=2.316   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.958   train_acc= 1.000   test_loss=2.305   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.946   train_acc= 1.000   test_loss=2.304   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.930   train_acc= 1.000   test_loss=2.377   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.922   train_acc= 1.000   test_loss=2.262   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.911   train_acc= 1.000   test_loss=2.307   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.893   train_acc= 0.988   test_loss=2.353   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.881   train_acc= 1.000   test_loss=2.266   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.870   train_acc= 1.000   test_loss=2.231   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.857   train_acc= 1.000   test_loss=2.271   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.842   train_acc= 1.000   test_loss=2.321   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.832   train_acc= 1.000   test_loss=2.276   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.818   train_acc= 1.000   test_loss=2.274   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.806   train_acc= 1.000   test_loss=2.276   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.793   train_acc= 1.000   test_loss=2.188   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.780   train_acc= 1.000   test_loss=2.248   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.766   train_acc= 1.000   test_loss=2.263   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.757   train_acc= 1.000   test_loss=2.262   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.747   train_acc= 1.000   test_loss=2.230   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.737   train_acc= 1.000   test_loss=2.236   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.728   train_acc= 1.000   test_loss=2.241   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.724   train_acc= 1.000   test_loss=2.182   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.708   train_acc= 1.000   test_loss=2.190   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.699   train_acc= 1.000   test_loss=2.164   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.694   train_acc= 1.000   test_loss=2.165   test_acc= 0.778\n",
      "run time: 0.7054358164469401 min\n",
      "test_acc=0.778\n",
      "run= 4   fold= 3\n",
      "epoch= 0   train_loss= 3.016   train_acc= 0.482   test_loss=2.909   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.842   train_acc= 0.783   test_loss=2.762   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.702   train_acc= 0.819   test_loss=2.661   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.634   train_acc= 0.843   test_loss=2.562   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.573   train_acc= 0.807   test_loss=2.539   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.508   train_acc= 0.904   test_loss=2.545   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.454   train_acc= 0.904   test_loss=2.394   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.398   train_acc= 0.928   test_loss=2.511   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.425   train_acc= 0.928   test_loss=2.397   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.357   train_acc= 0.952   test_loss=2.375   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.306   train_acc= 0.952   test_loss=2.453   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.287   train_acc= 0.964   test_loss=2.360   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.247   train_acc= 0.964   test_loss=2.453   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.232   train_acc= 0.988   test_loss=2.224   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.180   train_acc= 1.000   test_loss=2.287   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.169   train_acc= 0.976   test_loss=2.158   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.150   train_acc= 0.976   test_loss=2.208   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.128   train_acc= 0.964   test_loss=2.170   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.095   train_acc= 1.000   test_loss=2.242   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.083   train_acc= 0.988   test_loss=2.203   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.070   train_acc= 1.000   test_loss=2.139   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.047   train_acc= 1.000   test_loss=2.207   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.021   train_acc= 1.000   test_loss=2.227   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.001   train_acc= 1.000   test_loss=2.174   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.989   train_acc= 1.000   test_loss=2.184   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.995   train_acc= 0.988   test_loss=2.125   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.965   train_acc= 1.000   test_loss=2.122   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.943   train_acc= 1.000   test_loss=2.124   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.940   train_acc= 1.000   test_loss=2.156   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.927   train_acc= 1.000   test_loss=2.049   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.904   train_acc= 1.000   test_loss=2.058   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.894   train_acc= 1.000   test_loss=2.081   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.891   train_acc= 1.000   test_loss=2.041   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.868   train_acc= 1.000   test_loss=2.023   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.852   train_acc= 1.000   test_loss=2.014   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.846   train_acc= 1.000   test_loss=2.040   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.835   train_acc= 1.000   test_loss=2.031   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.831   train_acc= 1.000   test_loss=1.991   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.813   train_acc= 1.000   test_loss=2.006   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.803   train_acc= 1.000   test_loss=2.059   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.787   train_acc= 1.000   test_loss=2.002   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.781   train_acc= 1.000   test_loss=2.073   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.772   train_acc= 1.000   test_loss=1.986   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.752   train_acc= 1.000   test_loss=2.009   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.744   train_acc= 1.000   test_loss=1.932   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.735   train_acc= 1.000   test_loss=1.954   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.733   train_acc= 1.000   test_loss=1.911   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.713   train_acc= 1.000   test_loss=1.879   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.709   train_acc= 1.000   test_loss=1.851   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.694   train_acc= 1.000   test_loss=1.862   test_acc= 0.889\n",
      "run time: 0.6977852861086528 min\n",
      "test_acc=0.889\n",
      "run= 4   fold= 4\n",
      "epoch= 0   train_loss= 2.999   train_acc= 0.470   test_loss=2.946   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.818   train_acc= 0.675   test_loss=2.848   test_acc= 0.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2   train_loss= 2.699   train_acc= 0.855   test_loss=2.813   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.655   train_acc= 0.807   test_loss=2.719   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.545   train_acc= 0.880   test_loss=2.697   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.500   train_acc= 0.904   test_loss=2.745   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.469   train_acc= 0.892   test_loss=2.643   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.425   train_acc= 0.904   test_loss=2.623   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.401   train_acc= 0.904   test_loss=2.608   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.341   train_acc= 0.952   test_loss=2.649   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.309   train_acc= 0.940   test_loss=2.651   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.284   train_acc= 0.940   test_loss=2.650   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.252   train_acc= 0.952   test_loss=2.635   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.202   train_acc= 0.976   test_loss=2.656   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.194   train_acc= 0.964   test_loss=2.722   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.159   train_acc= 0.988   test_loss=2.724   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.126   train_acc= 1.000   test_loss=2.716   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.100   train_acc= 0.988   test_loss=2.691   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.106   train_acc= 0.976   test_loss=2.666   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.070   train_acc= 1.000   test_loss=2.664   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.072   train_acc= 0.976   test_loss=2.668   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.049   train_acc= 1.000   test_loss=2.713   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.032   train_acc= 0.988   test_loss=2.820   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.003   train_acc= 1.000   test_loss=2.749   test_acc= 0.778\n",
      "epoch= 24   train_loss= 2.004   train_acc= 1.000   test_loss=2.677   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.969   train_acc= 1.000   test_loss=2.687   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.960   train_acc= 1.000   test_loss=2.669   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.949   train_acc= 1.000   test_loss=2.721   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.939   train_acc= 1.000   test_loss=2.731   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.912   train_acc= 1.000   test_loss=2.715   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.904   train_acc= 1.000   test_loss=2.720   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.890   train_acc= 1.000   test_loss=2.668   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.879   train_acc= 1.000   test_loss=2.645   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.879   train_acc= 0.988   test_loss=2.746   test_acc= 0.667\n",
      "epoch= 34   train_loss= 1.852   train_acc= 1.000   test_loss=2.628   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.843   train_acc= 1.000   test_loss=2.653   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.830   train_acc= 1.000   test_loss=2.608   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.816   train_acc= 1.000   test_loss=2.544   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.806   train_acc= 1.000   test_loss=2.599   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.793   train_acc= 1.000   test_loss=2.560   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.790   train_acc= 1.000   test_loss=2.554   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.776   train_acc= 1.000   test_loss=2.567   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.769   train_acc= 1.000   test_loss=2.600   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.752   train_acc= 1.000   test_loss=2.564   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.758   train_acc= 1.000   test_loss=2.498   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.733   train_acc= 1.000   test_loss=2.555   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.724   train_acc= 1.000   test_loss=2.545   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.715   train_acc= 1.000   test_loss=2.535   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.710   train_acc= 1.000   test_loss=2.563   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.693   train_acc= 1.000   test_loss=2.533   test_acc= 0.778\n",
      "run time: 0.7276596983273824 min\n",
      "test_acc=0.778\n",
      "run= 4   fold= 5\n",
      "epoch= 0   train_loss= 2.974   train_acc= 0.566   test_loss=2.874   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.847   train_acc= 0.771   test_loss=2.777   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.696   train_acc= 0.831   test_loss=2.719   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.595   train_acc= 0.855   test_loss=2.641   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.526   train_acc= 0.867   test_loss=2.607   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.512   train_acc= 0.855   test_loss=2.550   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.515   train_acc= 0.831   test_loss=2.492   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.421   train_acc= 0.916   test_loss=2.482   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.402   train_acc= 0.916   test_loss=2.447   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.359   train_acc= 0.940   test_loss=2.405   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.333   train_acc= 0.928   test_loss=2.372   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.269   train_acc= 0.952   test_loss=2.330   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.271   train_acc= 0.940   test_loss=2.296   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.233   train_acc= 0.964   test_loss=2.320   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.184   train_acc= 0.964   test_loss=2.296   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.176   train_acc= 0.964   test_loss=2.285   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.132   train_acc= 0.976   test_loss=2.243   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.132   train_acc= 0.976   test_loss=2.233   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.110   train_acc= 0.976   test_loss=2.212   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.079   train_acc= 0.988   test_loss=2.219   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.051   train_acc= 1.000   test_loss=2.186   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.041   train_acc= 0.988   test_loss=2.190   test_acc= 0.889\n",
      "epoch= 22   train_loss= 2.045   train_acc= 1.000   test_loss=2.115   test_acc= 0.889\n",
      "epoch= 23   train_loss= 2.006   train_acc= 1.000   test_loss=2.117   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.985   train_acc= 1.000   test_loss=2.116   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.977   train_acc= 1.000   test_loss=2.077   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.965   train_acc= 1.000   test_loss=2.080   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.940   train_acc= 1.000   test_loss=2.073   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.930   train_acc= 1.000   test_loss=2.074   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.917   train_acc= 1.000   test_loss=2.058   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.906   train_acc= 1.000   test_loss=2.058   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.891   train_acc= 1.000   test_loss=2.027   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.878   train_acc= 1.000   test_loss=2.020   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.874   train_acc= 1.000   test_loss=2.059   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.852   train_acc= 1.000   test_loss=2.018   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.840   train_acc= 1.000   test_loss=2.003   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.828   train_acc= 1.000   test_loss=1.983   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.830   train_acc= 1.000   test_loss=1.979   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.806   train_acc= 1.000   test_loss=1.961   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.802   train_acc= 1.000   test_loss=1.982   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.789   train_acc= 1.000   test_loss=1.976   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.788   train_acc= 1.000   test_loss=1.932   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.758   train_acc= 1.000   test_loss=1.911   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.748   train_acc= 1.000   test_loss=1.894   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.744   train_acc= 1.000   test_loss=1.885   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.733   train_acc= 1.000   test_loss=1.882   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.729   train_acc= 1.000   test_loss=1.866   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.721   train_acc= 1.000   test_loss=1.845   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 48   train_loss= 1.698   train_acc= 1.000   test_loss=1.844   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.693   train_acc= 1.000   test_loss=1.871   test_acc= 0.889\n",
      "run time: 0.7094895680745442 min\n",
      "test_acc=0.889\n",
      "run= 4   fold= 6\n",
      "epoch= 0   train_loss= 2.918   train_acc= 0.711   test_loss=2.843   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.835   train_acc= 0.711   test_loss=2.783   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.721   train_acc= 0.819   test_loss=2.692   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.669   train_acc= 0.807   test_loss=2.647   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.582   train_acc= 0.867   test_loss=2.607   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.572   train_acc= 0.855   test_loss=2.546   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.479   train_acc= 0.892   test_loss=2.504   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.506   train_acc= 0.855   test_loss=2.493   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.419   train_acc= 0.940   test_loss=2.471   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.367   train_acc= 0.916   test_loss=2.455   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.331   train_acc= 0.940   test_loss=2.437   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.310   train_acc= 0.928   test_loss=2.424   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.246   train_acc= 0.952   test_loss=2.400   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.256   train_acc= 0.940   test_loss=2.395   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.212   train_acc= 0.964   test_loss=2.409   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.232   train_acc= 0.940   test_loss=2.358   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.169   train_acc= 0.964   test_loss=2.353   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.129   train_acc= 0.976   test_loss=2.366   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.120   train_acc= 0.976   test_loss=2.349   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.093   train_acc= 0.988   test_loss=2.377   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.098   train_acc= 0.976   test_loss=2.325   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.046   train_acc= 0.988   test_loss=2.315   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.037   train_acc= 1.000   test_loss=2.289   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.051   train_acc= 0.964   test_loss=2.326   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.999   train_acc= 1.000   test_loss=2.284   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.995   train_acc= 1.000   test_loss=2.300   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.997   train_acc= 0.988   test_loss=2.209   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.982   train_acc= 0.988   test_loss=2.226   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.963   train_acc= 1.000   test_loss=2.222   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.928   train_acc= 1.000   test_loss=2.225   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.913   train_acc= 1.000   test_loss=2.213   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.898   train_acc= 1.000   test_loss=2.181   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.899   train_acc= 1.000   test_loss=2.195   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.912   train_acc= 0.988   test_loss=2.134   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.865   train_acc= 1.000   test_loss=2.176   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.859   train_acc= 1.000   test_loss=2.184   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.839   train_acc= 1.000   test_loss=2.156   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.833   train_acc= 1.000   test_loss=2.137   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.824   train_acc= 1.000   test_loss=2.128   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.811   train_acc= 1.000   test_loss=2.128   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.798   train_acc= 1.000   test_loss=2.134   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.792   train_acc= 1.000   test_loss=2.130   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.791   train_acc= 1.000   test_loss=2.077   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.764   train_acc= 1.000   test_loss=2.087   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.760   train_acc= 1.000   test_loss=2.091   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.741   train_acc= 1.000   test_loss=2.040   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.733   train_acc= 1.000   test_loss=2.041   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.724   train_acc= 1.000   test_loss=2.026   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.717   train_acc= 1.000   test_loss=2.033   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.703   train_acc= 1.000   test_loss=2.015   test_acc= 0.889\n",
      "run time: 0.7070579965909322 min\n",
      "test_acc=0.889\n",
      "run= 4   fold= 7\n",
      "epoch= 0   train_loss= 2.930   train_acc= 0.687   test_loss=2.867   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.764   train_acc= 0.807   test_loss=2.802   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.648   train_acc= 0.867   test_loss=2.724   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.584   train_acc= 0.867   test_loss=2.788   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.554   train_acc= 0.867   test_loss=2.719   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.501   train_acc= 0.831   test_loss=2.760   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.403   train_acc= 0.928   test_loss=2.785   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.409   train_acc= 0.892   test_loss=2.851   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.346   train_acc= 0.940   test_loss=2.878   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.318   train_acc= 0.952   test_loss=3.102   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.274   train_acc= 0.964   test_loss=3.055   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.243   train_acc= 0.952   test_loss=3.038   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.211   train_acc= 0.976   test_loss=3.065   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.206   train_acc= 0.988   test_loss=3.117   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.158   train_acc= 0.964   test_loss=3.095   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.145   train_acc= 0.988   test_loss=3.386   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.128   train_acc= 0.964   test_loss=3.368   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.094   train_acc= 0.988   test_loss=3.206   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.084   train_acc= 0.988   test_loss=3.301   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.062   train_acc= 0.988   test_loss=3.328   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.039   train_acc= 1.000   test_loss=3.289   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.031   train_acc= 1.000   test_loss=3.357   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.000   train_acc= 1.000   test_loss=3.272   test_acc= 0.778\n",
      "epoch= 23   train_loss= 2.002   train_acc= 0.988   test_loss=3.284   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.980   train_acc= 1.000   test_loss=3.244   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.971   train_acc= 1.000   test_loss=3.369   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.945   train_acc= 1.000   test_loss=3.510   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.945   train_acc= 1.000   test_loss=3.286   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.931   train_acc= 1.000   test_loss=3.369   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.910   train_acc= 1.000   test_loss=3.411   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.893   train_acc= 1.000   test_loss=3.510   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.884   train_acc= 1.000   test_loss=3.345   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.872   train_acc= 1.000   test_loss=3.262   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.864   train_acc= 1.000   test_loss=3.310   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.849   train_acc= 1.000   test_loss=3.298   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.830   train_acc= 1.000   test_loss=3.305   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.829   train_acc= 1.000   test_loss=3.348   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.811   train_acc= 1.000   test_loss=3.380   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.805   train_acc= 1.000   test_loss=3.324   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.789   train_acc= 1.000   test_loss=3.345   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.777   train_acc= 1.000   test_loss=3.374   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.769   train_acc= 1.000   test_loss=3.299   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.764   train_acc= 1.000   test_loss=3.342   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.745   train_acc= 1.000   test_loss=3.325   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 44   train_loss= 1.732   train_acc= 1.000   test_loss=3.326   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.726   train_acc= 1.000   test_loss=3.319   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.710   train_acc= 1.000   test_loss=3.318   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.705   train_acc= 1.000   test_loss=3.291   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.696   train_acc= 1.000   test_loss=3.289   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.684   train_acc= 1.000   test_loss=3.270   test_acc= 0.778\n",
      "run time: 0.7162322839101155 min\n",
      "test_acc=0.778\n",
      "run= 4   fold= 8\n",
      "epoch= 0   train_loss= 3.014   train_acc= 0.627   test_loss=2.805   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.784   train_acc= 0.735   test_loss=2.753   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.655   train_acc= 0.855   test_loss=2.701   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.597   train_acc= 0.867   test_loss=2.692   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.503   train_acc= 0.880   test_loss=2.694   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.481   train_acc= 0.892   test_loss=2.643   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.412   train_acc= 0.904   test_loss=2.657   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.374   train_acc= 0.952   test_loss=2.630   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.368   train_acc= 0.916   test_loss=2.639   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.290   train_acc= 0.952   test_loss=2.736   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.280   train_acc= 0.952   test_loss=2.706   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.232   train_acc= 0.952   test_loss=2.712   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.232   train_acc= 0.952   test_loss=2.684   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.198   train_acc= 0.976   test_loss=2.797   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.177   train_acc= 0.976   test_loss=2.774   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.137   train_acc= 0.988   test_loss=2.755   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.115   train_acc= 0.988   test_loss=2.744   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.096   train_acc= 1.000   test_loss=2.758   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.087   train_acc= 1.000   test_loss=2.886   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.084   train_acc= 0.988   test_loss=2.845   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.059   train_acc= 0.976   test_loss=2.877   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.023   train_acc= 1.000   test_loss=2.935   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.020   train_acc= 0.976   test_loss=2.936   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.997   train_acc= 1.000   test_loss=2.869   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.986   train_acc= 1.000   test_loss=2.884   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.985   train_acc= 1.000   test_loss=2.867   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.945   train_acc= 1.000   test_loss=2.955   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.945   train_acc= 1.000   test_loss=2.891   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.916   train_acc= 1.000   test_loss=2.885   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.909   train_acc= 1.000   test_loss=3.069   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.899   train_acc= 1.000   test_loss=2.932   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.888   train_acc= 1.000   test_loss=3.033   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.878   train_acc= 0.988   test_loss=2.872   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.859   train_acc= 1.000   test_loss=2.990   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.842   train_acc= 1.000   test_loss=3.059   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.846   train_acc= 1.000   test_loss=2.997   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.831   train_acc= 1.000   test_loss=2.919   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.810   train_acc= 1.000   test_loss=3.035   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.797   train_acc= 1.000   test_loss=3.014   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.797   train_acc= 1.000   test_loss=3.149   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.782   train_acc= 1.000   test_loss=3.017   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.770   train_acc= 1.000   test_loss=3.026   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.756   train_acc= 1.000   test_loss=3.062   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.747   train_acc= 1.000   test_loss=3.065   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.735   train_acc= 1.000   test_loss=3.042   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.728   train_acc= 1.000   test_loss=2.984   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.711   train_acc= 1.000   test_loss=3.016   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.703   train_acc= 1.000   test_loss=2.967   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.690   train_acc= 1.000   test_loss=3.007   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.686   train_acc= 1.000   test_loss=3.021   test_acc= 0.778\n",
      "run time: 0.701923664410909 min\n",
      "test_acc=0.778\n",
      "run= 4   fold= 9\n",
      "epoch= 0   train_loss= 2.994   train_acc= 0.566   test_loss=2.973   test_acc= 0.333\n",
      "epoch= 1   train_loss= 2.859   train_acc= 0.687   test_loss=2.828   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.747   train_acc= 0.795   test_loss=2.806   test_acc= 0.556\n",
      "epoch= 3   train_loss= 2.690   train_acc= 0.855   test_loss=2.739   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.646   train_acc= 0.831   test_loss=2.643   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.566   train_acc= 0.880   test_loss=2.612   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.493   train_acc= 0.892   test_loss=2.536   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.423   train_acc= 0.928   test_loss=2.572   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.415   train_acc= 0.940   test_loss=2.409   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.370   train_acc= 0.928   test_loss=2.409   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.392   train_acc= 0.916   test_loss=2.414   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.322   train_acc= 0.940   test_loss=2.361   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.268   train_acc= 0.964   test_loss=2.315   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.263   train_acc= 0.952   test_loss=2.259   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.240   train_acc= 0.964   test_loss=2.269   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.198   train_acc= 0.988   test_loss=2.194   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.189   train_acc= 0.940   test_loss=2.257   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.174   train_acc= 0.952   test_loss=2.210   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.175   train_acc= 0.964   test_loss=2.163   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.157   train_acc= 0.928   test_loss=2.194   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.097   train_acc= 0.976   test_loss=2.166   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.077   train_acc= 0.976   test_loss=2.155   test_acc= 1.000\n",
      "epoch= 22   train_loss= 2.069   train_acc= 1.000   test_loss=2.087   test_acc= 1.000\n",
      "epoch= 23   train_loss= 2.036   train_acc= 0.988   test_loss=2.188   test_acc= 0.889\n",
      "epoch= 24   train_loss= 2.035   train_acc= 0.988   test_loss=2.075   test_acc= 1.000\n",
      "epoch= 25   train_loss= 2.025   train_acc= 0.988   test_loss=2.071   test_acc= 1.000\n",
      "epoch= 26   train_loss= 2.006   train_acc= 1.000   test_loss=2.142   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.976   train_acc= 1.000   test_loss=2.091   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.974   train_acc= 0.988   test_loss=2.048   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.956   train_acc= 1.000   test_loss=2.075   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.933   train_acc= 1.000   test_loss=1.995   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.918   train_acc= 1.000   test_loss=2.022   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.911   train_acc= 1.000   test_loss=1.976   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.904   train_acc= 0.976   test_loss=1.983   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.878   train_acc= 1.000   test_loss=1.946   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.872   train_acc= 1.000   test_loss=1.953   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.861   train_acc= 1.000   test_loss=1.927   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.848   train_acc= 1.000   test_loss=1.911   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.835   train_acc= 1.000   test_loss=1.894   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.819   train_acc= 1.000   test_loss=1.890   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 40   train_loss= 1.807   train_acc= 1.000   test_loss=1.882   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.796   train_acc= 1.000   test_loss=1.873   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.787   train_acc= 1.000   test_loss=1.834   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.777   train_acc= 1.000   test_loss=1.833   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.771   train_acc= 1.000   test_loss=1.803   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.756   train_acc= 1.000   test_loss=1.808   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.751   train_acc= 1.000   test_loss=1.796   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.734   train_acc= 1.000   test_loss=1.791   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.733   train_acc= 1.000   test_loss=1.797   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.719   train_acc= 1.000   test_loss=1.780   test_acc= 1.000\n",
      "run time: 0.7182445844014486 min\n",
      "test_acc=1.000\n",
      "mi-net mean accuracy =  0.8300000059604645\n",
      "std =  0.10402396848155698\n"
     ]
    }
   ],
   "source": [
    "# perform five times 10-fold cross-validation experiments\n",
    "run = 5\n",
    "n_folds = 10\n",
    "acc = np.zeros((run, n_folds), dtype=np.float32)\n",
    "for irun in range(run):\n",
    "    dataset = load_dataset('musk1', n_folds)\n",
    "    for ifold in range(n_folds):\n",
    "        print('run=', irun, '  fold=', ifold)\n",
    "        acc[irun][ifold] = mi_Net(dataset[ifold])\n",
    "print('mi-net mean accuracy = ', np.mean(acc))\n",
    "print('std = ', np.std(acc))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "qkCJLi5nxsiP"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
