{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5XLSnFHrxIc"
   },
   "source": [
    "## Machine Learning project\n",
    "[MI-net, instance space](#section_id)\n",
    "[MI-net deep supervision, instance space](#MInetdeepsuper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBvZXBosr_uG",
    "outputId": "1936e27c-5e3f-4272-b413-f602a4b73b54",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/chlorochrule/cknn (from -r requirements.txt (line 9))\n",
      "  Cloning https://github.com/chlorochrule/cknn to /tmp/pip-req-build-wsl8xmq4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/chlorochrule/cknn /tmp/pip-req-build-wsl8xmq4\n",
      "  Resolved https://github.com/chlorochrule/cknn to commit 7d05c5049da72a573bd486fca6647f8b0376243c\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting misvm\n",
      "  Cloning https://github.com/garydoranjr/misvm.git to /tmp/pip-install-3fnb55yk/misvm_c76dfc420c444166aa160bbde23ddccf\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/garydoranjr/misvm.git /tmp/pip-install-3fnb55yk/misvm_c76dfc420c444166aa160bbde23ddccf\n",
      "  Resolved https://github.com/garydoranjr/misvm.git to commit b2118fe04d98c00436bdf8a0e4bbfb6082c5751c\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting git+https://github.com/tibovanheule/MINNs (from -r requirements.txt (line 13))\n",
      "  Cloning https://github.com/tibovanheule/MINNs to /tmp/pip-req-build-j79qir78\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/tibovanheule/MINNs /tmp/pip-req-build-j79qir78\n",
      "  Resolved https://github.com/tibovanheule/MINNs to commit bba94bbfd67056a2bc1d2a2d1a3348a05c623b02\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: keras in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (2.11.0)\n",
      "Requirement already satisfied: sklearn in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.0.post1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.23.5)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (3.6.2)\n",
      "Requirement already satisfied: jupyter in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.0.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.9.3)\n",
      "Requirement already satisfied: seaborn in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: tensorflow in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (2.11.0)\n",
      "Requirement already satisfied: cvxopt in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 4)) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./venv/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 5)) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 5)) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 5)) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./venv/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: nbconvert in ./venv/lib/python3.8/site-packages (from jupyter->-r requirements.txt (line 6)) (7.2.5)\n",
      "Requirement already satisfied: ipywidgets in ./venv/lib/python3.8/site-packages (from jupyter->-r requirements.txt (line 6)) (8.0.2)\n",
      "Requirement already satisfied: jupyter-console in ./venv/lib/python3.8/site-packages (from jupyter->-r requirements.txt (line 6)) (6.4.4)\n",
      "Requirement already satisfied: notebook in ./venv/lib/python3.8/site-packages (from jupyter->-r requirements.txt (line 6)) (6.5.2)\n",
      "Requirement already satisfied: qtconsole in ./venv/lib/python3.8/site-packages (from jupyter->-r requirements.txt (line 6)) (5.4.0)\n",
      "Requirement already satisfied: ipykernel in ./venv/lib/python3.8/site-packages (from jupyter->-r requirements.txt (line 6)) (6.17.1)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.8/site-packages (from cknn==0.1.0->-r requirements.txt (line 9)) (1.1.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (2.11.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (1.50.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (3.19.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (2.1.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (22.11.23)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (0.28.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (14.0.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (65.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (3.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (1.14.1)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in ./venv/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 10)) (2.11.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.8/site-packages (from mil-nets==0.1->-r requirements.txt (line 13)) (5.9.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 10)) (0.38.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./venv/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./venv/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./venv/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./venv/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (2.14.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./venv/lib/python3.8/site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (0.1.6)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./venv/lib/python3.8/site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (8.7.0)\n",
      "Requirement already satisfied: tornado>=6.1 in ./venv/lib/python3.8/site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (6.2)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in ./venv/lib/python3.8/site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (5.5.0)\n",
      "Requirement already satisfied: pyzmq>=17 in ./venv/lib/python3.8/site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (24.0.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in ./venv/lib/python3.8/site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (1.6.4)\n",
      "Requirement already satisfied: nest-asyncio in ./venv/lib/python3.8/site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (1.5.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./venv/lib/python3.8/site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (7.4.7)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in ./venv/lib/python3.8/site-packages (from ipywidgets->jupyter->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in ./venv/lib/python3.8/site-packages (from ipywidgets->jupyter->-r requirements.txt (line 6)) (4.0.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in ./venv/lib/python3.8/site-packages (from jupyter-console->jupyter->-r requirements.txt (line 6)) (3.0.33)\n",
      "Requirement already satisfied: pygments in ./venv/lib/python3.8/site-packages (from jupyter-console->jupyter->-r requirements.txt (line 6)) (2.13.0)\n",
      "Requirement already satisfied: jinja2>=3.0 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (2.0.4)\n",
      "Requirement already satisfied: tinycss2 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (1.2.1)\n",
      "Requirement already satisfied: defusedxml in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (5.1.0)\n",
      "Requirement already satisfied: bleach in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (5.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (4.11.1)\n",
      "Requirement already satisfied: markupsafe>=2.0 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (2.1.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: nbformat>=5.1 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (5.7.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (1.5.0)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (5.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in ./venv/lib/python3.8/site-packages (from notebook->jupyter->-r requirements.txt (line 6)) (0.17.0)\n",
      "Requirement already satisfied: argon2-cffi in ./venv/lib/python3.8/site-packages (from notebook->jupyter->-r requirements.txt (line 6)) (21.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in ./venv/lib/python3.8/site-packages (from notebook->jupyter->-r requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in ./venv/lib/python3.8/site-packages (from notebook->jupyter->-r requirements.txt (line 6)) (0.15.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in ./venv/lib/python3.8/site-packages (from notebook->jupyter->-r requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: ipython-genutils in ./venv/lib/python3.8/site-packages (from notebook->jupyter->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in ./venv/lib/python3.8/site-packages (from qtconsole->jupyter->-r requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in ./venv/lib/python3.8/site-packages (from scikit-learn->cknn==0.1.0->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.8/site-packages (from scikit-learn->cknn==0.1.0->-r requirements.txt (line 9)) (3.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./venv/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.8/site-packages (from importlib-metadata>=3.6->nbconvert->jupyter->-r requirements.txt (line 6)) (3.11.0)\n",
      "Requirement already satisfied: pickleshare in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.7.5)\n",
      "Requirement already satisfied: stack-data in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.6.2)\n",
      "Requirement already satisfied: backcall in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.18.2)\n",
      "Requirement already satisfied: entrypoints in ./venv/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->-r requirements.txt (line 6)) (0.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./venv/lib/python3.8/site-packages (from jupyter-core>=4.7->nbconvert->jupyter->-r requirements.txt (line 6)) (2.5.4)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in ./venv/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook->jupyter->-r requirements.txt (line 6)) (1.23.3)\n",
      "Requirement already satisfied: notebook-shim>=0.1.0 in ./venv/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook->jupyter->-r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: fastjsonschema in ./venv/lib/python3.8/site-packages (from nbformat>=5.1->nbconvert->jupyter->-r requirements.txt (line 6)) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in ./venv/lib/python3.8/site-packages (from nbformat>=5.1->nbconvert->jupyter->-r requirements.txt (line 6)) (4.17.1)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->-r requirements.txt (line 6)) (0.2.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (2022.9.24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptyprocess in ./venv/lib/python3.8/site-packages (from terminado>=0.8.3->notebook->jupyter->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in ./venv/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter->-r requirements.txt (line 6)) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.8/site-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 6)) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in ./venv/lib/python3.8/site-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 6)) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./venv/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.8.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in ./venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->-r requirements.txt (line 6)) (0.19.2)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in ./venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->-r requirements.txt (line 6)) (5.10.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in ./venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->-r requirements.txt (line 6)) (1.3.10)\n",
      "Requirement already satisfied: attrs>=17.4.0 in ./venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->-r requirements.txt (line 6)) (22.1.0)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in ./venv/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->-r requirements.txt (line 6)) (3.6.2)\n",
      "Requirement already satisfied: websocket-client in ./venv/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->-r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 10)) (3.2.2)\n",
      "Requirement already satisfied: cffi>=1.0.1 in ./venv/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r requirements.txt (line 6)) (1.15.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.8/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r requirements.txt (line 6)) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75T7R_ehr3MU"
   },
   "source": [
    "### Embedded-Space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI-net\n",
    "<a id='section_id'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "_sDih52xsYL9",
    "outputId": "6e803c46-abda-4946-cbbe-6098086e21da"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from random import shuffle\n",
    "import argparse\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Layer, Dropout\n",
    "\n",
    "from mil_nets.dataset import load_dataset\n",
    "from mil_nets.layer import Feature_pooling\n",
    "from mil_nets.metrics import bag_accuracy\n",
    "from mil_nets.objectives import bag_loss\n",
    "from mil_nets.utils import convertToBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xRf0rM6oahO",
    "outputId": "f0adfdcc-8304-439b-97fd-da021eb8608e"
   },
   "outputs": [],
   "source": [
    "def test_eval(model, test_set):\n",
    "    \"\"\"Evaluate on testing set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training MI-Net model.\n",
    "    test_set : list\n",
    "        A list of testing set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on testing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_test_batch = len(test_set)\n",
    "    test_loss = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    test_acc = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    for ibatch, batch in enumerate(test_set):\n",
    "        result = model.test_on_batch({'input':batch[0].astype(np.float32)}, {'fp':batch[1].astype(np.float32)})\n",
    "        test_loss[ibatch] = result[0]\n",
    "        test_acc[ibatch][0] = result[1]\n",
    "    return np.mean(test_loss), np.mean(test_acc)\n",
    "\n",
    "def train_eval(model, train_set):\n",
    "    \"\"\"Evaluate on training set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training MI-Net model.\n",
    "    train_set : list\n",
    "        A list of training set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on traing set..astype(np.float32)\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_train_batch = len(train_set)\n",
    "    train_loss = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    train_acc = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    shuffle(train_set)\n",
    "    for ibatch, batch in enumerate(train_set):\n",
    "        result = model.train_on_batch({'input':batch[0].astype(np.float32)}, {'fp':batch[1].astype(np.float32)})\n",
    "        train_loss[ibatch] = result[0]\n",
    "        train_acc[ibatch][0] = result[1]\n",
    "    return np.mean(train_loss), np.mean(train_acc)\n",
    "\n",
    "def MI_Net(dataset):\n",
    "    \"\"\"Train and evaluate on MI-Net.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    dataset : dict\n",
    "        A dictionary contains all dataset information. We split train/test by keys.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_acc : float\n",
    "        Testing accuracy of MI-Net.\n",
    "    \"\"\"\n",
    "    weight_decay=0.005\n",
    "    init_lr=5e-4\n",
    "    pooling_mode='max'\n",
    "    momentum=0.9\n",
    "    max_epoch=50\n",
    "    # load data and convert type\n",
    "    train_bags = dataset['train']\n",
    "    test_bags = dataset['test']\n",
    "\n",
    "    # convert bag to batch\n",
    "    train_set = convertToBatch(train_bags)\n",
    "    test_set = convertToBatch(test_bags)\n",
    "    dimension = train_set[0][0].shape[1]\n",
    "\n",
    "    # data: instance feature, n*d, n = number of training instance\n",
    "    data_input = Input(shape=(dimension,), dtype='float32', name='input')\n",
    "\n",
    "    # fully-connected\n",
    "    fc1 = Dense(256, activation='relu', kernel_regularizer=l2(weight_decay))(data_input)\n",
    "    fc2 = Dense(128, activation='relu', kernel_regularizer=l2(weight_decay))(fc1)\n",
    "    fc3 = Dense(64, activation='relu', kernel_regularizer=l2(weight_decay))(fc2)\n",
    "\n",
    "    # dropout\n",
    "    dropout = Dropout(rate=0.5)(fc3)\n",
    "\n",
    "    # features pooling\n",
    "    fp = Feature_pooling(output_dim=1, kernel_regularizer=l2(weight_decay), pooling_mode=pooling_mode, name='fp')(dropout)\n",
    "\n",
    "    model = Model(inputs=[data_input], outputs=[fp])\n",
    "    sgd = SGD(lr=init_lr, decay=1e-4, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss=bag_loss, optimizer=sgd, metrics=[bag_accuracy])\n",
    "\n",
    "    # train model\n",
    "    t1 = time.time()\n",
    "    num_batch = len(train_set)\n",
    "    for epoch in range(max_epoch):\n",
    "        train_loss, train_acc = train_eval(model, train_set)\n",
    "        test_loss, test_acc = test_eval(model, test_set)\n",
    "        print('epoch=', epoch, '  train_loss= {:.3f}'.format(train_loss), '  train_acc= {:.3f}'.format(train_acc), '  test_loss={:.3f}'.format(test_loss), '  test_acc= {:.3f}'.format(test_acc))\n",
    "    t2 = time.time()\n",
    "    print('run time:', (t2-t1) / 60, 'min')\n",
    "    print('test_acc={:.3f}'.format(test_acc))\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xRf0rM6oahO",
    "outputId": "f0adfdcc-8304-439b-97fd-da021eb8608e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run= 0   fold= 0\n",
      "epoch= 0   train_loss= 3.015   train_acc= 0.610   test_loss=2.832   test_acc= 0.600\n",
      "epoch= 1   train_loss= 2.635   train_acc= 0.841   test_loss=2.704   test_acc= 0.800\n",
      "epoch= 2   train_loss= 2.479   train_acc= 0.915   test_loss=2.624   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.404   train_acc= 0.963   test_loss=2.583   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.368   train_acc= 0.951   test_loss=2.574   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.295   train_acc= 0.988   test_loss=2.524   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.256   train_acc= 1.000   test_loss=2.543   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.275   train_acc= 0.963   test_loss=2.460   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.230   train_acc= 0.988   test_loss=2.489   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.205   train_acc= 1.000   test_loss=2.468   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.190   train_acc= 0.988   test_loss=2.462   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.166   train_acc= 0.988   test_loss=2.460   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.127   train_acc= 1.000   test_loss=2.476   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.114   train_acc= 1.000   test_loss=2.451   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.109   train_acc= 1.000   test_loss=2.439   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.088   train_acc= 1.000   test_loss=2.417   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.070   train_acc= 1.000   test_loss=2.418   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.046   train_acc= 1.000   test_loss=2.411   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.034   train_acc= 1.000   test_loss=2.405   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.022   train_acc= 1.000   test_loss=2.399   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.009   train_acc= 1.000   test_loss=2.380   test_acc= 0.900\n",
      "epoch= 21   train_loss= 1.988   train_acc= 1.000   test_loss=2.387   test_acc= 0.900\n",
      "epoch= 22   train_loss= 1.977   train_acc= 1.000   test_loss=2.372   test_acc= 0.900\n",
      "epoch= 23   train_loss= 1.959   train_acc= 1.000   test_loss=2.365   test_acc= 0.900\n",
      "epoch= 24   train_loss= 1.953   train_acc= 1.000   test_loss=2.340   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.937   train_acc= 1.000   test_loss=2.330   test_acc= 0.900\n",
      "epoch= 26   train_loss= 1.924   train_acc= 1.000   test_loss=2.344   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.916   train_acc= 1.000   test_loss=2.330   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.906   train_acc= 1.000   test_loss=2.316   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.906   train_acc= 1.000   test_loss=2.275   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.876   train_acc= 1.000   test_loss=2.266   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.866   train_acc= 1.000   test_loss=2.254   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.853   train_acc= 1.000   test_loss=2.247   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.839   train_acc= 1.000   test_loss=2.243   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.830   train_acc= 1.000   test_loss=2.233   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.816   train_acc= 1.000   test_loss=2.229   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.805   train_acc= 1.000   test_loss=2.222   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.794   train_acc= 1.000   test_loss=2.214   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.785   train_acc= 1.000   test_loss=2.211   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.774   train_acc= 1.000   test_loss=2.202   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.763   train_acc= 1.000   test_loss=2.196   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.753   train_acc= 1.000   test_loss=2.187   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.742   train_acc= 1.000   test_loss=2.182   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.732   train_acc= 1.000   test_loss=2.180   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.722   train_acc= 1.000   test_loss=2.162   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.711   train_acc= 1.000   test_loss=2.154   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.702   train_acc= 1.000   test_loss=2.147   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.691   train_acc= 1.000   test_loss=2.142   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.685   train_acc= 1.000   test_loss=2.134   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.675   train_acc= 1.000   test_loss=2.128   test_acc= 0.900\n",
      "run time: 0.4443228244781494 min\n",
      "test_acc=0.900\n",
      "run= 0   fold= 1\n",
      "epoch= 0   train_loss= 2.944   train_acc= 0.561   test_loss=2.811   test_acc= 0.700\n",
      "epoch= 1   train_loss= 2.659   train_acc= 0.878   test_loss=2.739   test_acc= 0.800\n",
      "epoch= 2   train_loss= 2.511   train_acc= 0.902   test_loss=2.721   test_acc= 0.800\n",
      "epoch= 3   train_loss= 2.448   train_acc= 0.927   test_loss=2.684   test_acc= 0.800\n",
      "epoch= 4   train_loss= 2.392   train_acc= 0.951   test_loss=2.653   test_acc= 0.800\n",
      "epoch= 5   train_loss= 2.348   train_acc= 0.963   test_loss=2.748   test_acc= 0.800\n",
      "epoch= 6   train_loss= 2.304   train_acc= 0.988   test_loss=2.597   test_acc= 0.800\n",
      "epoch= 7   train_loss= 2.262   train_acc= 0.988   test_loss=2.539   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.220   train_acc= 1.000   test_loss=2.564   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.226   train_acc= 0.988   test_loss=2.562   test_acc= 0.800\n",
      "epoch= 10   train_loss= 2.185   train_acc= 0.988   test_loss=2.606   test_acc= 0.800\n",
      "epoch= 11   train_loss= 2.156   train_acc= 1.000   test_loss=2.577   test_acc= 0.800\n",
      "epoch= 12   train_loss= 2.132   train_acc= 1.000   test_loss=2.584   test_acc= 0.800\n",
      "epoch= 13   train_loss= 2.123   train_acc= 1.000   test_loss=2.515   test_acc= 0.800\n",
      "epoch= 14   train_loss= 2.109   train_acc= 1.000   test_loss=2.454   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.088   train_acc= 0.988   test_loss=2.408   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.085   train_acc= 0.988   test_loss=2.441   test_acc= 0.800\n",
      "epoch= 17   train_loss= 2.059   train_acc= 1.000   test_loss=2.506   test_acc= 0.800\n",
      "epoch= 18   train_loss= 2.041   train_acc= 1.000   test_loss=2.420   test_acc= 0.800\n",
      "epoch= 19   train_loss= 2.022   train_acc= 1.000   test_loss=2.419   test_acc= 0.800\n",
      "epoch= 20   train_loss= 2.011   train_acc= 1.000   test_loss=2.373   test_acc= 0.800\n",
      "epoch= 21   train_loss= 1.997   train_acc= 1.000   test_loss=2.362   test_acc= 0.800\n",
      "epoch= 22   train_loss= 1.986   train_acc= 1.000   test_loss=2.395   test_acc= 0.800\n",
      "epoch= 23   train_loss= 1.968   train_acc= 1.000   test_loss=2.370   test_acc= 0.800\n",
      "epoch= 24   train_loss= 1.959   train_acc= 1.000   test_loss=2.405   test_acc= 0.800\n",
      "epoch= 25   train_loss= 1.949   train_acc= 1.000   test_loss=2.335   test_acc= 0.800\n",
      "epoch= 26   train_loss= 1.925   train_acc= 1.000   test_loss=2.314   test_acc= 0.800\n",
      "epoch= 27   train_loss= 1.913   train_acc= 1.000   test_loss=2.324   test_acc= 0.800\n",
      "epoch= 28   train_loss= 1.899   train_acc= 1.000   test_loss=2.288   test_acc= 0.800\n",
      "epoch= 29   train_loss= 1.888   train_acc= 1.000   test_loss=2.269   test_acc= 0.800\n",
      "epoch= 30   train_loss= 1.882   train_acc= 1.000   test_loss=2.249   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.873   train_acc= 1.000   test_loss=2.157   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.855   train_acc= 1.000   test_loss=2.220   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.844   train_acc= 1.000   test_loss=2.210   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.838   train_acc= 1.000   test_loss=2.194   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.819   train_acc= 1.000   test_loss=2.196   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.809   train_acc= 1.000   test_loss=2.198   test_acc= 0.800\n",
      "epoch= 37   train_loss= 1.799   train_acc= 1.000   test_loss=2.218   test_acc= 0.800\n",
      "epoch= 38   train_loss= 1.786   train_acc= 1.000   test_loss=2.233   test_acc= 0.800\n",
      "epoch= 39   train_loss= 1.778   train_acc= 1.000   test_loss=2.155   test_acc= 0.800\n",
      "epoch= 40   train_loss= 1.768   train_acc= 1.000   test_loss=2.158   test_acc= 0.800\n",
      "epoch= 41   train_loss= 1.759   train_acc= 1.000   test_loss=2.150   test_acc= 0.800\n",
      "epoch= 42   train_loss= 1.746   train_acc= 1.000   test_loss=2.129   test_acc= 0.800\n",
      "epoch= 43   train_loss= 1.733   train_acc= 1.000   test_loss=2.129   test_acc= 0.800\n",
      "epoch= 44   train_loss= 1.724   train_acc= 1.000   test_loss=2.097   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.721   train_acc= 1.000   test_loss=2.077   test_acc= 0.900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 46   train_loss= 1.704   train_acc= 1.000   test_loss=2.075   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.695   train_acc= 1.000   test_loss=2.066   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.688   train_acc= 1.000   test_loss=2.096   test_acc= 0.800\n",
      "epoch= 49   train_loss= 1.674   train_acc= 1.000   test_loss=2.082   test_acc= 0.800\n",
      "run time: 0.4483219861984253 min\n",
      "test_acc=0.800\n",
      "run= 0   fold= 2\n",
      "epoch= 0   train_loss= 3.026   train_acc= 0.566   test_loss=2.823   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.655   train_acc= 0.843   test_loss=2.776   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.519   train_acc= 0.892   test_loss=2.669   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.414   train_acc= 0.952   test_loss=2.617   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.358   train_acc= 0.976   test_loss=2.598   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.328   train_acc= 0.988   test_loss=2.555   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.289   train_acc= 0.976   test_loss=2.531   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.257   train_acc= 1.000   test_loss=2.510   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.239   train_acc= 0.988   test_loss=2.486   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.209   train_acc= 1.000   test_loss=2.529   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.217   train_acc= 0.988   test_loss=2.455   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.173   train_acc= 0.988   test_loss=2.482   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.149   train_acc= 1.000   test_loss=2.409   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.137   train_acc= 1.000   test_loss=2.426   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.113   train_acc= 1.000   test_loss=2.397   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.098   train_acc= 1.000   test_loss=2.361   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.076   train_acc= 1.000   test_loss=2.349   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.065   train_acc= 1.000   test_loss=2.318   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.045   train_acc= 1.000   test_loss=2.320   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.050   train_acc= 0.988   test_loss=2.323   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.022   train_acc= 1.000   test_loss=2.299   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.006   train_acc= 1.000   test_loss=2.289   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.995   train_acc= 1.000   test_loss=2.260   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.982   train_acc= 1.000   test_loss=2.259   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.967   train_acc= 1.000   test_loss=2.223   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.957   train_acc= 1.000   test_loss=2.200   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.940   train_acc= 1.000   test_loss=2.211   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.925   train_acc= 1.000   test_loss=2.201   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.919   train_acc= 0.988   test_loss=2.182   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.904   train_acc= 1.000   test_loss=2.168   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.891   train_acc= 1.000   test_loss=2.158   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.880   train_acc= 1.000   test_loss=2.154   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.873   train_acc= 1.000   test_loss=2.132   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.855   train_acc= 1.000   test_loss=2.123   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.843   train_acc= 1.000   test_loss=2.128   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.829   train_acc= 1.000   test_loss=2.113   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.822   train_acc= 1.000   test_loss=2.102   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.807   train_acc= 1.000   test_loss=2.090   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.795   train_acc= 1.000   test_loss=2.078   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.787   train_acc= 1.000   test_loss=2.056   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.775   train_acc= 1.000   test_loss=2.058   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.762   train_acc= 1.000   test_loss=2.045   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.753   train_acc= 1.000   test_loss=2.037   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.742   train_acc= 1.000   test_loss=2.020   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.734   train_acc= 1.000   test_loss=2.005   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.725   train_acc= 1.000   test_loss=2.010   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.712   train_acc= 1.000   test_loss=1.999   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.702   train_acc= 1.000   test_loss=1.982   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.694   train_acc= 1.000   test_loss=1.970   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.683   train_acc= 1.000   test_loss=1.956   test_acc= 0.889\n",
      "run time: 0.42202502886454263 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 3\n",
      "epoch= 0   train_loss= 3.061   train_acc= 0.566   test_loss=2.840   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.685   train_acc= 0.880   test_loss=2.745   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.572   train_acc= 0.855   test_loss=2.695   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.460   train_acc= 0.964   test_loss=2.603   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.392   train_acc= 0.952   test_loss=2.587   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.370   train_acc= 0.964   test_loss=2.500   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.288   train_acc= 1.000   test_loss=2.508   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.281   train_acc= 0.976   test_loss=2.494   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.252   train_acc= 0.988   test_loss=2.474   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.219   train_acc= 1.000   test_loss=2.427   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.200   train_acc= 1.000   test_loss=2.426   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.162   train_acc= 1.000   test_loss=2.405   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.154   train_acc= 1.000   test_loss=2.369   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.130   train_acc= 1.000   test_loss=2.383   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.109   train_acc= 1.000   test_loss=2.345   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.095   train_acc= 1.000   test_loss=2.326   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.089   train_acc= 1.000   test_loss=2.334   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.069   train_acc= 1.000   test_loss=2.279   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.050   train_acc= 1.000   test_loss=2.281   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.038   train_acc= 1.000   test_loss=2.302   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.018   train_acc= 1.000   test_loss=2.266   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.004   train_acc= 1.000   test_loss=2.237   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.990   train_acc= 1.000   test_loss=2.249   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.980   train_acc= 1.000   test_loss=2.208   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.964   train_acc= 1.000   test_loss=2.207   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.953   train_acc= 1.000   test_loss=2.197   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.940   train_acc= 1.000   test_loss=2.181   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.925   train_acc= 1.000   test_loss=2.187   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.910   train_acc= 1.000   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.898   train_acc= 1.000   test_loss=2.149   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.884   train_acc= 1.000   test_loss=2.145   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.874   train_acc= 1.000   test_loss=2.113   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.866   train_acc= 1.000   test_loss=2.155   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.850   train_acc= 1.000   test_loss=2.139   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.840   train_acc= 1.000   test_loss=2.122   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.829   train_acc= 1.000   test_loss=2.076   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.817   train_acc= 1.000   test_loss=2.081   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.805   train_acc= 1.000   test_loss=2.067   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.793   train_acc= 1.000   test_loss=2.054   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.783   train_acc= 1.000   test_loss=2.030   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.780   train_acc= 1.000   test_loss=2.061   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.765   train_acc= 1.000   test_loss=2.047   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 42   train_loss= 1.754   train_acc= 1.000   test_loss=2.036   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.746   train_acc= 1.000   test_loss=2.051   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.729   train_acc= 1.000   test_loss=2.003   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.723   train_acc= 1.000   test_loss=1.996   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.710   train_acc= 1.000   test_loss=1.992   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.704   train_acc= 1.000   test_loss=1.976   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.694   train_acc= 1.000   test_loss=1.961   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.681   train_acc= 1.000   test_loss=1.961   test_acc= 0.889\n",
      "run time: 0.4253049731254578 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 4\n",
      "epoch= 0   train_loss= 2.938   train_acc= 0.614   test_loss=2.713   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.582   train_acc= 0.904   test_loss=2.669   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.508   train_acc= 0.928   test_loss=2.611   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.521   train_acc= 0.855   test_loss=2.553   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.393   train_acc= 0.940   test_loss=2.496   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.360   train_acc= 0.940   test_loss=2.466   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.305   train_acc= 0.976   test_loss=2.402   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.276   train_acc= 0.964   test_loss=2.408   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.239   train_acc= 0.988   test_loss=2.352   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.210   train_acc= 1.000   test_loss=2.317   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.194   train_acc= 1.000   test_loss=2.303   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.162   train_acc= 1.000   test_loss=2.276   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.153   train_acc= 1.000   test_loss=2.256   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.115   train_acc= 1.000   test_loss=2.249   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.101   train_acc= 1.000   test_loss=2.217   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.085   train_acc= 1.000   test_loss=2.196   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.066   train_acc= 1.000   test_loss=2.182   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.056   train_acc= 1.000   test_loss=2.156   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.035   train_acc= 1.000   test_loss=2.145   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.033   train_acc= 1.000   test_loss=2.117   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.011   train_acc= 1.000   test_loss=2.115   test_acc= 1.000\n",
      "epoch= 21   train_loss= 1.991   train_acc= 1.000   test_loss=2.109   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.980   train_acc= 1.000   test_loss=2.086   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.967   train_acc= 1.000   test_loss=2.072   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.962   train_acc= 1.000   test_loss=2.060   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.945   train_acc= 1.000   test_loss=2.044   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.929   train_acc= 1.000   test_loss=2.036   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.914   train_acc= 1.000   test_loss=2.022   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.899   train_acc= 1.000   test_loss=2.008   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.893   train_acc= 1.000   test_loss=1.996   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.873   train_acc= 1.000   test_loss=1.982   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.859   train_acc= 1.000   test_loss=1.972   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.852   train_acc= 1.000   test_loss=1.960   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.841   train_acc= 1.000   test_loss=1.952   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.826   train_acc= 1.000   test_loss=1.939   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.817   train_acc= 1.000   test_loss=1.923   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.804   train_acc= 1.000   test_loss=1.915   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.791   train_acc= 1.000   test_loss=1.897   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.783   train_acc= 1.000   test_loss=1.896   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.769   train_acc= 1.000   test_loss=1.887   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.762   train_acc= 1.000   test_loss=1.875   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.756   train_acc= 1.000   test_loss=1.859   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.740   train_acc= 1.000   test_loss=1.845   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.731   train_acc= 1.000   test_loss=1.830   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.722   train_acc= 1.000   test_loss=1.820   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.720   train_acc= 1.000   test_loss=1.815   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.699   train_acc= 1.000   test_loss=1.812   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.691   train_acc= 1.000   test_loss=1.796   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.678   train_acc= 1.000   test_loss=1.789   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.669   train_acc= 1.000   test_loss=1.778   test_acc= 1.000\n",
      "run time: 0.4391571005185445 min\n",
      "test_acc=1.000\n",
      "run= 0   fold= 5\n",
      "epoch= 0   train_loss= 2.956   train_acc= 0.602   test_loss=2.832   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.650   train_acc= 0.831   test_loss=2.776   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.473   train_acc= 0.952   test_loss=2.785   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.421   train_acc= 0.928   test_loss=2.726   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.352   train_acc= 0.940   test_loss=2.636   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.317   train_acc= 0.976   test_loss=2.577   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.303   train_acc= 0.952   test_loss=2.567   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.259   train_acc= 0.988   test_loss=2.525   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.216   train_acc= 0.988   test_loss=2.527   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.201   train_acc= 1.000   test_loss=2.625   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.185   train_acc= 1.000   test_loss=2.557   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.150   train_acc= 1.000   test_loss=2.497   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.143   train_acc= 1.000   test_loss=2.430   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.117   train_acc= 1.000   test_loss=2.411   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.099   train_acc= 1.000   test_loss=2.432   test_acc= 0.667\n",
      "epoch= 15   train_loss= 2.092   train_acc= 0.988   test_loss=2.377   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.067   train_acc= 1.000   test_loss=2.352   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.057   train_acc= 1.000   test_loss=2.336   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.036   train_acc= 1.000   test_loss=2.358   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.021   train_acc= 1.000   test_loss=2.297   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.010   train_acc= 1.000   test_loss=2.274   test_acc= 0.778\n",
      "epoch= 21   train_loss= 1.994   train_acc= 1.000   test_loss=2.279   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.981   train_acc= 1.000   test_loss=2.262   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.964   train_acc= 1.000   test_loss=2.247   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.953   train_acc= 1.000   test_loss=2.258   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.941   train_acc= 1.000   test_loss=2.225   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.926   train_acc= 1.000   test_loss=2.209   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.914   train_acc= 1.000   test_loss=2.200   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.903   train_acc= 1.000   test_loss=2.170   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.890   train_acc= 1.000   test_loss=2.168   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.875   train_acc= 1.000   test_loss=2.159   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.862   train_acc= 1.000   test_loss=2.146   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.854   train_acc= 1.000   test_loss=2.121   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.839   train_acc= 1.000   test_loss=2.113   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.831   train_acc= 1.000   test_loss=2.121   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.817   train_acc= 1.000   test_loss=2.084   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.805   train_acc= 1.000   test_loss=2.081   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.798   train_acc= 1.000   test_loss=2.077   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 38   train_loss= 1.785   train_acc= 1.000   test_loss=2.068   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.773   train_acc= 1.000   test_loss=2.037   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.760   train_acc= 1.000   test_loss=2.042   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.750   train_acc= 1.000   test_loss=2.026   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.743   train_acc= 1.000   test_loss=2.020   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.730   train_acc= 1.000   test_loss=1.997   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.723   train_acc= 1.000   test_loss=1.984   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.715   train_acc= 1.000   test_loss=2.000   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.709   train_acc= 1.000   test_loss=1.952   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.691   train_acc= 1.000   test_loss=1.963   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.684   train_acc= 1.000   test_loss=1.952   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.676   train_acc= 1.000   test_loss=1.951   test_acc= 0.889\n",
      "run time: 0.46815147399902346 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 6\n",
      "epoch= 0   train_loss= 3.025   train_acc= 0.578   test_loss=2.766   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.638   train_acc= 0.807   test_loss=2.571   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.481   train_acc= 0.940   test_loss=2.550   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.435   train_acc= 0.928   test_loss=2.504   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.365   train_acc= 0.952   test_loss=2.473   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.326   train_acc= 0.964   test_loss=2.416   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.293   train_acc= 0.976   test_loss=2.414   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.241   train_acc= 0.976   test_loss=2.536   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.212   train_acc= 1.000   test_loss=2.449   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.212   train_acc= 0.976   test_loss=2.376   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.161   train_acc= 1.000   test_loss=2.354   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.146   train_acc= 1.000   test_loss=2.362   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.133   train_acc= 1.000   test_loss=2.357   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.126   train_acc= 0.988   test_loss=2.274   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.098   train_acc= 1.000   test_loss=2.307   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.078   train_acc= 1.000   test_loss=2.293   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.077   train_acc= 0.988   test_loss=2.319   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.044   train_acc= 1.000   test_loss=2.275   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.039   train_acc= 1.000   test_loss=2.255   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.010   train_acc= 1.000   test_loss=2.231   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.004   train_acc= 1.000   test_loss=2.170   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.989   train_acc= 1.000   test_loss=2.176   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.981   train_acc= 1.000   test_loss=2.218   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.959   train_acc= 1.000   test_loss=2.193   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.954   train_acc= 1.000   test_loss=2.163   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.931   train_acc= 1.000   test_loss=2.177   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.920   train_acc= 1.000   test_loss=2.157   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.904   train_acc= 1.000   test_loss=2.140   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.900   train_acc= 1.000   test_loss=2.175   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.879   train_acc= 1.000   test_loss=2.104   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.872   train_acc= 1.000   test_loss=2.108   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.862   train_acc= 1.000   test_loss=2.114   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.845   train_acc= 1.000   test_loss=2.091   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.836   train_acc= 1.000   test_loss=2.077   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.826   train_acc= 1.000   test_loss=2.085   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.818   train_acc= 1.000   test_loss=2.041   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.803   train_acc= 1.000   test_loss=2.007   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.793   train_acc= 1.000   test_loss=2.015   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.778   train_acc= 1.000   test_loss=2.028   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.771   train_acc= 1.000   test_loss=2.000   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.762   train_acc= 1.000   test_loss=1.962   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.750   train_acc= 1.000   test_loss=1.977   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.735   train_acc= 1.000   test_loss=1.980   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.730   train_acc= 1.000   test_loss=1.956   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.717   train_acc= 1.000   test_loss=1.927   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.706   train_acc= 1.000   test_loss=1.930   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.698   train_acc= 1.000   test_loss=1.873   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.687   train_acc= 1.000   test_loss=1.907   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.677   train_acc= 1.000   test_loss=1.909   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.667   train_acc= 1.000   test_loss=1.868   test_acc= 0.889\n",
      "run time: 0.46172165075937904 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 7\n",
      "epoch= 0   train_loss= 3.016   train_acc= 0.627   test_loss=2.632   test_acc= 1.000\n",
      "epoch= 1   train_loss= 2.650   train_acc= 0.819   test_loss=2.381   test_acc= 1.000\n",
      "epoch= 2   train_loss= 2.568   train_acc= 0.855   test_loss=2.433   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.444   train_acc= 0.964   test_loss=2.533   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.396   train_acc= 0.964   test_loss=2.316   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.339   train_acc= 0.964   test_loss=2.353   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.300   train_acc= 0.988   test_loss=2.294   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.260   train_acc= 0.988   test_loss=2.322   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.241   train_acc= 0.988   test_loss=2.216   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.217   train_acc= 0.988   test_loss=2.225   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.185   train_acc= 1.000   test_loss=2.230   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.164   train_acc= 1.000   test_loss=2.210   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.142   train_acc= 1.000   test_loss=2.187   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.137   train_acc= 1.000   test_loss=2.171   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.118   train_acc= 1.000   test_loss=2.143   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.094   train_acc= 1.000   test_loss=2.142   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.083   train_acc= 1.000   test_loss=2.103   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.078   train_acc= 1.000   test_loss=2.117   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.046   train_acc= 1.000   test_loss=2.073   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.031   train_acc= 1.000   test_loss=2.055   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.014   train_acc= 1.000   test_loss=2.044   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.000   train_acc= 1.000   test_loss=2.026   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.985   train_acc= 1.000   test_loss=2.017   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.972   train_acc= 1.000   test_loss=1.994   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.970   train_acc= 1.000   test_loss=1.996   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.949   train_acc= 1.000   test_loss=1.980   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.932   train_acc= 1.000   test_loss=1.965   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.930   train_acc= 1.000   test_loss=1.968   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.912   train_acc= 1.000   test_loss=1.949   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.896   train_acc= 1.000   test_loss=1.930   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.885   train_acc= 1.000   test_loss=1.913   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.874   train_acc= 1.000   test_loss=1.903   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.861   train_acc= 1.000   test_loss=1.893   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.859   train_acc= 1.000   test_loss=1.881   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 34   train_loss= 1.837   train_acc= 1.000   test_loss=1.873   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.825   train_acc= 1.000   test_loss=1.855   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.811   train_acc= 1.000   test_loss=1.844   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.802   train_acc= 1.000   test_loss=1.834   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.790   train_acc= 1.000   test_loss=1.825   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.783   train_acc= 1.000   test_loss=1.809   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.767   train_acc= 1.000   test_loss=1.799   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.759   train_acc= 1.000   test_loss=1.791   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.749   train_acc= 1.000   test_loss=1.785   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.738   train_acc= 1.000   test_loss=1.774   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.730   train_acc= 1.000   test_loss=1.764   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.722   train_acc= 1.000   test_loss=1.759   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.709   train_acc= 1.000   test_loss=1.740   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.698   train_acc= 1.000   test_loss=1.730   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.687   train_acc= 1.000   test_loss=1.719   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.679   train_acc= 1.000   test_loss=1.710   test_acc= 1.000\n",
      "run time: 0.43878817558288574 min\n",
      "test_acc=1.000\n",
      "run= 0   fold= 8\n",
      "epoch= 0   train_loss= 2.966   train_acc= 0.614   test_loss=2.913   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.629   train_acc= 0.855   test_loss=2.668   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.546   train_acc= 0.892   test_loss=2.653   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.438   train_acc= 0.952   test_loss=2.642   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.393   train_acc= 0.976   test_loss=2.589   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.330   train_acc= 0.964   test_loss=2.561   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.280   train_acc= 0.988   test_loss=2.590   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.281   train_acc= 0.988   test_loss=2.503   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.224   train_acc= 1.000   test_loss=2.493   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.204   train_acc= 1.000   test_loss=2.517   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.204   train_acc= 0.976   test_loss=2.466   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.174   train_acc= 1.000   test_loss=2.520   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.158   train_acc= 1.000   test_loss=2.498   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.127   train_acc= 1.000   test_loss=2.427   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.120   train_acc= 1.000   test_loss=2.402   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.096   train_acc= 1.000   test_loss=2.404   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.088   train_acc= 1.000   test_loss=2.390   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.065   train_acc= 1.000   test_loss=2.357   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.049   train_acc= 1.000   test_loss=2.334   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.035   train_acc= 1.000   test_loss=2.327   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.023   train_acc= 1.000   test_loss=2.336   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.007   train_acc= 1.000   test_loss=2.316   test_acc= 0.778\n",
      "epoch= 22   train_loss= 2.003   train_acc= 1.000   test_loss=2.253   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.980   train_acc= 1.000   test_loss=2.255   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.965   train_acc= 1.000   test_loss=2.216   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.954   train_acc= 1.000   test_loss=2.230   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.937   train_acc= 1.000   test_loss=2.225   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.934   train_acc= 1.000   test_loss=2.168   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.917   train_acc= 1.000   test_loss=2.200   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.906   train_acc= 1.000   test_loss=2.161   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.897   train_acc= 1.000   test_loss=2.157   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.878   train_acc= 1.000   test_loss=2.171   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.871   train_acc= 1.000   test_loss=2.165   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.854   train_acc= 1.000   test_loss=2.123   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.845   train_acc= 1.000   test_loss=2.104   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.832   train_acc= 1.000   test_loss=2.115   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.821   train_acc= 1.000   test_loss=2.114   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.810   train_acc= 1.000   test_loss=2.084   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.798   train_acc= 1.000   test_loss=2.058   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.789   train_acc= 1.000   test_loss=2.047   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.779   train_acc= 1.000   test_loss=2.030   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.769   train_acc= 1.000   test_loss=2.045   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.758   train_acc= 1.000   test_loss=2.024   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.751   train_acc= 1.000   test_loss=2.013   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.735   train_acc= 1.000   test_loss=2.011   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.723   train_acc= 1.000   test_loss=1.984   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.713   train_acc= 1.000   test_loss=1.965   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.707   train_acc= 1.000   test_loss=1.965   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.698   train_acc= 1.000   test_loss=1.954   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.685   train_acc= 1.000   test_loss=1.973   test_acc= 0.889\n",
      "run time: 0.4427270730336507 min\n",
      "test_acc=0.889\n",
      "run= 0   fold= 9\n",
      "epoch= 0   train_loss= 2.926   train_acc= 0.735   test_loss=2.729   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.618   train_acc= 0.855   test_loss=2.679   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.529   train_acc= 0.904   test_loss=2.654   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.437   train_acc= 0.964   test_loss=2.634   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.387   train_acc= 0.964   test_loss=2.614   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.345   train_acc= 0.964   test_loss=2.517   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.288   train_acc= 0.976   test_loss=2.503   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.253   train_acc= 1.000   test_loss=2.483   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.214   train_acc= 1.000   test_loss=2.465   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.206   train_acc= 0.988   test_loss=2.448   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.182   train_acc= 1.000   test_loss=2.433   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.155   train_acc= 1.000   test_loss=2.401   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.140   train_acc= 1.000   test_loss=2.393   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.121   train_acc= 1.000   test_loss=2.356   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.100   train_acc= 1.000   test_loss=2.349   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.081   train_acc= 1.000   test_loss=2.344   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.068   train_acc= 1.000   test_loss=2.319   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.055   train_acc= 1.000   test_loss=2.310   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.040   train_acc= 1.000   test_loss=2.305   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.032   train_acc= 1.000   test_loss=2.275   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.009   train_acc= 1.000   test_loss=2.258   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.004   train_acc= 1.000   test_loss=2.272   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.993   train_acc= 0.988   test_loss=2.232   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.970   train_acc= 1.000   test_loss=2.224   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.956   train_acc= 1.000   test_loss=2.203   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.941   train_acc= 1.000   test_loss=2.192   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.924   train_acc= 1.000   test_loss=2.181   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.917   train_acc= 1.000   test_loss=2.162   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.904   train_acc= 1.000   test_loss=2.153   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.892   train_acc= 1.000   test_loss=2.143   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 30   train_loss= 1.878   train_acc= 1.000   test_loss=2.133   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.869   train_acc= 1.000   test_loss=2.122   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.856   train_acc= 1.000   test_loss=2.111   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.843   train_acc= 1.000   test_loss=2.103   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.830   train_acc= 1.000   test_loss=2.087   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.818   train_acc= 1.000   test_loss=2.077   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.807   train_acc= 1.000   test_loss=2.068   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.800   train_acc= 1.000   test_loss=2.055   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.787   train_acc= 1.000   test_loss=2.040   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.775   train_acc= 1.000   test_loss=2.030   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.766   train_acc= 1.000   test_loss=2.028   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.757   train_acc= 1.000   test_loss=2.012   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.747   train_acc= 1.000   test_loss=2.001   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.733   train_acc= 1.000   test_loss=1.990   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.724   train_acc= 1.000   test_loss=1.983   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.713   train_acc= 1.000   test_loss=1.978   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.704   train_acc= 1.000   test_loss=1.964   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.693   train_acc= 1.000   test_loss=1.954   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.683   train_acc= 1.000   test_loss=1.945   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.676   train_acc= 1.000   test_loss=1.951   test_acc= 0.889\n",
      "run time: 0.4365064303080241 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 0\n",
      "epoch= 0   train_loss= 2.938   train_acc= 0.671   test_loss=2.839   test_acc= 0.700\n",
      "epoch= 1   train_loss= 2.596   train_acc= 0.890   test_loss=2.730   test_acc= 0.800\n",
      "epoch= 2   train_loss= 2.539   train_acc= 0.878   test_loss=2.707   test_acc= 0.700\n",
      "epoch= 3   train_loss= 2.440   train_acc= 0.963   test_loss=2.686   test_acc= 0.700\n",
      "epoch= 4   train_loss= 2.381   train_acc= 0.976   test_loss=2.603   test_acc= 0.700\n",
      "epoch= 5   train_loss= 2.348   train_acc= 0.939   test_loss=2.529   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.289   train_acc= 0.976   test_loss=2.622   test_acc= 0.700\n",
      "epoch= 7   train_loss= 2.275   train_acc= 0.963   test_loss=2.564   test_acc= 0.700\n",
      "epoch= 8   train_loss= 2.232   train_acc= 0.988   test_loss=2.454   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.205   train_acc= 1.000   test_loss=2.503   test_acc= 0.700\n",
      "epoch= 10   train_loss= 2.180   train_acc= 1.000   test_loss=2.524   test_acc= 0.700\n",
      "epoch= 11   train_loss= 2.164   train_acc= 0.988   test_loss=2.531   test_acc= 0.700\n",
      "epoch= 12   train_loss= 2.148   train_acc= 1.000   test_loss=2.435   test_acc= 0.700\n",
      "epoch= 13   train_loss= 2.126   train_acc= 0.988   test_loss=2.448   test_acc= 0.700\n",
      "epoch= 14   train_loss= 2.107   train_acc= 1.000   test_loss=2.450   test_acc= 0.700\n",
      "epoch= 15   train_loss= 2.082   train_acc= 1.000   test_loss=2.398   test_acc= 0.700\n",
      "epoch= 16   train_loss= 2.073   train_acc= 1.000   test_loss=2.405   test_acc= 0.700\n",
      "epoch= 17   train_loss= 2.054   train_acc= 1.000   test_loss=2.424   test_acc= 0.700\n",
      "epoch= 18   train_loss= 2.044   train_acc= 1.000   test_loss=2.427   test_acc= 0.700\n",
      "epoch= 19   train_loss= 2.021   train_acc= 1.000   test_loss=2.388   test_acc= 0.700\n",
      "epoch= 20   train_loss= 2.011   train_acc= 1.000   test_loss=2.379   test_acc= 0.700\n",
      "epoch= 21   train_loss= 1.992   train_acc= 1.000   test_loss=2.375   test_acc= 0.700\n",
      "epoch= 22   train_loss= 1.983   train_acc= 1.000   test_loss=2.351   test_acc= 0.700\n",
      "epoch= 23   train_loss= 1.967   train_acc= 1.000   test_loss=2.321   test_acc= 0.700\n",
      "epoch= 24   train_loss= 1.953   train_acc= 1.000   test_loss=2.314   test_acc= 0.700\n",
      "epoch= 25   train_loss= 1.940   train_acc= 1.000   test_loss=2.307   test_acc= 0.700\n",
      "epoch= 26   train_loss= 1.927   train_acc= 1.000   test_loss=2.295   test_acc= 0.700\n",
      "epoch= 27   train_loss= 1.914   train_acc= 1.000   test_loss=2.285   test_acc= 0.700\n",
      "epoch= 28   train_loss= 1.903   train_acc= 1.000   test_loss=2.287   test_acc= 0.700\n",
      "epoch= 29   train_loss= 1.893   train_acc= 1.000   test_loss=2.254   test_acc= 0.700\n",
      "epoch= 30   train_loss= 1.881   train_acc= 1.000   test_loss=2.241   test_acc= 0.700\n",
      "epoch= 31   train_loss= 1.866   train_acc= 1.000   test_loss=2.229   test_acc= 0.700\n",
      "epoch= 32   train_loss= 1.857   train_acc= 1.000   test_loss=2.240   test_acc= 0.700\n",
      "epoch= 33   train_loss= 1.847   train_acc= 1.000   test_loss=2.233   test_acc= 0.700\n",
      "epoch= 34   train_loss= 1.831   train_acc= 1.000   test_loss=2.216   test_acc= 0.700\n",
      "epoch= 35   train_loss= 1.822   train_acc= 1.000   test_loss=2.189   test_acc= 0.700\n",
      "epoch= 36   train_loss= 1.809   train_acc= 1.000   test_loss=2.189   test_acc= 0.700\n",
      "epoch= 37   train_loss= 1.805   train_acc= 1.000   test_loss=2.186   test_acc= 0.700\n",
      "epoch= 38   train_loss= 1.789   train_acc= 1.000   test_loss=2.162   test_acc= 0.700\n",
      "epoch= 39   train_loss= 1.776   train_acc= 1.000   test_loss=2.146   test_acc= 0.700\n",
      "epoch= 40   train_loss= 1.766   train_acc= 1.000   test_loss=2.118   test_acc= 0.700\n",
      "epoch= 41   train_loss= 1.755   train_acc= 1.000   test_loss=2.119   test_acc= 0.700\n",
      "epoch= 42   train_loss= 1.745   train_acc= 1.000   test_loss=2.121   test_acc= 0.700\n",
      "epoch= 43   train_loss= 1.733   train_acc= 1.000   test_loss=2.117   test_acc= 0.700\n",
      "epoch= 44   train_loss= 1.733   train_acc= 1.000   test_loss=2.093   test_acc= 0.700\n",
      "epoch= 45   train_loss= 1.718   train_acc= 1.000   test_loss=2.088   test_acc= 0.700\n",
      "epoch= 46   train_loss= 1.707   train_acc= 1.000   test_loss=2.090   test_acc= 0.700\n",
      "epoch= 47   train_loss= 1.701   train_acc= 1.000   test_loss=2.080   test_acc= 0.700\n",
      "epoch= 48   train_loss= 1.687   train_acc= 1.000   test_loss=2.073   test_acc= 0.700\n",
      "epoch= 49   train_loss= 1.676   train_acc= 1.000   test_loss=2.068   test_acc= 0.700\n",
      "run time: 0.42330820163091026 min\n",
      "test_acc=0.700\n",
      "run= 1   fold= 1\n",
      "epoch= 0   train_loss= 2.977   train_acc= 0.512   test_loss=2.805   test_acc= 0.900\n",
      "epoch= 1   train_loss= 2.674   train_acc= 0.829   test_loss=2.678   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.586   train_acc= 0.927   test_loss=2.617   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.478   train_acc= 0.951   test_loss=2.560   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.404   train_acc= 0.976   test_loss=2.538   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.368   train_acc= 0.963   test_loss=2.481   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.306   train_acc= 0.988   test_loss=2.452   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.273   train_acc= 0.988   test_loss=2.433   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.244   train_acc= 0.988   test_loss=2.457   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.217   train_acc= 1.000   test_loss=2.397   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.193   train_acc= 0.988   test_loss=2.357   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.175   train_acc= 1.000   test_loss=2.363   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.151   train_acc= 1.000   test_loss=2.342   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.134   train_acc= 1.000   test_loss=2.330   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.114   train_acc= 1.000   test_loss=2.319   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.094   train_acc= 1.000   test_loss=2.297   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.081   train_acc= 1.000   test_loss=2.275   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.062   train_acc= 1.000   test_loss=2.280   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.047   train_acc= 1.000   test_loss=2.252   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.028   train_acc= 1.000   test_loss=2.239   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.022   train_acc= 1.000   test_loss=2.223   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.002   train_acc= 1.000   test_loss=2.218   test_acc= 0.900\n",
      "epoch= 22   train_loss= 1.996   train_acc= 1.000   test_loss=2.209   test_acc= 0.900\n",
      "epoch= 23   train_loss= 1.971   train_acc= 1.000   test_loss=2.193   test_acc= 0.900\n",
      "epoch= 24   train_loss= 1.959   train_acc= 1.000   test_loss=2.184   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.946   train_acc= 1.000   test_loss=2.179   test_acc= 0.900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 26   train_loss= 1.936   train_acc= 1.000   test_loss=2.158   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.923   train_acc= 1.000   test_loss=2.143   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.909   train_acc= 1.000   test_loss=2.127   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.891   train_acc= 1.000   test_loss=2.127   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.880   train_acc= 1.000   test_loss=2.113   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.873   train_acc= 1.000   test_loss=2.088   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.856   train_acc= 1.000   test_loss=2.079   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.846   train_acc= 1.000   test_loss=2.071   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.836   train_acc= 1.000   test_loss=2.063   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.824   train_acc= 1.000   test_loss=2.048   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.813   train_acc= 1.000   test_loss=2.033   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.803   train_acc= 1.000   test_loss=2.027   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.791   train_acc= 1.000   test_loss=2.019   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.781   train_acc= 1.000   test_loss=2.007   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.767   train_acc= 1.000   test_loss=1.996   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.765   train_acc= 1.000   test_loss=1.992   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.748   train_acc= 1.000   test_loss=1.982   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.740   train_acc= 1.000   test_loss=1.974   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.727   train_acc= 1.000   test_loss=1.964   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.718   train_acc= 1.000   test_loss=1.955   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.708   train_acc= 1.000   test_loss=1.948   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.700   train_acc= 1.000   test_loss=1.936   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.691   train_acc= 1.000   test_loss=1.926   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.686   train_acc= 1.000   test_loss=1.925   test_acc= 0.900\n",
      "run time: 0.43822017113367717 min\n",
      "test_acc=0.900\n",
      "run= 1   fold= 2\n",
      "epoch= 0   train_loss= 2.909   train_acc= 0.602   test_loss=2.876   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.642   train_acc= 0.831   test_loss=2.766   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.514   train_acc= 0.904   test_loss=2.724   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.390   train_acc= 0.976   test_loss=2.682   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.368   train_acc= 0.976   test_loss=2.627   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.332   train_acc= 0.964   test_loss=2.613   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.309   train_acc= 0.976   test_loss=2.597   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.256   train_acc= 0.988   test_loss=2.631   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.237   train_acc= 1.000   test_loss=2.528   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.198   train_acc= 1.000   test_loss=2.505   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.205   train_acc= 0.988   test_loss=2.509   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.171   train_acc= 1.000   test_loss=2.471   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.150   train_acc= 1.000   test_loss=2.444   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.144   train_acc= 0.988   test_loss=2.429   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.112   train_acc= 1.000   test_loss=2.400   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.096   train_acc= 1.000   test_loss=2.400   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.075   train_acc= 1.000   test_loss=2.368   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.060   train_acc= 1.000   test_loss=2.356   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.052   train_acc= 1.000   test_loss=2.330   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.033   train_acc= 1.000   test_loss=2.329   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.017   train_acc= 1.000   test_loss=2.318   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.009   train_acc= 1.000   test_loss=2.292   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.987   train_acc= 1.000   test_loss=2.261   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.973   train_acc= 1.000   test_loss=2.252   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.961   train_acc= 1.000   test_loss=2.240   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.950   train_acc= 1.000   test_loss=2.240   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.941   train_acc= 1.000   test_loss=2.213   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.924   train_acc= 1.000   test_loss=2.205   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.911   train_acc= 1.000   test_loss=2.170   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.895   train_acc= 1.000   test_loss=2.163   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.883   train_acc= 1.000   test_loss=2.154   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.875   train_acc= 1.000   test_loss=2.153   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.865   train_acc= 1.000   test_loss=2.141   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.848   train_acc= 1.000   test_loss=2.129   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.844   train_acc= 1.000   test_loss=2.116   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.829   train_acc= 1.000   test_loss=2.127   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.816   train_acc= 1.000   test_loss=2.118   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.804   train_acc= 1.000   test_loss=2.087   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.790   train_acc= 1.000   test_loss=2.058   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.781   train_acc= 1.000   test_loss=2.058   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.772   train_acc= 1.000   test_loss=2.044   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.764   train_acc= 1.000   test_loss=2.029   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.752   train_acc= 1.000   test_loss=2.018   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.739   train_acc= 1.000   test_loss=2.009   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.732   train_acc= 1.000   test_loss=2.004   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.724   train_acc= 1.000   test_loss=1.988   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.710   train_acc= 1.000   test_loss=1.966   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.698   train_acc= 1.000   test_loss=1.953   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.691   train_acc= 1.000   test_loss=1.946   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.683   train_acc= 1.000   test_loss=1.929   test_acc= 0.889\n",
      "run time: 0.43428186178207395 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 3\n",
      "epoch= 0   train_loss= 2.949   train_acc= 0.518   test_loss=2.744   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.660   train_acc= 0.771   test_loss=2.667   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.505   train_acc= 0.952   test_loss=2.579   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.415   train_acc= 0.940   test_loss=2.502   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.348   train_acc= 0.976   test_loss=2.480   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.337   train_acc= 0.964   test_loss=2.418   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.325   train_acc= 0.964   test_loss=2.384   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.269   train_acc= 0.988   test_loss=2.382   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.237   train_acc= 1.000   test_loss=2.335   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.220   train_acc= 0.988   test_loss=2.295   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.171   train_acc= 1.000   test_loss=2.291   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.179   train_acc= 0.988   test_loss=2.255   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.150   train_acc= 1.000   test_loss=2.238   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.128   train_acc= 1.000   test_loss=2.214   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.112   train_acc= 1.000   test_loss=2.203   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.089   train_acc= 1.000   test_loss=2.186   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.083   train_acc= 1.000   test_loss=2.164   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.066   train_acc= 1.000   test_loss=2.150   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.050   train_acc= 1.000   test_loss=2.129   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.029   train_acc= 1.000   test_loss=2.113   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.017   train_acc= 1.000   test_loss=2.096   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.003   train_acc= 1.000   test_loss=2.079   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 22   train_loss= 1.986   train_acc= 1.000   test_loss=2.070   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.973   train_acc= 1.000   test_loss=2.061   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.961   train_acc= 1.000   test_loss=2.036   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.949   train_acc= 1.000   test_loss=2.023   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.935   train_acc= 1.000   test_loss=2.005   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.921   train_acc= 1.000   test_loss=1.992   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.910   train_acc= 1.000   test_loss=1.984   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.894   train_acc= 1.000   test_loss=1.968   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.885   train_acc= 1.000   test_loss=1.960   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.874   train_acc= 1.000   test_loss=1.943   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.862   train_acc= 1.000   test_loss=1.931   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.850   train_acc= 1.000   test_loss=1.916   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.839   train_acc= 1.000   test_loss=1.906   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.828   train_acc= 1.000   test_loss=1.895   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.817   train_acc= 1.000   test_loss=1.883   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.802   train_acc= 1.000   test_loss=1.875   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.795   train_acc= 1.000   test_loss=1.864   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.783   train_acc= 1.000   test_loss=1.850   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.776   train_acc= 1.000   test_loss=1.837   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.764   train_acc= 1.000   test_loss=1.828   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.750   train_acc= 1.000   test_loss=1.819   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.740   train_acc= 1.000   test_loss=1.810   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.729   train_acc= 1.000   test_loss=1.802   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.716   train_acc= 1.000   test_loss=1.789   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.711   train_acc= 1.000   test_loss=1.776   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.702   train_acc= 1.000   test_loss=1.767   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.689   train_acc= 1.000   test_loss=1.759   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.680   train_acc= 1.000   test_loss=1.747   test_acc= 1.000\n",
      "run time: 0.45771348079045615 min\n",
      "test_acc=1.000\n",
      "run= 1   fold= 4\n",
      "epoch= 0   train_loss= 3.001   train_acc= 0.566   test_loss=2.892   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.613   train_acc= 0.880   test_loss=2.756   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.553   train_acc= 0.904   test_loss=2.700   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.473   train_acc= 0.928   test_loss=2.801   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.358   train_acc= 0.964   test_loss=2.610   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.343   train_acc= 0.964   test_loss=2.610   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.310   train_acc= 0.964   test_loss=2.600   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.256   train_acc= 0.988   test_loss=2.578   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.245   train_acc= 0.988   test_loss=2.627   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.205   train_acc= 1.000   test_loss=2.588   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.206   train_acc= 0.976   test_loss=2.545   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.172   train_acc= 0.988   test_loss=2.603   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.143   train_acc= 1.000   test_loss=2.528   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.124   train_acc= 1.000   test_loss=2.522   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.131   train_acc= 0.964   test_loss=2.495   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.105   train_acc= 0.988   test_loss=2.436   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.081   train_acc= 1.000   test_loss=2.518   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.054   train_acc= 1.000   test_loss=2.470   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.047   train_acc= 1.000   test_loss=2.457   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.025   train_acc= 1.000   test_loss=2.476   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.012   train_acc= 1.000   test_loss=2.459   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.996   train_acc= 1.000   test_loss=2.460   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.985   train_acc= 1.000   test_loss=2.485   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.968   train_acc= 1.000   test_loss=2.439   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.955   train_acc= 1.000   test_loss=2.436   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.949   train_acc= 1.000   test_loss=2.421   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.932   train_acc= 1.000   test_loss=2.410   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.921   train_acc= 1.000   test_loss=2.399   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.903   train_acc= 1.000   test_loss=2.380   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.893   train_acc= 1.000   test_loss=2.374   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.882   train_acc= 1.000   test_loss=2.355   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.867   train_acc= 1.000   test_loss=2.383   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.856   train_acc= 1.000   test_loss=2.346   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.844   train_acc= 1.000   test_loss=2.333   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.830   train_acc= 1.000   test_loss=2.333   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.823   train_acc= 1.000   test_loss=2.315   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.815   train_acc= 0.988   test_loss=2.243   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.801   train_acc= 1.000   test_loss=2.261   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.789   train_acc= 1.000   test_loss=2.251   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.775   train_acc= 1.000   test_loss=2.259   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.767   train_acc= 1.000   test_loss=2.260   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.753   train_acc= 1.000   test_loss=2.252   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.744   train_acc= 1.000   test_loss=2.230   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.738   train_acc= 1.000   test_loss=2.220   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.724   train_acc= 1.000   test_loss=2.217   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.715   train_acc= 1.000   test_loss=2.233   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.703   train_acc= 1.000   test_loss=2.218   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.692   train_acc= 1.000   test_loss=2.207   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.687   train_acc= 1.000   test_loss=2.197   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.677   train_acc= 1.000   test_loss=2.190   test_acc= 0.889\n",
      "run time: 0.44899473190307615 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 5\n",
      "epoch= 0   train_loss= 2.977   train_acc= 0.602   test_loss=2.869   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.596   train_acc= 0.867   test_loss=2.966   test_acc= 0.556\n",
      "epoch= 2   train_loss= 2.507   train_acc= 0.904   test_loss=2.866   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.384   train_acc= 0.964   test_loss=2.801   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.354   train_acc= 0.976   test_loss=2.799   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.292   train_acc= 0.988   test_loss=2.694   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.254   train_acc= 1.000   test_loss=2.806   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.233   train_acc= 1.000   test_loss=2.796   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.226   train_acc= 0.988   test_loss=2.746   test_acc= 0.667\n",
      "epoch= 9   train_loss= 2.200   train_acc= 1.000   test_loss=2.820   test_acc= 0.667\n",
      "epoch= 10   train_loss= 2.170   train_acc= 1.000   test_loss=2.770   test_acc= 0.667\n",
      "epoch= 11   train_loss= 2.149   train_acc= 1.000   test_loss=2.743   test_acc= 0.667\n",
      "epoch= 12   train_loss= 2.143   train_acc= 1.000   test_loss=2.793   test_acc= 0.667\n",
      "epoch= 13   train_loss= 2.112   train_acc= 1.000   test_loss=2.742   test_acc= 0.667\n",
      "epoch= 14   train_loss= 2.098   train_acc= 1.000   test_loss=2.795   test_acc= 0.667\n",
      "epoch= 15   train_loss= 2.091   train_acc= 1.000   test_loss=2.838   test_acc= 0.667\n",
      "epoch= 16   train_loss= 2.066   train_acc= 1.000   test_loss=2.800   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.055   train_acc= 1.000   test_loss=2.853   test_acc= 0.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 18   train_loss= 2.049   train_acc= 1.000   test_loss=2.880   test_acc= 0.667\n",
      "epoch= 19   train_loss= 2.028   train_acc= 1.000   test_loss=2.834   test_acc= 0.667\n",
      "epoch= 20   train_loss= 2.009   train_acc= 1.000   test_loss=2.754   test_acc= 0.667\n",
      "epoch= 21   train_loss= 1.998   train_acc= 1.000   test_loss=2.823   test_acc= 0.667\n",
      "epoch= 22   train_loss= 1.981   train_acc= 1.000   test_loss=2.744   test_acc= 0.667\n",
      "epoch= 23   train_loss= 1.978   train_acc= 0.988   test_loss=2.742   test_acc= 0.667\n",
      "epoch= 24   train_loss= 1.959   train_acc= 1.000   test_loss=2.743   test_acc= 0.667\n",
      "epoch= 25   train_loss= 1.942   train_acc= 1.000   test_loss=2.758   test_acc= 0.667\n",
      "epoch= 26   train_loss= 1.928   train_acc= 1.000   test_loss=2.754   test_acc= 0.667\n",
      "epoch= 27   train_loss= 1.912   train_acc= 1.000   test_loss=2.705   test_acc= 0.667\n",
      "epoch= 28   train_loss= 1.901   train_acc= 1.000   test_loss=2.763   test_acc= 0.667\n",
      "epoch= 29   train_loss= 1.890   train_acc= 1.000   test_loss=2.740   test_acc= 0.667\n",
      "epoch= 30   train_loss= 1.878   train_acc= 1.000   test_loss=2.768   test_acc= 0.667\n",
      "epoch= 31   train_loss= 1.867   train_acc= 1.000   test_loss=2.708   test_acc= 0.667\n",
      "epoch= 32   train_loss= 1.853   train_acc= 1.000   test_loss=2.747   test_acc= 0.667\n",
      "epoch= 33   train_loss= 1.847   train_acc= 1.000   test_loss=2.753   test_acc= 0.667\n",
      "epoch= 34   train_loss= 1.831   train_acc= 1.000   test_loss=2.737   test_acc= 0.667\n",
      "epoch= 35   train_loss= 1.818   train_acc= 1.000   test_loss=2.743   test_acc= 0.667\n",
      "epoch= 36   train_loss= 1.806   train_acc= 1.000   test_loss=2.693   test_acc= 0.667\n",
      "epoch= 37   train_loss= 1.795   train_acc= 1.000   test_loss=2.692   test_acc= 0.667\n",
      "epoch= 38   train_loss= 1.785   train_acc= 1.000   test_loss=2.708   test_acc= 0.667\n",
      "epoch= 39   train_loss= 1.776   train_acc= 1.000   test_loss=2.711   test_acc= 0.667\n",
      "epoch= 40   train_loss= 1.766   train_acc= 1.000   test_loss=2.711   test_acc= 0.667\n",
      "epoch= 41   train_loss= 1.753   train_acc= 1.000   test_loss=2.717   test_acc= 0.667\n",
      "epoch= 42   train_loss= 1.746   train_acc= 1.000   test_loss=2.680   test_acc= 0.667\n",
      "epoch= 43   train_loss= 1.732   train_acc= 1.000   test_loss=2.636   test_acc= 0.667\n",
      "epoch= 44   train_loss= 1.723   train_acc= 1.000   test_loss=2.654   test_acc= 0.667\n",
      "epoch= 45   train_loss= 1.711   train_acc= 1.000   test_loss=2.618   test_acc= 0.667\n",
      "epoch= 46   train_loss= 1.703   train_acc= 1.000   test_loss=2.646   test_acc= 0.667\n",
      "epoch= 47   train_loss= 1.693   train_acc= 1.000   test_loss=2.609   test_acc= 0.667\n",
      "epoch= 48   train_loss= 1.682   train_acc= 1.000   test_loss=2.628   test_acc= 0.667\n",
      "epoch= 49   train_loss= 1.672   train_acc= 1.000   test_loss=2.623   test_acc= 0.667\n",
      "run time: 0.43757563829421997 min\n",
      "test_acc=0.667\n",
      "run= 1   fold= 6\n",
      "epoch= 0   train_loss= 2.901   train_acc= 0.614   test_loss=2.784   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.613   train_acc= 0.867   test_loss=2.518   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.517   train_acc= 0.904   test_loss=2.471   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.407   train_acc= 0.976   test_loss=2.394   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.361   train_acc= 0.952   test_loss=2.371   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.324   train_acc= 0.964   test_loss=2.342   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.295   train_acc= 0.976   test_loss=2.313   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.248   train_acc= 0.988   test_loss=2.296   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.232   train_acc= 1.000   test_loss=2.287   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.203   train_acc= 0.988   test_loss=2.218   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.191   train_acc= 0.988   test_loss=2.204   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.161   train_acc= 1.000   test_loss=2.188   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.136   train_acc= 1.000   test_loss=2.166   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.122   train_acc= 1.000   test_loss=2.158   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.104   train_acc= 1.000   test_loss=2.144   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.090   train_acc= 1.000   test_loss=2.116   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.071   train_acc= 1.000   test_loss=2.106   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.053   train_acc= 1.000   test_loss=2.083   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.047   train_acc= 1.000   test_loss=2.065   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.025   train_acc= 1.000   test_loss=2.064   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.009   train_acc= 1.000   test_loss=2.045   test_acc= 1.000\n",
      "epoch= 21   train_loss= 1.999   train_acc= 1.000   test_loss=2.026   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.979   train_acc= 1.000   test_loss=2.015   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.977   train_acc= 0.988   test_loss=2.003   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.955   train_acc= 1.000   test_loss=1.986   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.939   train_acc= 1.000   test_loss=1.964   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.929   train_acc= 1.000   test_loss=1.957   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.914   train_acc= 1.000   test_loss=1.945   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.900   train_acc= 1.000   test_loss=1.931   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.898   train_acc= 1.000   test_loss=1.909   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.889   train_acc= 0.988   test_loss=1.894   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.869   train_acc= 1.000   test_loss=1.905   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.856   train_acc= 1.000   test_loss=1.878   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.841   train_acc= 1.000   test_loss=1.875   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.834   train_acc= 1.000   test_loss=1.860   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.823   train_acc= 1.000   test_loss=1.847   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.807   train_acc= 1.000   test_loss=1.838   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.796   train_acc= 1.000   test_loss=1.823   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.787   train_acc= 1.000   test_loss=1.819   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.773   train_acc= 1.000   test_loss=1.802   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.765   train_acc= 1.000   test_loss=1.792   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.752   train_acc= 1.000   test_loss=1.779   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.744   train_acc= 1.000   test_loss=1.770   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.735   train_acc= 1.000   test_loss=1.766   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.722   train_acc= 1.000   test_loss=1.752   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.714   train_acc= 1.000   test_loss=1.743   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.702   train_acc= 1.000   test_loss=1.731   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.693   train_acc= 1.000   test_loss=1.722   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.684   train_acc= 1.000   test_loss=1.709   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.676   train_acc= 1.000   test_loss=1.698   test_acc= 1.000\n",
      "run time: 0.43953709602355956 min\n",
      "test_acc=1.000\n",
      "run= 1   fold= 7\n",
      "epoch= 0   train_loss= 2.909   train_acc= 0.663   test_loss=2.818   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.651   train_acc= 0.855   test_loss=2.862   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.537   train_acc= 0.904   test_loss=2.659   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.396   train_acc= 0.988   test_loss=2.867   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.379   train_acc= 0.964   test_loss=2.807   test_acc= 0.556\n",
      "epoch= 5   train_loss= 2.331   train_acc= 1.000   test_loss=2.597   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.291   train_acc= 0.976   test_loss=2.618   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.271   train_acc= 0.976   test_loss=2.508   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.214   train_acc= 1.000   test_loss=2.572   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.196   train_acc= 1.000   test_loss=2.472   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.182   train_acc= 1.000   test_loss=2.485   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.161   train_acc= 0.988   test_loss=2.540   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.144   train_acc= 1.000   test_loss=2.487   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.128   train_acc= 0.988   test_loss=2.506   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 14   train_loss= 2.102   train_acc= 1.000   test_loss=2.467   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.090   train_acc= 1.000   test_loss=2.434   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.064   train_acc= 1.000   test_loss=2.426   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.050   train_acc= 1.000   test_loss=2.414   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.039   train_acc= 1.000   test_loss=2.347   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.021   train_acc= 1.000   test_loss=2.329   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.010   train_acc= 1.000   test_loss=2.356   test_acc= 0.778\n",
      "epoch= 21   train_loss= 1.994   train_acc= 1.000   test_loss=2.333   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.978   train_acc= 1.000   test_loss=2.335   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.974   train_acc= 1.000   test_loss=2.275   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.952   train_acc= 1.000   test_loss=2.318   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.946   train_acc= 1.000   test_loss=2.307   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.932   train_acc= 1.000   test_loss=2.287   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.914   train_acc= 1.000   test_loss=2.252   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.904   train_acc= 1.000   test_loss=2.264   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.895   train_acc= 1.000   test_loss=2.252   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.875   train_acc= 1.000   test_loss=2.257   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.875   train_acc= 1.000   test_loss=2.162   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.858   train_acc= 1.000   test_loss=2.171   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.845   train_acc= 1.000   test_loss=2.208   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.831   train_acc= 1.000   test_loss=2.168   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.818   train_acc= 1.000   test_loss=2.175   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.809   train_acc= 1.000   test_loss=2.180   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.796   train_acc= 1.000   test_loss=2.129   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.786   train_acc= 1.000   test_loss=2.113   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.776   train_acc= 1.000   test_loss=2.106   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.764   train_acc= 1.000   test_loss=2.086   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.759   train_acc= 1.000   test_loss=2.137   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.749   train_acc= 1.000   test_loss=2.106   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.733   train_acc= 1.000   test_loss=2.092   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.729   train_acc= 1.000   test_loss=2.131   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.712   train_acc= 1.000   test_loss=2.078   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.702   train_acc= 1.000   test_loss=2.049   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.695   train_acc= 1.000   test_loss=2.011   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.685   train_acc= 1.000   test_loss=2.003   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.682   train_acc= 1.000   test_loss=1.927   test_acc= 0.889\n",
      "run time: 0.4228022456169128 min\n",
      "test_acc=0.889\n",
      "run= 1   fold= 8\n",
      "epoch= 0   train_loss= 2.905   train_acc= 0.614   test_loss=2.746   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.607   train_acc= 0.904   test_loss=2.594   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.497   train_acc= 0.940   test_loss=2.581   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.392   train_acc= 0.976   test_loss=2.614   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.365   train_acc= 0.976   test_loss=2.488   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.314   train_acc= 0.976   test_loss=2.389   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.285   train_acc= 1.000   test_loss=2.493   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.296   train_acc= 0.976   test_loss=2.390   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.222   train_acc= 0.988   test_loss=2.390   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.219   train_acc= 0.988   test_loss=2.355   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.181   train_acc= 1.000   test_loss=2.279   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.165   train_acc= 1.000   test_loss=2.312   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.140   train_acc= 1.000   test_loss=2.248   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.124   train_acc= 1.000   test_loss=2.260   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.100   train_acc= 1.000   test_loss=2.208   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.093   train_acc= 1.000   test_loss=2.225   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.084   train_acc= 1.000   test_loss=2.185   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.062   train_acc= 1.000   test_loss=2.185   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.051   train_acc= 1.000   test_loss=2.159   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.035   train_acc= 1.000   test_loss=2.172   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.023   train_acc= 1.000   test_loss=2.127   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.002   train_acc= 1.000   test_loss=2.121   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.989   train_acc= 1.000   test_loss=2.108   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.975   train_acc= 1.000   test_loss=2.101   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.958   train_acc= 1.000   test_loss=2.097   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.946   train_acc= 1.000   test_loss=2.076   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.931   train_acc= 1.000   test_loss=2.056   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.920   train_acc= 1.000   test_loss=2.032   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.905   train_acc= 1.000   test_loss=2.029   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.900   train_acc= 1.000   test_loss=2.015   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.884   train_acc= 1.000   test_loss=2.008   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.869   train_acc= 1.000   test_loss=1.996   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.860   train_acc= 1.000   test_loss=1.996   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.850   train_acc= 1.000   test_loss=1.993   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.834   train_acc= 1.000   test_loss=1.975   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.826   train_acc= 1.000   test_loss=1.949   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.810   train_acc= 1.000   test_loss=1.935   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.801   train_acc= 1.000   test_loss=1.930   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.789   train_acc= 1.000   test_loss=1.909   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.780   train_acc= 1.000   test_loss=1.929   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.768   train_acc= 1.000   test_loss=1.916   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.756   train_acc= 1.000   test_loss=1.889   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.748   train_acc= 1.000   test_loss=1.887   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.738   train_acc= 1.000   test_loss=1.857   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.727   train_acc= 1.000   test_loss=1.819   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.718   train_acc= 1.000   test_loss=1.825   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.708   train_acc= 1.000   test_loss=1.838   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.700   train_acc= 1.000   test_loss=1.795   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.690   train_acc= 1.000   test_loss=1.828   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.678   train_acc= 1.000   test_loss=1.813   test_acc= 1.000\n",
      "run time: 0.4162963151931763 min\n",
      "test_acc=1.000\n",
      "run= 1   fold= 9\n",
      "epoch= 0   train_loss= 2.995   train_acc= 0.627   test_loss=2.719   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.645   train_acc= 0.880   test_loss=2.645   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.527   train_acc= 0.940   test_loss=2.538   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.417   train_acc= 0.940   test_loss=2.504   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.397   train_acc= 0.976   test_loss=2.457   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.348   train_acc= 0.976   test_loss=2.447   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.299   train_acc= 0.988   test_loss=2.414   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.279   train_acc= 0.976   test_loss=2.382   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.245   train_acc= 1.000   test_loss=2.368   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.198   train_acc= 1.000   test_loss=2.347   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 10   train_loss= 2.179   train_acc= 1.000   test_loss=2.331   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.161   train_acc= 1.000   test_loss=2.303   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.149   train_acc= 0.988   test_loss=2.284   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.124   train_acc= 1.000   test_loss=2.271   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.106   train_acc= 1.000   test_loss=2.249   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.085   train_acc= 1.000   test_loss=2.229   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.066   train_acc= 1.000   test_loss=2.215   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.051   train_acc= 1.000   test_loss=2.198   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.042   train_acc= 1.000   test_loss=2.182   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.024   train_acc= 1.000   test_loss=2.171   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.008   train_acc= 1.000   test_loss=2.153   test_acc= 1.000\n",
      "epoch= 21   train_loss= 1.999   train_acc= 1.000   test_loss=2.137   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.983   train_acc= 1.000   test_loss=2.129   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.968   train_acc= 1.000   test_loss=2.119   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.959   train_acc= 1.000   test_loss=2.101   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.946   train_acc= 1.000   test_loss=2.088   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.933   train_acc= 1.000   test_loss=2.068   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.917   train_acc= 1.000   test_loss=2.052   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.905   train_acc= 1.000   test_loss=2.041   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.892   train_acc= 1.000   test_loss=2.031   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.879   train_acc= 1.000   test_loss=2.019   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.867   train_acc= 1.000   test_loss=2.004   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.854   train_acc= 1.000   test_loss=1.992   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.850   train_acc= 1.000   test_loss=1.988   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.835   train_acc= 1.000   test_loss=1.975   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.823   train_acc= 1.000   test_loss=1.964   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.812   train_acc= 1.000   test_loss=1.942   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.798   train_acc= 1.000   test_loss=1.931   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.788   train_acc= 1.000   test_loss=1.920   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.774   train_acc= 1.000   test_loss=1.907   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.768   train_acc= 1.000   test_loss=1.900   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.756   train_acc= 1.000   test_loss=1.889   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.745   train_acc= 1.000   test_loss=1.879   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.734   train_acc= 1.000   test_loss=1.870   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.723   train_acc= 1.000   test_loss=1.861   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.713   train_acc= 1.000   test_loss=1.851   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.709   train_acc= 1.000   test_loss=1.843   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.695   train_acc= 1.000   test_loss=1.833   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.686   train_acc= 1.000   test_loss=1.821   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.676   train_acc= 1.000   test_loss=1.811   test_acc= 1.000\n",
      "run time: 0.41794711351394653 min\n",
      "test_acc=1.000\n",
      "run= 2   fold= 0\n",
      "epoch= 0   train_loss= 3.029   train_acc= 0.573   test_loss=2.809   test_acc= 0.700\n",
      "epoch= 1   train_loss= 2.620   train_acc= 0.829   test_loss=2.721   test_acc= 0.700\n",
      "epoch= 2   train_loss= 2.526   train_acc= 0.866   test_loss=2.728   test_acc= 0.800\n",
      "epoch= 3   train_loss= 2.457   train_acc= 0.963   test_loss=2.618   test_acc= 0.700\n",
      "epoch= 4   train_loss= 2.371   train_acc= 0.976   test_loss=2.590   test_acc= 0.800\n",
      "epoch= 5   train_loss= 2.332   train_acc= 0.976   test_loss=2.515   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.291   train_acc= 0.963   test_loss=2.478   test_acc= 0.800\n",
      "epoch= 7   train_loss= 2.253   train_acc= 1.000   test_loss=2.458   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.240   train_acc= 0.988   test_loss=2.435   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.225   train_acc= 0.976   test_loss=2.395   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.194   train_acc= 0.988   test_loss=2.366   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.168   train_acc= 1.000   test_loss=2.358   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.140   train_acc= 1.000   test_loss=2.304   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.126   train_acc= 1.000   test_loss=2.289   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.106   train_acc= 1.000   test_loss=2.281   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.102   train_acc= 1.000   test_loss=2.265   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.074   train_acc= 1.000   test_loss=2.257   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.055   train_acc= 1.000   test_loss=2.252   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.048   train_acc= 1.000   test_loss=2.221   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.030   train_acc= 1.000   test_loss=2.209   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.015   train_acc= 1.000   test_loss=2.201   test_acc= 0.900\n",
      "epoch= 21   train_loss= 1.997   train_acc= 1.000   test_loss=2.181   test_acc= 0.900\n",
      "epoch= 22   train_loss= 1.991   train_acc= 1.000   test_loss=2.170   test_acc= 0.900\n",
      "epoch= 23   train_loss= 1.975   train_acc= 1.000   test_loss=2.160   test_acc= 0.900\n",
      "epoch= 24   train_loss= 1.964   train_acc= 1.000   test_loss=2.151   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.957   train_acc= 0.988   test_loss=2.119   test_acc= 0.900\n",
      "epoch= 26   train_loss= 1.931   train_acc= 1.000   test_loss=2.107   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.919   train_acc= 1.000   test_loss=2.087   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.906   train_acc= 1.000   test_loss=2.064   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.892   train_acc= 1.000   test_loss=2.053   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.882   train_acc= 1.000   test_loss=2.040   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.875   train_acc= 1.000   test_loss=2.037   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.861   train_acc= 1.000   test_loss=2.015   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.846   train_acc= 1.000   test_loss=2.015   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.837   train_acc= 1.000   test_loss=1.993   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.828   train_acc= 1.000   test_loss=1.992   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.814   train_acc= 1.000   test_loss=1.989   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.803   train_acc= 1.000   test_loss=1.970   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.792   train_acc= 1.000   test_loss=1.965   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.782   train_acc= 1.000   test_loss=1.955   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.770   train_acc= 1.000   test_loss=1.940   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.765   train_acc= 1.000   test_loss=1.920   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.757   train_acc= 1.000   test_loss=1.922   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.740   train_acc= 1.000   test_loss=1.901   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.733   train_acc= 1.000   test_loss=1.900   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.720   train_acc= 1.000   test_loss=1.880   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.710   train_acc= 1.000   test_loss=1.871   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.699   train_acc= 1.000   test_loss=1.863   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.690   train_acc= 1.000   test_loss=1.852   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.681   train_acc= 1.000   test_loss=1.839   test_acc= 0.900\n",
      "run time: 0.4490312258402506 min\n",
      "test_acc=0.900\n",
      "run= 2   fold= 1\n",
      "epoch= 0   train_loss= 2.952   train_acc= 0.622   test_loss=2.978   test_acc= 0.500\n",
      "epoch= 1   train_loss= 2.651   train_acc= 0.817   test_loss=3.004   test_acc= 0.700\n",
      "epoch= 2   train_loss= 2.475   train_acc= 0.927   test_loss=2.891   test_acc= 0.700\n",
      "epoch= 3   train_loss= 2.412   train_acc= 0.951   test_loss=2.833   test_acc= 0.700\n",
      "epoch= 4   train_loss= 2.340   train_acc= 1.000   test_loss=2.854   test_acc= 0.700\n",
      "epoch= 5   train_loss= 2.302   train_acc= 1.000   test_loss=2.851   test_acc= 0.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 6   train_loss= 2.263   train_acc= 1.000   test_loss=2.845   test_acc= 0.700\n",
      "epoch= 7   train_loss= 2.247   train_acc= 1.000   test_loss=2.805   test_acc= 0.700\n",
      "epoch= 8   train_loss= 2.217   train_acc= 1.000   test_loss=2.835   test_acc= 0.700\n",
      "epoch= 9   train_loss= 2.201   train_acc= 0.988   test_loss=2.844   test_acc= 0.700\n",
      "epoch= 10   train_loss= 2.169   train_acc= 1.000   test_loss=2.849   test_acc= 0.700\n",
      "epoch= 11   train_loss= 2.159   train_acc= 1.000   test_loss=2.816   test_acc= 0.700\n",
      "epoch= 12   train_loss= 2.138   train_acc= 1.000   test_loss=2.790   test_acc= 0.700\n",
      "epoch= 13   train_loss= 2.122   train_acc= 1.000   test_loss=2.803   test_acc= 0.700\n",
      "epoch= 14   train_loss= 2.099   train_acc= 1.000   test_loss=2.779   test_acc= 0.700\n",
      "epoch= 15   train_loss= 2.086   train_acc= 1.000   test_loss=2.777   test_acc= 0.700\n",
      "epoch= 16   train_loss= 2.078   train_acc= 1.000   test_loss=2.727   test_acc= 0.700\n",
      "epoch= 17   train_loss= 2.058   train_acc= 1.000   test_loss=2.729   test_acc= 0.700\n",
      "epoch= 18   train_loss= 2.048   train_acc= 1.000   test_loss=2.695   test_acc= 0.700\n",
      "epoch= 19   train_loss= 2.022   train_acc= 1.000   test_loss=2.693   test_acc= 0.700\n",
      "epoch= 20   train_loss= 2.008   train_acc= 1.000   test_loss=2.706   test_acc= 0.700\n",
      "epoch= 21   train_loss= 2.002   train_acc= 1.000   test_loss=2.694   test_acc= 0.700\n",
      "epoch= 22   train_loss= 1.979   train_acc= 1.000   test_loss=2.683   test_acc= 0.700\n",
      "epoch= 23   train_loss= 1.970   train_acc= 1.000   test_loss=2.688   test_acc= 0.700\n",
      "epoch= 24   train_loss= 1.955   train_acc= 1.000   test_loss=2.684   test_acc= 0.700\n",
      "epoch= 25   train_loss= 1.941   train_acc= 1.000   test_loss=2.674   test_acc= 0.700\n",
      "epoch= 26   train_loss= 1.930   train_acc= 1.000   test_loss=2.647   test_acc= 0.700\n",
      "epoch= 27   train_loss= 1.920   train_acc= 1.000   test_loss=2.666   test_acc= 0.700\n",
      "epoch= 28   train_loss= 1.905   train_acc= 1.000   test_loss=2.656   test_acc= 0.700\n",
      "epoch= 29   train_loss= 1.893   train_acc= 1.000   test_loss=2.642   test_acc= 0.700\n",
      "epoch= 30   train_loss= 1.882   train_acc= 1.000   test_loss=2.628   test_acc= 0.700\n",
      "epoch= 31   train_loss= 1.870   train_acc= 1.000   test_loss=2.592   test_acc= 0.700\n",
      "epoch= 32   train_loss= 1.854   train_acc= 1.000   test_loss=2.588   test_acc= 0.700\n",
      "epoch= 33   train_loss= 1.842   train_acc= 1.000   test_loss=2.582   test_acc= 0.700\n",
      "epoch= 34   train_loss= 1.834   train_acc= 1.000   test_loss=2.581   test_acc= 0.700\n",
      "epoch= 35   train_loss= 1.829   train_acc= 1.000   test_loss=2.540   test_acc= 0.700\n",
      "epoch= 36   train_loss= 1.809   train_acc= 1.000   test_loss=2.536   test_acc= 0.700\n",
      "epoch= 37   train_loss= 1.800   train_acc= 1.000   test_loss=2.533   test_acc= 0.700\n",
      "epoch= 38   train_loss= 1.788   train_acc= 1.000   test_loss=2.524   test_acc= 0.700\n",
      "epoch= 39   train_loss= 1.776   train_acc= 1.000   test_loss=2.519   test_acc= 0.700\n",
      "epoch= 40   train_loss= 1.767   train_acc= 1.000   test_loss=2.497   test_acc= 0.700\n",
      "epoch= 41   train_loss= 1.757   train_acc= 1.000   test_loss=2.485   test_acc= 0.700\n",
      "epoch= 42   train_loss= 1.746   train_acc= 1.000   test_loss=2.464   test_acc= 0.700\n",
      "epoch= 43   train_loss= 1.738   train_acc= 1.000   test_loss=2.452   test_acc= 0.700\n",
      "epoch= 44   train_loss= 1.724   train_acc= 1.000   test_loss=2.462   test_acc= 0.700\n",
      "epoch= 45   train_loss= 1.716   train_acc= 1.000   test_loss=2.462   test_acc= 0.700\n",
      "epoch= 46   train_loss= 1.704   train_acc= 1.000   test_loss=2.458   test_acc= 0.700\n",
      "epoch= 47   train_loss= 1.697   train_acc= 1.000   test_loss=2.441   test_acc= 0.700\n",
      "epoch= 48   train_loss= 1.689   train_acc= 1.000   test_loss=2.438   test_acc= 0.700\n",
      "epoch= 49   train_loss= 1.679   train_acc= 1.000   test_loss=2.436   test_acc= 0.700\n",
      "run time: 0.4353485306104024 min\n",
      "test_acc=0.700\n",
      "run= 2   fold= 2\n",
      "epoch= 0   train_loss= 2.978   train_acc= 0.627   test_loss=3.122   test_acc= 0.444\n",
      "epoch= 1   train_loss= 2.694   train_acc= 0.819   test_loss=2.873   test_acc= 0.556\n",
      "epoch= 2   train_loss= 2.556   train_acc= 0.892   test_loss=2.724   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.442   train_acc= 0.952   test_loss=2.623   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.379   train_acc= 0.952   test_loss=2.646   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.352   train_acc= 0.964   test_loss=2.566   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.312   train_acc= 0.964   test_loss=2.582   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.277   train_acc= 0.988   test_loss=2.509   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.251   train_acc= 0.976   test_loss=2.446   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.216   train_acc= 0.988   test_loss=2.439   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.177   train_acc= 1.000   test_loss=2.395   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.160   train_acc= 1.000   test_loss=2.377   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.143   train_acc= 1.000   test_loss=2.328   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.115   train_acc= 1.000   test_loss=2.323   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.110   train_acc= 1.000   test_loss=2.284   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.088   train_acc= 1.000   test_loss=2.263   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.084   train_acc= 0.988   test_loss=2.240   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.052   train_acc= 1.000   test_loss=2.230   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.045   train_acc= 1.000   test_loss=2.195   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.026   train_acc= 1.000   test_loss=2.191   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.012   train_acc= 1.000   test_loss=2.217   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.003   train_acc= 1.000   test_loss=2.179   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.989   train_acc= 1.000   test_loss=2.159   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.972   train_acc= 1.000   test_loss=2.139   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.960   train_acc= 1.000   test_loss=2.118   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.942   train_acc= 1.000   test_loss=2.113   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.932   train_acc= 1.000   test_loss=2.113   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.918   train_acc= 1.000   test_loss=2.092   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.906   train_acc= 1.000   test_loss=2.100   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.891   train_acc= 1.000   test_loss=2.076   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.879   train_acc= 1.000   test_loss=2.064   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.871   train_acc= 1.000   test_loss=2.055   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.857   train_acc= 1.000   test_loss=2.037   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.853   train_acc= 1.000   test_loss=2.029   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.833   train_acc= 1.000   test_loss=2.002   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.819   train_acc= 1.000   test_loss=1.994   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.808   train_acc= 1.000   test_loss=1.982   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.795   train_acc= 1.000   test_loss=1.976   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.787   train_acc= 1.000   test_loss=1.958   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.773   train_acc= 1.000   test_loss=1.953   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.768   train_acc= 1.000   test_loss=1.942   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.763   train_acc= 1.000   test_loss=1.937   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.747   train_acc= 1.000   test_loss=1.926   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.734   train_acc= 1.000   test_loss=1.890   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.726   train_acc= 1.000   test_loss=1.911   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.714   train_acc= 1.000   test_loss=1.892   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.705   train_acc= 1.000   test_loss=1.899   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.696   train_acc= 1.000   test_loss=1.876   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.688   train_acc= 1.000   test_loss=1.865   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.676   train_acc= 1.000   test_loss=1.857   test_acc= 0.889\n",
      "run time: 0.4211353739102682 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 3\n",
      "epoch= 0   train_loss= 2.920   train_acc= 0.663   test_loss=2.745   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.621   train_acc= 0.855   test_loss=2.676   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2   train_loss= 2.523   train_acc= 0.904   test_loss=2.527   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.417   train_acc= 0.976   test_loss=2.503   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.366   train_acc= 0.952   test_loss=2.446   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.305   train_acc= 0.988   test_loss=2.392   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.295   train_acc= 0.976   test_loss=2.388   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.266   train_acc= 0.988   test_loss=2.372   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.204   train_acc= 1.000   test_loss=2.304   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.209   train_acc= 0.988   test_loss=2.272   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.176   train_acc= 1.000   test_loss=2.263   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.157   train_acc= 1.000   test_loss=2.243   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.139   train_acc= 1.000   test_loss=2.235   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.137   train_acc= 0.988   test_loss=2.211   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.102   train_acc= 0.988   test_loss=2.207   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.081   train_acc= 1.000   test_loss=2.206   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.061   train_acc= 1.000   test_loss=2.168   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.050   train_acc= 1.000   test_loss=2.129   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.030   train_acc= 1.000   test_loss=2.119   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.026   train_acc= 1.000   test_loss=2.117   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.016   train_acc= 1.000   test_loss=2.107   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.001   train_acc= 1.000   test_loss=2.053   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.980   train_acc= 1.000   test_loss=2.091   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.962   train_acc= 1.000   test_loss=2.049   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.952   train_acc= 1.000   test_loss=2.036   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.936   train_acc= 1.000   test_loss=2.042   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.929   train_acc= 1.000   test_loss=2.005   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.908   train_acc= 1.000   test_loss=1.991   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.898   train_acc= 1.000   test_loss=1.988   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.884   train_acc= 1.000   test_loss=1.972   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.873   train_acc= 1.000   test_loss=1.954   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.859   train_acc= 1.000   test_loss=1.976   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.848   train_acc= 1.000   test_loss=1.947   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.839   train_acc= 1.000   test_loss=1.927   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.830   train_acc= 1.000   test_loss=1.917   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.815   train_acc= 1.000   test_loss=1.908   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.802   train_acc= 1.000   test_loss=1.896   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.794   train_acc= 1.000   test_loss=1.883   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.782   train_acc= 1.000   test_loss=1.870   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.772   train_acc= 1.000   test_loss=1.891   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.757   train_acc= 1.000   test_loss=1.856   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.755   train_acc= 1.000   test_loss=1.837   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.739   train_acc= 1.000   test_loss=1.835   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.730   train_acc= 1.000   test_loss=1.826   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.718   train_acc= 1.000   test_loss=1.816   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.709   train_acc= 1.000   test_loss=1.805   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.701   train_acc= 1.000   test_loss=1.796   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.688   train_acc= 1.000   test_loss=1.782   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.681   train_acc= 1.000   test_loss=1.775   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.674   train_acc= 1.000   test_loss=1.771   test_acc= 1.000\n",
      "run time: 0.4181962569554647 min\n",
      "test_acc=1.000\n",
      "run= 2   fold= 4\n",
      "epoch= 0   train_loss= 2.887   train_acc= 0.687   test_loss=2.830   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.657   train_acc= 0.892   test_loss=2.935   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.529   train_acc= 0.904   test_loss=2.684   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.429   train_acc= 0.952   test_loss=2.630   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.412   train_acc= 0.940   test_loss=2.609   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.326   train_acc= 0.976   test_loss=2.569   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.294   train_acc= 0.988   test_loss=2.519   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.265   train_acc= 0.988   test_loss=2.477   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.244   train_acc= 0.976   test_loss=2.431   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.209   train_acc= 0.988   test_loss=2.378   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.170   train_acc= 1.000   test_loss=2.383   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.156   train_acc= 1.000   test_loss=2.379   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.136   train_acc= 1.000   test_loss=2.381   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.117   train_acc= 1.000   test_loss=2.350   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.097   train_acc= 1.000   test_loss=2.387   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.098   train_acc= 1.000   test_loss=2.308   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.071   train_acc= 1.000   test_loss=2.304   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.054   train_acc= 1.000   test_loss=2.289   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.043   train_acc= 1.000   test_loss=2.282   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.024   train_acc= 1.000   test_loss=2.247   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.014   train_acc= 1.000   test_loss=2.271   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.997   train_acc= 1.000   test_loss=2.286   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.982   train_acc= 1.000   test_loss=2.216   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.971   train_acc= 1.000   test_loss=2.191   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.954   train_acc= 1.000   test_loss=2.175   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.938   train_acc= 1.000   test_loss=2.162   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.930   train_acc= 1.000   test_loss=2.143   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.914   train_acc= 1.000   test_loss=2.137   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.905   train_acc= 1.000   test_loss=2.129   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.888   train_acc= 1.000   test_loss=2.123   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.877   train_acc= 1.000   test_loss=2.108   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.866   train_acc= 1.000   test_loss=2.094   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.851   train_acc= 1.000   test_loss=2.097   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.840   train_acc= 1.000   test_loss=2.090   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.832   train_acc= 1.000   test_loss=2.109   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.818   train_acc= 1.000   test_loss=2.082   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.811   train_acc= 1.000   test_loss=2.054   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.798   train_acc= 1.000   test_loss=2.044   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.786   train_acc= 1.000   test_loss=2.017   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.774   train_acc= 1.000   test_loss=2.018   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.763   train_acc= 1.000   test_loss=2.001   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.758   train_acc= 1.000   test_loss=2.033   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.745   train_acc= 1.000   test_loss=2.008   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.732   train_acc= 1.000   test_loss=1.989   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.726   train_acc= 1.000   test_loss=1.981   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.712   train_acc= 1.000   test_loss=1.981   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.701   train_acc= 1.000   test_loss=1.977   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.692   train_acc= 1.000   test_loss=1.935   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 48   train_loss= 1.683   train_acc= 1.000   test_loss=1.933   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.672   train_acc= 1.000   test_loss=1.913   test_acc= 0.889\n",
      "run time: 0.4215832193692525 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 5\n",
      "epoch= 0   train_loss= 2.911   train_acc= 0.614   test_loss=2.799   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.620   train_acc= 0.867   test_loss=2.754   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.497   train_acc= 0.952   test_loss=2.629   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.437   train_acc= 0.940   test_loss=2.608   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.379   train_acc= 0.952   test_loss=2.535   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.328   train_acc= 0.976   test_loss=2.530   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.298   train_acc= 0.988   test_loss=2.451   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.258   train_acc= 0.988   test_loss=2.435   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.225   train_acc= 0.988   test_loss=2.392   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.196   train_acc= 1.000   test_loss=2.376   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.182   train_acc= 1.000   test_loss=2.391   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.159   train_acc= 1.000   test_loss=2.358   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.136   train_acc= 1.000   test_loss=2.300   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.113   train_acc= 1.000   test_loss=2.337   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.102   train_acc= 1.000   test_loss=2.285   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.081   train_acc= 1.000   test_loss=2.270   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.065   train_acc= 1.000   test_loss=2.256   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.053   train_acc= 1.000   test_loss=2.228   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.032   train_acc= 1.000   test_loss=2.225   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.021   train_acc= 1.000   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.012   train_acc= 1.000   test_loss=2.198   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.990   train_acc= 1.000   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.988   train_acc= 1.000   test_loss=2.175   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.962   train_acc= 1.000   test_loss=2.151   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.949   train_acc= 1.000   test_loss=2.156   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.935   train_acc= 1.000   test_loss=2.135   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.929   train_acc= 1.000   test_loss=2.109   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.917   train_acc= 1.000   test_loss=2.112   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.899   train_acc= 1.000   test_loss=2.080   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.885   train_acc= 1.000   test_loss=2.080   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.873   train_acc= 1.000   test_loss=2.060   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.866   train_acc= 1.000   test_loss=2.043   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.854   train_acc= 1.000   test_loss=2.051   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.843   train_acc= 1.000   test_loss=2.020   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.829   train_acc= 1.000   test_loss=2.021   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.816   train_acc= 1.000   test_loss=2.004   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.808   train_acc= 1.000   test_loss=2.007   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.797   train_acc= 1.000   test_loss=1.966   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.795   train_acc= 1.000   test_loss=1.960   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.774   train_acc= 1.000   test_loss=1.944   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.761   train_acc= 1.000   test_loss=1.945   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.752   train_acc= 1.000   test_loss=1.938   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.738   train_acc= 1.000   test_loss=1.920   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.730   train_acc= 1.000   test_loss=1.912   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.721   train_acc= 1.000   test_loss=1.905   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.710   train_acc= 1.000   test_loss=1.898   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.701   train_acc= 1.000   test_loss=1.902   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.692   train_acc= 1.000   test_loss=1.884   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.678   train_acc= 1.000   test_loss=1.873   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.671   train_acc= 1.000   test_loss=1.863   test_acc= 0.889\n",
      "run time: 0.44539800882339475 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 6\n",
      "epoch= 0   train_loss= 3.057   train_acc= 0.482   test_loss=2.796   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.663   train_acc= 0.855   test_loss=2.635   test_acc= 0.667\n",
      "epoch= 2   train_loss= 2.521   train_acc= 0.940   test_loss=2.596   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.447   train_acc= 0.940   test_loss=2.570   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.372   train_acc= 0.976   test_loss=2.546   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.353   train_acc= 0.964   test_loss=2.517   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.312   train_acc= 0.976   test_loss=2.485   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.281   train_acc= 0.976   test_loss=2.505   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.235   train_acc= 1.000   test_loss=2.466   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.216   train_acc= 0.988   test_loss=2.466   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.201   train_acc= 0.988   test_loss=2.435   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.187   train_acc= 0.988   test_loss=2.427   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.157   train_acc= 1.000   test_loss=2.412   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.131   train_acc= 1.000   test_loss=2.406   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.125   train_acc= 0.988   test_loss=2.381   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.104   train_acc= 1.000   test_loss=2.369   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.077   train_acc= 1.000   test_loss=2.347   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.065   train_acc= 1.000   test_loss=2.324   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.045   train_acc= 1.000   test_loss=2.316   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.032   train_acc= 1.000   test_loss=2.314   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.019   train_acc= 1.000   test_loss=2.309   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.006   train_acc= 1.000   test_loss=2.306   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.990   train_acc= 1.000   test_loss=2.254   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.975   train_acc= 1.000   test_loss=2.240   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.963   train_acc= 1.000   test_loss=2.228   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.949   train_acc= 1.000   test_loss=2.208   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.941   train_acc= 1.000   test_loss=2.173   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.929   train_acc= 1.000   test_loss=2.180   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.915   train_acc= 1.000   test_loss=2.161   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.902   train_acc= 1.000   test_loss=2.176   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.887   train_acc= 1.000   test_loss=2.136   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.873   train_acc= 1.000   test_loss=2.124   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.865   train_acc= 1.000   test_loss=2.120   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.852   train_acc= 1.000   test_loss=2.105   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.840   train_acc= 1.000   test_loss=2.093   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.828   train_acc= 1.000   test_loss=2.082   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.816   train_acc= 1.000   test_loss=2.070   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.803   train_acc= 1.000   test_loss=2.051   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.796   train_acc= 1.000   test_loss=2.072   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.785   train_acc= 1.000   test_loss=2.050   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.772   train_acc= 1.000   test_loss=2.039   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.762   train_acc= 1.000   test_loss=2.021   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.753   train_acc= 1.000   test_loss=2.004   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.741   train_acc= 1.000   test_loss=1.993   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 44   train_loss= 1.736   train_acc= 1.000   test_loss=1.989   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.720   train_acc= 1.000   test_loss=1.978   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.713   train_acc= 1.000   test_loss=1.977   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.703   train_acc= 1.000   test_loss=1.954   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.690   train_acc= 1.000   test_loss=1.934   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.684   train_acc= 1.000   test_loss=1.933   test_acc= 0.778\n",
      "run time: 0.43807835976282755 min\n",
      "test_acc=0.778\n",
      "run= 2   fold= 7\n",
      "epoch= 0   train_loss= 2.908   train_acc= 0.651   test_loss=2.976   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.633   train_acc= 0.819   test_loss=2.787   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.503   train_acc= 0.892   test_loss=2.850   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.454   train_acc= 0.928   test_loss=2.715   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.364   train_acc= 0.964   test_loss=2.770   test_acc= 0.667\n",
      "epoch= 5   train_loss= 2.352   train_acc= 0.952   test_loss=2.654   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.290   train_acc= 0.976   test_loss=2.588   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.280   train_acc= 0.976   test_loss=2.645   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.240   train_acc= 0.988   test_loss=2.541   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.202   train_acc= 1.000   test_loss=2.494   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.187   train_acc= 1.000   test_loss=2.473   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.170   train_acc= 1.000   test_loss=2.451   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.157   train_acc= 0.988   test_loss=2.447   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.135   train_acc= 0.988   test_loss=2.436   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.105   train_acc= 1.000   test_loss=2.448   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.099   train_acc= 1.000   test_loss=2.420   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.077   train_acc= 1.000   test_loss=2.370   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.072   train_acc= 1.000   test_loss=2.348   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.045   train_acc= 1.000   test_loss=2.412   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.031   train_acc= 1.000   test_loss=2.335   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.018   train_acc= 1.000   test_loss=2.335   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.001   train_acc= 1.000   test_loss=2.291   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.984   train_acc= 1.000   test_loss=2.300   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.975   train_acc= 1.000   test_loss=2.251   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.958   train_acc= 1.000   test_loss=2.269   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.942   train_acc= 1.000   test_loss=2.289   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.936   train_acc= 1.000   test_loss=2.280   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.918   train_acc= 1.000   test_loss=2.238   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.905   train_acc= 1.000   test_loss=2.218   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.892   train_acc= 1.000   test_loss=2.201   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.882   train_acc= 1.000   test_loss=2.237   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.872   train_acc= 1.000   test_loss=2.222   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.860   train_acc= 1.000   test_loss=2.175   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.845   train_acc= 1.000   test_loss=2.166   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.835   train_acc= 1.000   test_loss=2.125   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.824   train_acc= 1.000   test_loss=2.126   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.811   train_acc= 1.000   test_loss=2.123   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.798   train_acc= 1.000   test_loss=2.112   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.790   train_acc= 1.000   test_loss=2.101   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.779   train_acc= 1.000   test_loss=2.073   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.766   train_acc= 1.000   test_loss=2.053   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.763   train_acc= 1.000   test_loss=2.015   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.745   train_acc= 1.000   test_loss=2.031   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.736   train_acc= 1.000   test_loss=2.033   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.723   train_acc= 1.000   test_loss=1.992   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.717   train_acc= 1.000   test_loss=2.004   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.705   train_acc= 1.000   test_loss=1.987   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.698   train_acc= 1.000   test_loss=1.994   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.683   train_acc= 1.000   test_loss=1.986   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.679   train_acc= 1.000   test_loss=1.971   test_acc= 0.889\n",
      "run time: 0.47528165181477866 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 8\n",
      "epoch= 0   train_loss= 2.998   train_acc= 0.554   test_loss=2.834   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.605   train_acc= 0.880   test_loss=2.758   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.490   train_acc= 0.916   test_loss=2.639   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.417   train_acc= 0.928   test_loss=2.622   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.369   train_acc= 0.952   test_loss=2.588   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.328   train_acc= 0.964   test_loss=2.529   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.279   train_acc= 0.988   test_loss=2.518   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.266   train_acc= 0.976   test_loss=2.490   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.219   train_acc= 1.000   test_loss=2.460   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.208   train_acc= 0.988   test_loss=2.465   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.164   train_acc= 1.000   test_loss=2.436   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.150   train_acc= 1.000   test_loss=2.407   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.138   train_acc= 0.988   test_loss=2.395   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.112   train_acc= 1.000   test_loss=2.374   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.096   train_acc= 0.988   test_loss=2.360   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.077   train_acc= 1.000   test_loss=2.335   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.052   train_acc= 1.000   test_loss=2.326   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.040   train_acc= 1.000   test_loss=2.313   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.035   train_acc= 1.000   test_loss=2.297   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.009   train_acc= 1.000   test_loss=2.290   test_acc= 0.889\n",
      "epoch= 20   train_loss= 1.994   train_acc= 1.000   test_loss=2.270   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.990   train_acc= 1.000   test_loss=2.251   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.972   train_acc= 1.000   test_loss=2.238   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.953   train_acc= 1.000   test_loss=2.227   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.945   train_acc= 1.000   test_loss=2.213   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.930   train_acc= 1.000   test_loss=2.201   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.920   train_acc= 1.000   test_loss=2.196   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.902   train_acc= 1.000   test_loss=2.188   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.895   train_acc= 1.000   test_loss=2.173   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.879   train_acc= 1.000   test_loss=2.160   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.872   train_acc= 1.000   test_loss=2.145   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.855   train_acc= 1.000   test_loss=2.130   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.843   train_acc= 1.000   test_loss=2.118   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.834   train_acc= 1.000   test_loss=2.106   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.821   train_acc= 1.000   test_loss=2.084   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.810   train_acc= 1.000   test_loss=2.077   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.797   train_acc= 1.000   test_loss=2.063   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.790   train_acc= 1.000   test_loss=2.040   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.775   train_acc= 1.000   test_loss=2.032   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.771   train_acc= 1.000   test_loss=2.028   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 40   train_loss= 1.757   train_acc= 1.000   test_loss=2.019   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.746   train_acc= 1.000   test_loss=2.007   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.733   train_acc= 1.000   test_loss=1.997   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.725   train_acc= 1.000   test_loss=1.994   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.715   train_acc= 1.000   test_loss=1.981   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.704   train_acc= 1.000   test_loss=1.973   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.696   train_acc= 1.000   test_loss=1.959   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.684   train_acc= 1.000   test_loss=1.952   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.674   train_acc= 1.000   test_loss=1.940   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.667   train_acc= 1.000   test_loss=1.927   test_acc= 0.889\n",
      "run time: 0.4604421774546305 min\n",
      "test_acc=0.889\n",
      "run= 2   fold= 9\n",
      "epoch= 0   train_loss= 2.842   train_acc= 0.723   test_loss=3.064   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.681   train_acc= 0.843   test_loss=3.006   test_acc= 0.556\n",
      "epoch= 2   train_loss= 2.507   train_acc= 0.916   test_loss=3.018   test_acc= 0.556\n",
      "epoch= 3   train_loss= 2.397   train_acc= 0.964   test_loss=2.869   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.381   train_acc= 0.952   test_loss=2.617   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.360   train_acc= 0.952   test_loss=2.653   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.331   train_acc= 0.964   test_loss=2.522   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.257   train_acc= 0.988   test_loss=2.606   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.234   train_acc= 0.988   test_loss=2.508   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.212   train_acc= 0.988   test_loss=2.451   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.197   train_acc= 0.988   test_loss=2.456   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.168   train_acc= 0.988   test_loss=2.387   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.137   train_acc= 1.000   test_loss=2.439   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.126   train_acc= 1.000   test_loss=2.379   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.107   train_acc= 1.000   test_loss=2.503   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.099   train_acc= 0.988   test_loss=2.425   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.070   train_acc= 1.000   test_loss=2.318   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.062   train_acc= 1.000   test_loss=2.383   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.041   train_acc= 1.000   test_loss=2.365   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.027   train_acc= 1.000   test_loss=2.294   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.006   train_acc= 1.000   test_loss=2.310   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.005   train_acc= 1.000   test_loss=2.235   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.993   train_acc= 1.000   test_loss=2.267   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.969   train_acc= 1.000   test_loss=2.296   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.958   train_acc= 1.000   test_loss=2.260   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.945   train_acc= 1.000   test_loss=2.221   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.928   train_acc= 1.000   test_loss=2.205   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.912   train_acc= 1.000   test_loss=2.205   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.902   train_acc= 1.000   test_loss=2.215   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.890   train_acc= 1.000   test_loss=2.172   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.875   train_acc= 1.000   test_loss=2.129   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.866   train_acc= 1.000   test_loss=2.143   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.857   train_acc= 1.000   test_loss=2.136   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.844   train_acc= 1.000   test_loss=2.116   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.832   train_acc= 1.000   test_loss=2.106   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.822   train_acc= 1.000   test_loss=2.174   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.817   train_acc= 1.000   test_loss=2.091   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.795   train_acc= 1.000   test_loss=2.086   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.789   train_acc= 1.000   test_loss=2.055   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.774   train_acc= 1.000   test_loss=2.020   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.782   train_acc= 0.988   test_loss=2.051   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.756   train_acc= 1.000   test_loss=2.035   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.744   train_acc= 1.000   test_loss=2.054   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.734   train_acc= 1.000   test_loss=2.043   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.727   train_acc= 1.000   test_loss=1.994   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.712   train_acc= 1.000   test_loss=1.987   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.702   train_acc= 1.000   test_loss=1.988   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.696   train_acc= 1.000   test_loss=1.974   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.683   train_acc= 1.000   test_loss=1.962   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.676   train_acc= 1.000   test_loss=1.964   test_acc= 0.889\n",
      "run time: 0.45778672297795614 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 0\n",
      "epoch= 0   train_loss= 2.925   train_acc= 0.659   test_loss=2.756   test_acc= 0.700\n",
      "epoch= 1   train_loss= 2.603   train_acc= 0.878   test_loss=2.735   test_acc= 0.800\n",
      "epoch= 2   train_loss= 2.506   train_acc= 0.890   test_loss=2.686   test_acc= 0.800\n",
      "epoch= 3   train_loss= 2.424   train_acc= 0.939   test_loss=2.673   test_acc= 0.700\n",
      "epoch= 4   train_loss= 2.360   train_acc= 0.988   test_loss=2.674   test_acc= 0.800\n",
      "epoch= 5   train_loss= 2.326   train_acc= 1.000   test_loss=2.714   test_acc= 0.700\n",
      "epoch= 6   train_loss= 2.280   train_acc= 1.000   test_loss=2.703   test_acc= 0.700\n",
      "epoch= 7   train_loss= 2.255   train_acc= 0.988   test_loss=2.677   test_acc= 0.800\n",
      "epoch= 8   train_loss= 2.231   train_acc= 0.988   test_loss=2.715   test_acc= 0.800\n",
      "epoch= 9   train_loss= 2.203   train_acc= 0.988   test_loss=2.662   test_acc= 0.700\n",
      "epoch= 10   train_loss= 2.182   train_acc= 1.000   test_loss=2.702   test_acc= 0.800\n",
      "epoch= 11   train_loss= 2.172   train_acc= 1.000   test_loss=2.719   test_acc= 0.700\n",
      "epoch= 12   train_loss= 2.135   train_acc= 1.000   test_loss=2.739   test_acc= 0.700\n",
      "epoch= 13   train_loss= 2.116   train_acc= 1.000   test_loss=2.704   test_acc= 0.800\n",
      "epoch= 14   train_loss= 2.122   train_acc= 0.988   test_loss=2.622   test_acc= 0.800\n",
      "epoch= 15   train_loss= 2.088   train_acc= 1.000   test_loss=2.659   test_acc= 0.800\n",
      "epoch= 16   train_loss= 2.073   train_acc= 1.000   test_loss=2.665   test_acc= 0.700\n",
      "epoch= 17   train_loss= 2.063   train_acc= 1.000   test_loss=2.678   test_acc= 0.800\n",
      "epoch= 18   train_loss= 2.044   train_acc= 1.000   test_loss=2.658   test_acc= 0.800\n",
      "epoch= 19   train_loss= 2.040   train_acc= 1.000   test_loss=2.619   test_acc= 0.800\n",
      "epoch= 20   train_loss= 2.018   train_acc= 1.000   test_loss=2.621   test_acc= 0.800\n",
      "epoch= 21   train_loss= 2.000   train_acc= 1.000   test_loss=2.628   test_acc= 0.800\n",
      "epoch= 22   train_loss= 1.987   train_acc= 1.000   test_loss=2.643   test_acc= 0.800\n",
      "epoch= 23   train_loss= 1.981   train_acc= 1.000   test_loss=2.603   test_acc= 0.800\n",
      "epoch= 24   train_loss= 1.960   train_acc= 1.000   test_loss=2.591   test_acc= 0.800\n",
      "epoch= 25   train_loss= 1.949   train_acc= 1.000   test_loss=2.605   test_acc= 0.700\n",
      "epoch= 26   train_loss= 1.945   train_acc= 1.000   test_loss=2.528   test_acc= 0.700\n",
      "epoch= 27   train_loss= 1.924   train_acc= 1.000   test_loss=2.526   test_acc= 0.800\n",
      "epoch= 28   train_loss= 1.913   train_acc= 1.000   test_loss=2.553   test_acc= 0.700\n",
      "epoch= 29   train_loss= 1.895   train_acc= 1.000   test_loss=2.559   test_acc= 0.700\n",
      "epoch= 30   train_loss= 1.885   train_acc= 1.000   test_loss=2.555   test_acc= 0.800\n",
      "epoch= 31   train_loss= 1.874   train_acc= 1.000   test_loss=2.525   test_acc= 0.700\n",
      "epoch= 32   train_loss= 1.861   train_acc= 1.000   test_loss=2.504   test_acc= 0.700\n",
      "epoch= 33   train_loss= 1.850   train_acc= 1.000   test_loss=2.535   test_acc= 0.800\n",
      "epoch= 34   train_loss= 1.838   train_acc= 1.000   test_loss=2.527   test_acc= 0.800\n",
      "epoch= 35   train_loss= 1.827   train_acc= 1.000   test_loss=2.527   test_acc= 0.800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 36   train_loss= 1.814   train_acc= 1.000   test_loss=2.511   test_acc= 0.700\n",
      "epoch= 37   train_loss= 1.806   train_acc= 1.000   test_loss=2.444   test_acc= 0.700\n",
      "epoch= 38   train_loss= 1.794   train_acc= 1.000   test_loss=2.428   test_acc= 0.800\n",
      "epoch= 39   train_loss= 1.783   train_acc= 1.000   test_loss=2.445   test_acc= 0.800\n",
      "epoch= 40   train_loss= 1.771   train_acc= 1.000   test_loss=2.450   test_acc= 0.700\n",
      "epoch= 41   train_loss= 1.760   train_acc= 1.000   test_loss=2.440   test_acc= 0.800\n",
      "epoch= 42   train_loss= 1.752   train_acc= 1.000   test_loss=2.429   test_acc= 0.800\n",
      "epoch= 43   train_loss= 1.742   train_acc= 1.000   test_loss=2.437   test_acc= 0.800\n",
      "epoch= 44   train_loss= 1.729   train_acc= 1.000   test_loss=2.424   test_acc= 0.800\n",
      "epoch= 45   train_loss= 1.720   train_acc= 1.000   test_loss=2.402   test_acc= 0.800\n",
      "epoch= 46   train_loss= 1.711   train_acc= 1.000   test_loss=2.393   test_acc= 0.700\n",
      "epoch= 47   train_loss= 1.700   train_acc= 1.000   test_loss=2.394   test_acc= 0.800\n",
      "epoch= 48   train_loss= 1.691   train_acc= 1.000   test_loss=2.393   test_acc= 0.700\n",
      "epoch= 49   train_loss= 1.683   train_acc= 1.000   test_loss=2.387   test_acc= 0.700\n",
      "run time: 0.4502840518951416 min\n",
      "test_acc=0.700\n",
      "run= 3   fold= 1\n",
      "epoch= 0   train_loss= 2.958   train_acc= 0.610   test_loss=2.926   test_acc= 0.600\n",
      "epoch= 1   train_loss= 2.678   train_acc= 0.866   test_loss=2.723   test_acc= 0.800\n",
      "epoch= 2   train_loss= 2.486   train_acc= 0.951   test_loss=2.606   test_acc= 1.000\n",
      "epoch= 3   train_loss= 2.431   train_acc= 0.939   test_loss=2.589   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.392   train_acc= 0.951   test_loss=2.514   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.350   train_acc= 0.963   test_loss=2.479   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.290   train_acc= 0.988   test_loss=2.441   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.271   train_acc= 0.976   test_loss=2.395   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.253   train_acc= 0.976   test_loss=2.381   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.222   train_acc= 1.000   test_loss=2.366   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.189   train_acc= 1.000   test_loss=2.334   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.162   train_acc= 1.000   test_loss=2.301   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.151   train_acc= 1.000   test_loss=2.270   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.154   train_acc= 0.988   test_loss=2.243   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.117   train_acc= 1.000   test_loss=2.230   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.111   train_acc= 1.000   test_loss=2.204   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.085   train_acc= 1.000   test_loss=2.190   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.066   train_acc= 1.000   test_loss=2.179   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.059   train_acc= 1.000   test_loss=2.165   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.034   train_acc= 1.000   test_loss=2.140   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.019   train_acc= 1.000   test_loss=2.124   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.012   train_acc= 1.000   test_loss=2.099   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.992   train_acc= 1.000   test_loss=2.084   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.978   train_acc= 1.000   test_loss=2.065   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.963   train_acc= 1.000   test_loss=2.048   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.952   train_acc= 1.000   test_loss=2.036   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.935   train_acc= 1.000   test_loss=2.019   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.926   train_acc= 1.000   test_loss=2.007   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.913   train_acc= 1.000   test_loss=1.995   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.903   train_acc= 1.000   test_loss=1.979   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.897   train_acc= 1.000   test_loss=1.971   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.873   train_acc= 1.000   test_loss=1.955   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.867   train_acc= 1.000   test_loss=1.944   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.853   train_acc= 1.000   test_loss=1.931   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.842   train_acc= 1.000   test_loss=1.920   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.827   train_acc= 1.000   test_loss=1.906   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.819   train_acc= 1.000   test_loss=1.897   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.807   train_acc= 1.000   test_loss=1.884   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.797   train_acc= 1.000   test_loss=1.870   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.784   train_acc= 1.000   test_loss=1.858   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.772   train_acc= 1.000   test_loss=1.850   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.764   train_acc= 1.000   test_loss=1.837   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.752   train_acc= 1.000   test_loss=1.826   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.740   train_acc= 1.000   test_loss=1.817   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.732   train_acc= 1.000   test_loss=1.806   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.723   train_acc= 1.000   test_loss=1.797   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.712   train_acc= 1.000   test_loss=1.787   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.707   train_acc= 1.000   test_loss=1.778   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.693   train_acc= 1.000   test_loss=1.763   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.683   train_acc= 1.000   test_loss=1.755   test_acc= 1.000\n",
      "run time: 0.45635783672332764 min\n",
      "test_acc=1.000\n",
      "run= 3   fold= 2\n",
      "epoch= 0   train_loss= 2.977   train_acc= 0.639   test_loss=2.921   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.579   train_acc= 0.843   test_loss=2.776   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.487   train_acc= 0.916   test_loss=2.728   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.405   train_acc= 0.964   test_loss=2.697   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.327   train_acc= 0.988   test_loss=2.769   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.293   train_acc= 0.976   test_loss=2.654   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.272   train_acc= 1.000   test_loss=2.579   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.250   train_acc= 0.988   test_loss=2.662   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.201   train_acc= 0.988   test_loss=2.606   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.201   train_acc= 0.988   test_loss=2.639   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.173   train_acc= 0.988   test_loss=2.632   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.142   train_acc= 1.000   test_loss=2.563   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.125   train_acc= 1.000   test_loss=2.624   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.108   train_acc= 1.000   test_loss=2.552   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.089   train_acc= 1.000   test_loss=2.538   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.074   train_acc= 1.000   test_loss=2.534   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.061   train_acc= 1.000   test_loss=2.499   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.050   train_acc= 0.988   test_loss=2.504   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.034   train_acc= 1.000   test_loss=2.476   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.018   train_acc= 1.000   test_loss=2.452   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.003   train_acc= 1.000   test_loss=2.493   test_acc= 0.778\n",
      "epoch= 21   train_loss= 1.997   train_acc= 1.000   test_loss=2.398   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.971   train_acc= 1.000   test_loss=2.424   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.962   train_acc= 1.000   test_loss=2.412   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.955   train_acc= 1.000   test_loss=2.420   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.931   train_acc= 1.000   test_loss=2.349   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.919   train_acc= 1.000   test_loss=2.337   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.907   train_acc= 1.000   test_loss=2.326   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.893   train_acc= 1.000   test_loss=2.342   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.879   train_acc= 1.000   test_loss=2.324   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.867   train_acc= 1.000   test_loss=2.304   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.856   train_acc= 1.000   test_loss=2.309   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 32   train_loss= 1.842   train_acc= 1.000   test_loss=2.291   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.831   train_acc= 1.000   test_loss=2.259   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.830   train_acc= 1.000   test_loss=2.261   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.809   train_acc= 1.000   test_loss=2.261   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.799   train_acc= 1.000   test_loss=2.257   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.786   train_acc= 1.000   test_loss=2.230   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.777   train_acc= 1.000   test_loss=2.219   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.764   train_acc= 1.000   test_loss=2.226   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.756   train_acc= 1.000   test_loss=2.206   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.744   train_acc= 1.000   test_loss=2.198   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.735   train_acc= 1.000   test_loss=2.205   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.722   train_acc= 1.000   test_loss=2.206   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.713   train_acc= 1.000   test_loss=2.178   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.704   train_acc= 1.000   test_loss=2.178   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.693   train_acc= 1.000   test_loss=2.153   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.684   train_acc= 1.000   test_loss=2.146   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.679   train_acc= 1.000   test_loss=2.164   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.666   train_acc= 1.000   test_loss=2.149   test_acc= 0.778\n",
      "run time: 0.4616053541501363 min\n",
      "test_acc=0.778\n",
      "run= 3   fold= 3\n",
      "epoch= 0   train_loss= 2.969   train_acc= 0.639   test_loss=2.831   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.639   train_acc= 0.807   test_loss=2.740   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.480   train_acc= 0.928   test_loss=2.684   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.428   train_acc= 0.928   test_loss=2.618   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.365   train_acc= 0.952   test_loss=2.576   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.330   train_acc= 0.976   test_loss=2.569   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.268   train_acc= 0.988   test_loss=2.529   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.239   train_acc= 1.000   test_loss=2.496   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.200   train_acc= 1.000   test_loss=2.476   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.176   train_acc= 1.000   test_loss=2.462   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.173   train_acc= 0.988   test_loss=2.414   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.133   train_acc= 1.000   test_loss=2.405   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.119   train_acc= 1.000   test_loss=2.418   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.103   train_acc= 1.000   test_loss=2.394   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.081   train_acc= 1.000   test_loss=2.372   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.067   train_acc= 1.000   test_loss=2.350   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.052   train_acc= 1.000   test_loss=2.340   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.046   train_acc= 1.000   test_loss=2.321   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.020   train_acc= 1.000   test_loss=2.328   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.005   train_acc= 1.000   test_loss=2.307   test_acc= 0.889\n",
      "epoch= 20   train_loss= 1.996   train_acc= 1.000   test_loss=2.323   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.975   train_acc= 1.000   test_loss=2.300   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.960   train_acc= 1.000   test_loss=2.288   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.952   train_acc= 1.000   test_loss=2.272   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.938   train_acc= 1.000   test_loss=2.264   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.920   train_acc= 1.000   test_loss=2.249   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.912   train_acc= 1.000   test_loss=2.227   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.895   train_acc= 1.000   test_loss=2.222   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.882   train_acc= 1.000   test_loss=2.199   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.875   train_acc= 1.000   test_loss=2.198   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.858   train_acc= 1.000   test_loss=2.180   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.850   train_acc= 1.000   test_loss=2.157   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.835   train_acc= 1.000   test_loss=2.156   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.825   train_acc= 1.000   test_loss=2.153   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.825   train_acc= 0.988   test_loss=2.127   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.803   train_acc= 1.000   test_loss=2.119   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.788   train_acc= 1.000   test_loss=2.112   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.780   train_acc= 1.000   test_loss=2.107   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.769   train_acc= 1.000   test_loss=2.095   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.760   train_acc= 1.000   test_loss=2.088   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.748   train_acc= 1.000   test_loss=2.079   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.740   train_acc= 1.000   test_loss=2.074   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.729   train_acc= 1.000   test_loss=2.071   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.716   train_acc= 1.000   test_loss=2.057   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.708   train_acc= 1.000   test_loss=2.048   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.698   train_acc= 1.000   test_loss=2.033   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.685   train_acc= 1.000   test_loss=2.020   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.678   train_acc= 1.000   test_loss=2.005   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.670   train_acc= 1.000   test_loss=1.997   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.661   train_acc= 1.000   test_loss=1.984   test_acc= 0.889\n",
      "run time: 0.4399786631266276 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 4\n",
      "epoch= 0   train_loss= 3.015   train_acc= 0.590   test_loss=2.905   test_acc= 0.556\n",
      "epoch= 1   train_loss= 2.647   train_acc= 0.867   test_loss=2.851   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.507   train_acc= 0.940   test_loss=2.722   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.404   train_acc= 0.976   test_loss=2.663   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.418   train_acc= 0.928   test_loss=2.643   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.317   train_acc= 0.988   test_loss=2.506   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.301   train_acc= 0.976   test_loss=2.539   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.274   train_acc= 0.976   test_loss=2.438   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.232   train_acc= 0.988   test_loss=2.592   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.212   train_acc= 0.988   test_loss=2.391   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.184   train_acc= 1.000   test_loss=2.419   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.174   train_acc= 0.976   test_loss=2.398   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.142   train_acc= 1.000   test_loss=2.327   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.129   train_acc= 0.988   test_loss=2.327   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.103   train_acc= 1.000   test_loss=2.335   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.094   train_acc= 1.000   test_loss=2.361   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.074   train_acc= 1.000   test_loss=2.236   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.058   train_acc= 1.000   test_loss=2.293   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.047   train_acc= 1.000   test_loss=2.338   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.032   train_acc= 1.000   test_loss=2.241   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.008   train_acc= 1.000   test_loss=2.209   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.006   train_acc= 1.000   test_loss=2.197   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.984   train_acc= 1.000   test_loss=2.207   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.971   train_acc= 1.000   test_loss=2.165   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.960   train_acc= 1.000   test_loss=2.164   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.951   train_acc= 1.000   test_loss=2.131   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.930   train_acc= 1.000   test_loss=2.108   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.920   train_acc= 1.000   test_loss=2.098   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 28   train_loss= 1.904   train_acc= 1.000   test_loss=2.092   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.889   train_acc= 1.000   test_loss=2.087   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.882   train_acc= 1.000   test_loss=2.035   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.869   train_acc= 1.000   test_loss=2.044   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.857   train_acc= 1.000   test_loss=2.043   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.847   train_acc= 1.000   test_loss=2.014   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.832   train_acc= 1.000   test_loss=2.022   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.819   train_acc= 1.000   test_loss=1.976   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.810   train_acc= 1.000   test_loss=2.014   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.799   train_acc= 1.000   test_loss=1.965   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.789   train_acc= 1.000   test_loss=1.959   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.777   train_acc= 1.000   test_loss=1.965   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.765   train_acc= 1.000   test_loss=1.941   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.757   train_acc= 1.000   test_loss=1.939   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.743   train_acc= 1.000   test_loss=1.917   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.745   train_acc= 1.000   test_loss=1.948   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.730   train_acc= 1.000   test_loss=1.924   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.716   train_acc= 1.000   test_loss=1.896   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.705   train_acc= 1.000   test_loss=1.898   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.697   train_acc= 1.000   test_loss=1.887   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.687   train_acc= 1.000   test_loss=1.852   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.674   train_acc= 1.000   test_loss=1.853   test_acc= 1.000\n",
      "run time: 0.4495194435119629 min\n",
      "test_acc=1.000\n",
      "run= 3   fold= 5\n",
      "epoch= 0   train_loss= 2.891   train_acc= 0.675   test_loss=2.789   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.665   train_acc= 0.819   test_loss=2.655   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.568   train_acc= 0.867   test_loss=2.614   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.460   train_acc= 0.928   test_loss=2.638   test_acc= 0.667\n",
      "epoch= 4   train_loss= 2.394   train_acc= 0.964   test_loss=2.579   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.355   train_acc= 0.976   test_loss=2.492   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.319   train_acc= 0.976   test_loss=2.465   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.279   train_acc= 0.976   test_loss=2.442   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.249   train_acc= 0.988   test_loss=2.390   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.225   train_acc= 1.000   test_loss=2.366   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.200   train_acc= 0.988   test_loss=2.323   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.194   train_acc= 0.976   test_loss=2.294   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.156   train_acc= 1.000   test_loss=2.271   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.147   train_acc= 0.988   test_loss=2.240   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.112   train_acc= 1.000   test_loss=2.237   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.089   train_acc= 1.000   test_loss=2.213   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.087   train_acc= 0.988   test_loss=2.186   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.069   train_acc= 1.000   test_loss=2.193   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.045   train_acc= 1.000   test_loss=2.185   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.032   train_acc= 1.000   test_loss=2.160   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.023   train_acc= 1.000   test_loss=2.147   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.001   train_acc= 1.000   test_loss=2.131   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.993   train_acc= 1.000   test_loss=2.137   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.979   train_acc= 1.000   test_loss=2.104   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.967   train_acc= 1.000   test_loss=2.082   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.946   train_acc= 1.000   test_loss=2.069   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.935   train_acc= 1.000   test_loss=2.054   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.919   train_acc= 1.000   test_loss=2.045   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.913   train_acc= 1.000   test_loss=2.035   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.901   train_acc= 1.000   test_loss=2.019   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.886   train_acc= 1.000   test_loss=2.002   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.873   train_acc= 1.000   test_loss=1.989   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.859   train_acc= 1.000   test_loss=1.971   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.847   train_acc= 1.000   test_loss=1.963   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.834   train_acc= 1.000   test_loss=1.947   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.823   train_acc= 1.000   test_loss=1.951   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.813   train_acc= 1.000   test_loss=1.933   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.805   train_acc= 1.000   test_loss=1.930   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.793   train_acc= 1.000   test_loss=1.898   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.782   train_acc= 1.000   test_loss=1.891   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.771   train_acc= 1.000   test_loss=1.899   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.760   train_acc= 1.000   test_loss=1.878   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.749   train_acc= 1.000   test_loss=1.878   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.738   train_acc= 1.000   test_loss=1.860   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.727   train_acc= 1.000   test_loss=1.854   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.719   train_acc= 1.000   test_loss=1.845   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.706   train_acc= 1.000   test_loss=1.830   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.698   train_acc= 1.000   test_loss=1.843   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.689   train_acc= 1.000   test_loss=1.824   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.679   train_acc= 1.000   test_loss=1.812   test_acc= 0.889\n",
      "run time: 0.4801591475804647 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 6\n",
      "epoch= 0   train_loss= 2.925   train_acc= 0.663   test_loss=2.792   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.614   train_acc= 0.843   test_loss=2.698   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.517   train_acc= 0.916   test_loss=2.630   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.458   train_acc= 0.928   test_loss=2.604   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.386   train_acc= 0.964   test_loss=2.544   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.336   train_acc= 0.976   test_loss=2.495   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.291   train_acc= 1.000   test_loss=2.459   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.251   train_acc= 0.988   test_loss=2.455   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.251   train_acc= 0.988   test_loss=2.443   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.213   train_acc= 1.000   test_loss=2.416   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.183   train_acc= 1.000   test_loss=2.379   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.172   train_acc= 1.000   test_loss=2.399   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.146   train_acc= 1.000   test_loss=2.372   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.127   train_acc= 1.000   test_loss=2.352   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.115   train_acc= 1.000   test_loss=2.339   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.097   train_acc= 1.000   test_loss=2.321   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.077   train_acc= 1.000   test_loss=2.311   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.062   train_acc= 1.000   test_loss=2.284   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.059   train_acc= 1.000   test_loss=2.271   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.038   train_acc= 1.000   test_loss=2.253   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.020   train_acc= 1.000   test_loss=2.241   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.008   train_acc= 1.000   test_loss=2.228   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.988   train_acc= 1.000   test_loss=2.227   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.980   train_acc= 1.000   test_loss=2.195   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 24   train_loss= 1.960   train_acc= 1.000   test_loss=2.179   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.956   train_acc= 1.000   test_loss=2.146   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.939   train_acc= 1.000   test_loss=2.149   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.924   train_acc= 1.000   test_loss=2.127   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.914   train_acc= 1.000   test_loss=2.119   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.899   train_acc= 1.000   test_loss=2.113   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.907   train_acc= 0.988   test_loss=2.088   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.876   train_acc= 1.000   test_loss=2.075   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.863   train_acc= 1.000   test_loss=2.070   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.851   train_acc= 1.000   test_loss=2.060   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.841   train_acc= 1.000   test_loss=2.060   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.830   train_acc= 1.000   test_loss=2.025   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.818   train_acc= 1.000   test_loss=2.015   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.809   train_acc= 1.000   test_loss=1.999   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.796   train_acc= 1.000   test_loss=1.994   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.785   train_acc= 1.000   test_loss=2.006   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.777   train_acc= 1.000   test_loss=1.991   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.765   train_acc= 1.000   test_loss=1.968   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.754   train_acc= 1.000   test_loss=1.958   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.741   train_acc= 1.000   test_loss=1.949   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.735   train_acc= 1.000   test_loss=1.945   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.726   train_acc= 1.000   test_loss=1.934   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.712   train_acc= 1.000   test_loss=1.933   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.703   train_acc= 1.000   test_loss=1.919   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.693   train_acc= 1.000   test_loss=1.909   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.681   train_acc= 1.000   test_loss=1.905   test_acc= 0.889\n",
      "run time: 0.46004215876261395 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 7\n",
      "epoch= 0   train_loss= 2.951   train_acc= 0.663   test_loss=2.629   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.653   train_acc= 0.831   test_loss=2.518   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.514   train_acc= 0.940   test_loss=2.472   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.409   train_acc= 0.952   test_loss=2.433   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.352   train_acc= 0.964   test_loss=2.437   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.311   train_acc= 0.988   test_loss=2.413   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.272   train_acc= 0.988   test_loss=2.377   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.266   train_acc= 0.988   test_loss=2.341   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.224   train_acc= 1.000   test_loss=2.311   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.200   train_acc= 0.988   test_loss=2.279   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.173   train_acc= 1.000   test_loss=2.270   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.146   train_acc= 1.000   test_loss=2.279   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.138   train_acc= 0.988   test_loss=2.237   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.115   train_acc= 1.000   test_loss=2.257   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.094   train_acc= 1.000   test_loss=2.239   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.090   train_acc= 1.000   test_loss=2.219   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.060   train_acc= 1.000   test_loss=2.207   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.052   train_acc= 1.000   test_loss=2.174   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.037   train_acc= 1.000   test_loss=2.166   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.029   train_acc= 1.000   test_loss=2.120   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.001   train_acc= 1.000   test_loss=2.133   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.990   train_acc= 1.000   test_loss=2.112   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.975   train_acc= 1.000   test_loss=2.123   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.960   train_acc= 1.000   test_loss=2.108   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.947   train_acc= 1.000   test_loss=2.089   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.933   train_acc= 1.000   test_loss=2.065   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.920   train_acc= 1.000   test_loss=2.067   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.909   train_acc= 1.000   test_loss=2.047   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.894   train_acc= 1.000   test_loss=2.033   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.888   train_acc= 1.000   test_loss=2.017   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.878   train_acc= 1.000   test_loss=2.000   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.860   train_acc= 1.000   test_loss=2.005   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.849   train_acc= 1.000   test_loss=1.984   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.837   train_acc= 1.000   test_loss=1.969   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.825   train_acc= 1.000   test_loss=1.958   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.817   train_acc= 1.000   test_loss=1.977   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.805   train_acc= 1.000   test_loss=1.923   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.795   train_acc= 1.000   test_loss=1.935   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.785   train_acc= 1.000   test_loss=1.947   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.770   train_acc= 1.000   test_loss=1.929   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.756   train_acc= 1.000   test_loss=1.916   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.753   train_acc= 1.000   test_loss=1.886   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.740   train_acc= 1.000   test_loss=1.888   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.735   train_acc= 1.000   test_loss=1.875   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.718   train_acc= 1.000   test_loss=1.860   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.709   train_acc= 1.000   test_loss=1.848   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.698   train_acc= 1.000   test_loss=1.850   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.689   train_acc= 1.000   test_loss=1.840   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.687   train_acc= 1.000   test_loss=1.832   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.674   train_acc= 1.000   test_loss=1.819   test_acc= 0.889\n",
      "run time: 0.4657753547032674 min\n",
      "test_acc=0.889\n",
      "run= 3   fold= 8\n",
      "epoch= 0   train_loss= 2.881   train_acc= 0.699   test_loss=2.797   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.616   train_acc= 0.904   test_loss=2.692   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.523   train_acc= 0.904   test_loss=2.651   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.444   train_acc= 0.940   test_loss=2.640   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.371   train_acc= 0.976   test_loss=2.584   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.320   train_acc= 0.988   test_loss=2.555   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.314   train_acc= 0.964   test_loss=2.554   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.254   train_acc= 0.988   test_loss=2.587   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.233   train_acc= 0.988   test_loss=2.566   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.212   train_acc= 1.000   test_loss=2.500   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.181   train_acc= 1.000   test_loss=2.490   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.156   train_acc= 1.000   test_loss=2.465   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.134   train_acc= 1.000   test_loss=2.455   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.128   train_acc= 1.000   test_loss=2.407   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.099   train_acc= 1.000   test_loss=2.402   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.090   train_acc= 1.000   test_loss=2.403   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.071   train_acc= 1.000   test_loss=2.416   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.057   train_acc= 1.000   test_loss=2.368   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.036   train_acc= 1.000   test_loss=2.356   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.038   train_acc= 0.988   test_loss=2.351   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 20   train_loss= 2.021   train_acc= 1.000   test_loss=2.386   test_acc= 0.778\n",
      "epoch= 21   train_loss= 1.996   train_acc= 1.000   test_loss=2.355   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.986   train_acc= 1.000   test_loss=2.320   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.978   train_acc= 1.000   test_loss=2.293   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.956   train_acc= 1.000   test_loss=2.291   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.942   train_acc= 1.000   test_loss=2.252   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.928   train_acc= 1.000   test_loss=2.244   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.923   train_acc= 1.000   test_loss=2.206   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.906   train_acc= 1.000   test_loss=2.225   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.894   train_acc= 1.000   test_loss=2.212   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.877   train_acc= 1.000   test_loss=2.194   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.873   train_acc= 1.000   test_loss=2.179   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.857   train_acc= 1.000   test_loss=2.165   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.846   train_acc= 1.000   test_loss=2.159   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.832   train_acc= 1.000   test_loss=2.153   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.822   train_acc= 1.000   test_loss=2.136   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.810   train_acc= 1.000   test_loss=2.126   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.802   train_acc= 1.000   test_loss=2.112   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.789   train_acc= 1.000   test_loss=2.124   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.780   train_acc= 1.000   test_loss=2.108   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.766   train_acc= 1.000   test_loss=2.099   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.759   train_acc= 1.000   test_loss=2.084   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.746   train_acc= 1.000   test_loss=2.065   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.737   train_acc= 1.000   test_loss=2.066   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.726   train_acc= 1.000   test_loss=2.053   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.714   train_acc= 1.000   test_loss=2.038   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.704   train_acc= 1.000   test_loss=2.041   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.698   train_acc= 1.000   test_loss=2.030   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.687   train_acc= 1.000   test_loss=2.021   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.677   train_acc= 1.000   test_loss=2.014   test_acc= 0.778\n",
      "run time: 0.47519054015477497 min\n",
      "test_acc=0.778\n",
      "run= 3   fold= 9\n",
      "epoch= 0   train_loss= 2.891   train_acc= 0.663   test_loss=2.735   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.639   train_acc= 0.843   test_loss=2.633   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.501   train_acc= 0.928   test_loss=2.541   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.405   train_acc= 0.940   test_loss=2.493   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.355   train_acc= 0.976   test_loss=2.456   test_acc= 1.000\n",
      "epoch= 5   train_loss= 2.307   train_acc= 1.000   test_loss=2.374   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.269   train_acc= 1.000   test_loss=2.353   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.263   train_acc= 0.988   test_loss=2.334   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.216   train_acc= 1.000   test_loss=2.291   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.208   train_acc= 0.988   test_loss=2.285   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.189   train_acc= 1.000   test_loss=2.245   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.175   train_acc= 1.000   test_loss=2.215   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.141   train_acc= 1.000   test_loss=2.205   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.126   train_acc= 1.000   test_loss=2.180   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.108   train_acc= 1.000   test_loss=2.158   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.088   train_acc= 1.000   test_loss=2.141   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.072   train_acc= 1.000   test_loss=2.121   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.062   train_acc= 1.000   test_loss=2.103   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.046   train_acc= 1.000   test_loss=2.095   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.036   train_acc= 1.000   test_loss=2.069   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.017   train_acc= 1.000   test_loss=2.056   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.001   train_acc= 1.000   test_loss=2.040   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.981   train_acc= 1.000   test_loss=2.026   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.979   train_acc= 1.000   test_loss=2.010   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.959   train_acc= 1.000   test_loss=1.999   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.944   train_acc= 1.000   test_loss=1.984   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.933   train_acc= 1.000   test_loss=1.972   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.924   train_acc= 1.000   test_loss=1.954   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.913   train_acc= 1.000   test_loss=1.945   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.890   train_acc= 1.000   test_loss=1.934   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.883   train_acc= 1.000   test_loss=1.920   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.868   train_acc= 1.000   test_loss=1.907   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.861   train_acc= 1.000   test_loss=1.897   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.853   train_acc= 1.000   test_loss=1.881   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.835   train_acc= 1.000   test_loss=1.870   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.825   train_acc= 1.000   test_loss=1.855   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.813   train_acc= 1.000   test_loss=1.846   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.803   train_acc= 1.000   test_loss=1.834   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.790   train_acc= 1.000   test_loss=1.825   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.780   train_acc= 1.000   test_loss=1.815   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.768   train_acc= 1.000   test_loss=1.805   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.761   train_acc= 1.000   test_loss=1.791   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.752   train_acc= 1.000   test_loss=1.780   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.739   train_acc= 1.000   test_loss=1.770   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.726   train_acc= 1.000   test_loss=1.759   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.717   train_acc= 1.000   test_loss=1.751   test_acc= 1.000\n",
      "epoch= 46   train_loss= 1.709   train_acc= 1.000   test_loss=1.740   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.700   train_acc= 1.000   test_loss=1.733   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.690   train_acc= 1.000   test_loss=1.722   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.681   train_acc= 1.000   test_loss=1.713   test_acc= 1.000\n",
      "run time: 0.4679766615231832 min\n",
      "test_acc=1.000\n",
      "run= 4   fold= 0\n",
      "epoch= 0   train_loss= 2.843   train_acc= 0.732   test_loss=2.687   test_acc= 0.900\n",
      "epoch= 1   train_loss= 2.646   train_acc= 0.817   test_loss=2.606   test_acc= 0.900\n",
      "epoch= 2   train_loss= 2.451   train_acc= 0.951   test_loss=2.555   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.414   train_acc= 0.939   test_loss=2.521   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.349   train_acc= 0.976   test_loss=2.542   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.319   train_acc= 0.988   test_loss=2.443   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.280   train_acc= 1.000   test_loss=2.442   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.293   train_acc= 0.976   test_loss=2.395   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.226   train_acc= 0.988   test_loss=2.380   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.215   train_acc= 1.000   test_loss=2.403   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.201   train_acc= 1.000   test_loss=2.327   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.159   train_acc= 1.000   test_loss=2.318   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.145   train_acc= 1.000   test_loss=2.293   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.133   train_acc= 1.000   test_loss=2.309   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.119   train_acc= 1.000   test_loss=2.283   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.102   train_acc= 1.000   test_loss=2.281   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 16   train_loss= 2.090   train_acc= 1.000   test_loss=2.269   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.078   train_acc= 0.988   test_loss=2.231   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.046   train_acc= 1.000   test_loss=2.226   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.042   train_acc= 1.000   test_loss=2.219   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.018   train_acc= 1.000   test_loss=2.207   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.005   train_acc= 1.000   test_loss=2.177   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.988   train_acc= 1.000   test_loss=2.159   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.982   train_acc= 1.000   test_loss=2.179   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.969   train_acc= 1.000   test_loss=2.145   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.958   train_acc= 1.000   test_loss=2.134   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.945   train_acc= 1.000   test_loss=2.121   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.927   train_acc= 1.000   test_loss=2.109   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.920   train_acc= 1.000   test_loss=2.079   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.902   train_acc= 1.000   test_loss=2.091   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.889   train_acc= 1.000   test_loss=2.067   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.877   train_acc= 1.000   test_loss=2.047   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.863   train_acc= 1.000   test_loss=2.036   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.853   train_acc= 1.000   test_loss=2.021   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.844   train_acc= 1.000   test_loss=2.023   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.834   train_acc= 1.000   test_loss=2.008   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.817   train_acc= 1.000   test_loss=2.007   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.808   train_acc= 1.000   test_loss=1.989   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.798   train_acc= 1.000   test_loss=1.970   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.789   train_acc= 1.000   test_loss=1.965   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.775   train_acc= 1.000   test_loss=1.970   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.764   train_acc= 1.000   test_loss=1.947   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.761   train_acc= 1.000   test_loss=1.930   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.748   train_acc= 1.000   test_loss=1.932   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.738   train_acc= 1.000   test_loss=1.921   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.729   train_acc= 1.000   test_loss=1.913   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.718   train_acc= 1.000   test_loss=1.879   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.704   train_acc= 1.000   test_loss=1.875   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.695   train_acc= 1.000   test_loss=1.874   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.687   train_acc= 1.000   test_loss=1.864   test_acc= 1.000\n",
      "run time: 0.44533613125483196 min\n",
      "test_acc=1.000\n",
      "run= 4   fold= 1\n",
      "epoch= 0   train_loss= 3.016   train_acc= 0.573   test_loss=2.740   test_acc= 0.800\n",
      "epoch= 1   train_loss= 2.583   train_acc= 0.915   test_loss=2.664   test_acc= 0.800\n",
      "epoch= 2   train_loss= 2.509   train_acc= 0.902   test_loss=2.585   test_acc= 0.900\n",
      "epoch= 3   train_loss= 2.471   train_acc= 0.915   test_loss=2.566   test_acc= 0.900\n",
      "epoch= 4   train_loss= 2.400   train_acc= 0.951   test_loss=2.535   test_acc= 0.900\n",
      "epoch= 5   train_loss= 2.347   train_acc= 0.963   test_loss=2.471   test_acc= 0.900\n",
      "epoch= 6   train_loss= 2.309   train_acc= 0.963   test_loss=2.458   test_acc= 0.900\n",
      "epoch= 7   train_loss= 2.267   train_acc= 0.976   test_loss=2.435   test_acc= 0.900\n",
      "epoch= 8   train_loss= 2.225   train_acc= 0.988   test_loss=2.421   test_acc= 0.900\n",
      "epoch= 9   train_loss= 2.226   train_acc= 0.976   test_loss=2.409   test_acc= 0.900\n",
      "epoch= 10   train_loss= 2.208   train_acc= 0.988   test_loss=2.388   test_acc= 0.900\n",
      "epoch= 11   train_loss= 2.171   train_acc= 0.988   test_loss=2.355   test_acc= 0.900\n",
      "epoch= 12   train_loss= 2.140   train_acc= 1.000   test_loss=2.356   test_acc= 0.900\n",
      "epoch= 13   train_loss= 2.120   train_acc= 1.000   test_loss=2.347   test_acc= 0.900\n",
      "epoch= 14   train_loss= 2.114   train_acc= 1.000   test_loss=2.352   test_acc= 0.900\n",
      "epoch= 15   train_loss= 2.082   train_acc= 1.000   test_loss=2.297   test_acc= 0.900\n",
      "epoch= 16   train_loss= 2.078   train_acc= 1.000   test_loss=2.274   test_acc= 0.900\n",
      "epoch= 17   train_loss= 2.056   train_acc= 1.000   test_loss=2.272   test_acc= 0.900\n",
      "epoch= 18   train_loss= 2.044   train_acc= 1.000   test_loss=2.289   test_acc= 0.900\n",
      "epoch= 19   train_loss= 2.026   train_acc= 1.000   test_loss=2.259   test_acc= 0.900\n",
      "epoch= 20   train_loss= 2.025   train_acc= 0.988   test_loss=2.258   test_acc= 0.900\n",
      "epoch= 21   train_loss= 2.004   train_acc= 1.000   test_loss=2.198   test_acc= 0.900\n",
      "epoch= 22   train_loss= 1.989   train_acc= 1.000   test_loss=2.198   test_acc= 0.900\n",
      "epoch= 23   train_loss= 1.972   train_acc= 1.000   test_loss=2.207   test_acc= 0.900\n",
      "epoch= 24   train_loss= 1.968   train_acc= 1.000   test_loss=2.194   test_acc= 0.900\n",
      "epoch= 25   train_loss= 1.944   train_acc= 1.000   test_loss=2.155   test_acc= 0.900\n",
      "epoch= 26   train_loss= 1.934   train_acc= 1.000   test_loss=2.130   test_acc= 0.900\n",
      "epoch= 27   train_loss= 1.926   train_acc= 1.000   test_loss=2.152   test_acc= 0.900\n",
      "epoch= 28   train_loss= 1.908   train_acc= 1.000   test_loss=2.130   test_acc= 0.900\n",
      "epoch= 29   train_loss= 1.905   train_acc= 0.988   test_loss=2.085   test_acc= 0.900\n",
      "epoch= 30   train_loss= 1.884   train_acc= 1.000   test_loss=2.092   test_acc= 0.900\n",
      "epoch= 31   train_loss= 1.877   train_acc= 1.000   test_loss=2.103   test_acc= 0.900\n",
      "epoch= 32   train_loss= 1.859   train_acc= 1.000   test_loss=2.076   test_acc= 0.900\n",
      "epoch= 33   train_loss= 1.846   train_acc= 1.000   test_loss=2.055   test_acc= 0.900\n",
      "epoch= 34   train_loss= 1.835   train_acc= 1.000   test_loss=2.044   test_acc= 0.900\n",
      "epoch= 35   train_loss= 1.820   train_acc= 1.000   test_loss=2.039   test_acc= 0.900\n",
      "epoch= 36   train_loss= 1.820   train_acc= 1.000   test_loss=2.032   test_acc= 0.900\n",
      "epoch= 37   train_loss= 1.806   train_acc= 1.000   test_loss=2.055   test_acc= 0.900\n",
      "epoch= 38   train_loss= 1.790   train_acc= 1.000   test_loss=2.044   test_acc= 0.900\n",
      "epoch= 39   train_loss= 1.783   train_acc= 1.000   test_loss=2.001   test_acc= 0.900\n",
      "epoch= 40   train_loss= 1.772   train_acc= 1.000   test_loss=1.972   test_acc= 0.900\n",
      "epoch= 41   train_loss= 1.757   train_acc= 1.000   test_loss=1.977   test_acc= 0.900\n",
      "epoch= 42   train_loss= 1.751   train_acc= 1.000   test_loss=1.958   test_acc= 0.900\n",
      "epoch= 43   train_loss= 1.735   train_acc= 1.000   test_loss=1.955   test_acc= 0.900\n",
      "epoch= 44   train_loss= 1.727   train_acc= 1.000   test_loss=1.936   test_acc= 0.900\n",
      "epoch= 45   train_loss= 1.718   train_acc= 1.000   test_loss=1.934   test_acc= 0.900\n",
      "epoch= 46   train_loss= 1.709   train_acc= 1.000   test_loss=1.913   test_acc= 0.900\n",
      "epoch= 47   train_loss= 1.700   train_acc= 1.000   test_loss=1.926   test_acc= 0.900\n",
      "epoch= 48   train_loss= 1.691   train_acc= 1.000   test_loss=1.899   test_acc= 0.900\n",
      "epoch= 49   train_loss= 1.680   train_acc= 1.000   test_loss=1.912   test_acc= 0.900\n",
      "run time: 0.44785167773564655 min\n",
      "test_acc=0.900\n",
      "run= 4   fold= 2\n",
      "epoch= 0   train_loss= 2.950   train_acc= 0.675   test_loss=2.798   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.671   train_acc= 0.843   test_loss=2.723   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.523   train_acc= 0.904   test_loss=2.690   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.439   train_acc= 0.964   test_loss=2.610   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.413   train_acc= 0.952   test_loss=2.562   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.329   train_acc= 0.988   test_loss=2.517   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.289   train_acc= 1.000   test_loss=2.490   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.265   train_acc= 0.976   test_loss=2.477   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.231   train_acc= 1.000   test_loss=2.540   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.204   train_acc= 1.000   test_loss=2.484   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.195   train_acc= 1.000   test_loss=2.454   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.167   train_acc= 1.000   test_loss=2.420   test_acc= 0.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 12   train_loss= 2.146   train_acc= 1.000   test_loss=2.407   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.126   train_acc= 1.000   test_loss=2.455   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.113   train_acc= 1.000   test_loss=2.389   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.088   train_acc= 1.000   test_loss=2.379   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.075   train_acc= 1.000   test_loss=2.388   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.057   train_acc= 1.000   test_loss=2.330   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.042   train_acc= 1.000   test_loss=2.347   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.030   train_acc= 1.000   test_loss=2.362   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.016   train_acc= 1.000   test_loss=2.298   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.002   train_acc= 1.000   test_loss=2.329   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.988   train_acc= 1.000   test_loss=2.263   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.988   train_acc= 1.000   test_loss=2.318   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.960   train_acc= 1.000   test_loss=2.279   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.954   train_acc= 0.988   test_loss=2.202   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.938   train_acc= 1.000   test_loss=2.273   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.921   train_acc= 1.000   test_loss=2.227   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.915   train_acc= 1.000   test_loss=2.239   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.901   train_acc= 1.000   test_loss=2.193   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.895   train_acc= 1.000   test_loss=2.183   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.873   train_acc= 1.000   test_loss=2.168   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.858   train_acc= 1.000   test_loss=2.178   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.847   train_acc= 1.000   test_loss=2.148   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.835   train_acc= 1.000   test_loss=2.135   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.823   train_acc= 1.000   test_loss=2.117   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.814   train_acc= 1.000   test_loss=2.091   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.802   train_acc= 1.000   test_loss=2.094   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.791   train_acc= 1.000   test_loss=2.097   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.779   train_acc= 1.000   test_loss=2.087   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.771   train_acc= 1.000   test_loss=2.061   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.759   train_acc= 1.000   test_loss=2.063   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.746   train_acc= 1.000   test_loss=2.061   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.738   train_acc= 1.000   test_loss=2.038   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.725   train_acc= 1.000   test_loss=2.034   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.717   train_acc= 1.000   test_loss=2.034   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.706   train_acc= 1.000   test_loss=2.018   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.698   train_acc= 1.000   test_loss=2.005   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.687   train_acc= 1.000   test_loss=1.993   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.679   train_acc= 1.000   test_loss=1.961   test_acc= 0.778\n",
      "run time: 0.48762920300165813 min\n",
      "test_acc=0.778\n",
      "run= 4   fold= 3\n",
      "epoch= 0   train_loss= 2.951   train_acc= 0.627   test_loss=2.690   test_acc= 1.000\n",
      "epoch= 1   train_loss= 2.672   train_acc= 0.855   test_loss=2.596   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.555   train_acc= 0.855   test_loss=2.596   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.458   train_acc= 0.940   test_loss=2.584   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.379   train_acc= 0.952   test_loss=2.504   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.335   train_acc= 0.976   test_loss=2.407   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.322   train_acc= 0.976   test_loss=2.505   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.269   train_acc= 0.976   test_loss=2.397   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.234   train_acc= 1.000   test_loss=2.369   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.210   train_acc= 0.988   test_loss=2.315   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.186   train_acc= 0.988   test_loss=2.301   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.174   train_acc= 0.988   test_loss=2.360   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.131   train_acc= 1.000   test_loss=2.334   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.124   train_acc= 0.988   test_loss=2.301   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.100   train_acc= 1.000   test_loss=2.272   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.084   train_acc= 1.000   test_loss=2.241   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.072   train_acc= 1.000   test_loss=2.247   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.064   train_acc= 0.988   test_loss=2.209   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.049   train_acc= 1.000   test_loss=2.232   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.022   train_acc= 1.000   test_loss=2.206   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.012   train_acc= 1.000   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.995   train_acc= 1.000   test_loss=2.193   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.984   train_acc= 1.000   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.961   train_acc= 1.000   test_loss=2.159   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.955   train_acc= 1.000   test_loss=2.147   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.940   train_acc= 1.000   test_loss=2.146   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.928   train_acc= 1.000   test_loss=2.113   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.918   train_acc= 1.000   test_loss=2.091   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.900   train_acc= 1.000   test_loss=2.097   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.888   train_acc= 1.000   test_loss=2.094   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.871   train_acc= 1.000   test_loss=2.089   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.866   train_acc= 1.000   test_loss=2.082   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.852   train_acc= 1.000   test_loss=2.044   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.839   train_acc= 1.000   test_loss=2.053   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.834   train_acc= 1.000   test_loss=2.016   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.813   train_acc= 1.000   test_loss=2.015   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.804   train_acc= 1.000   test_loss=2.014   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.796   train_acc= 1.000   test_loss=1.985   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.784   train_acc= 1.000   test_loss=2.021   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.772   train_acc= 1.000   test_loss=1.983   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.763   train_acc= 1.000   test_loss=1.966   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.752   train_acc= 1.000   test_loss=1.951   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.746   train_acc= 1.000   test_loss=1.940   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.731   train_acc= 1.000   test_loss=1.938   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.719   train_acc= 1.000   test_loss=1.928   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.711   train_acc= 1.000   test_loss=1.920   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.700   train_acc= 1.000   test_loss=1.912   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.695   train_acc= 1.000   test_loss=1.890   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.679   train_acc= 1.000   test_loss=1.887   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.671   train_acc= 1.000   test_loss=1.890   test_acc= 0.889\n",
      "run time: 0.4795552452405294 min\n",
      "test_acc=0.889\n",
      "run= 4   fold= 4\n",
      "epoch= 0   train_loss= 3.010   train_acc= 0.675   test_loss=3.049   test_acc= 0.444\n",
      "epoch= 1   train_loss= 2.601   train_acc= 0.855   test_loss=2.860   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.487   train_acc= 0.892   test_loss=2.841   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.390   train_acc= 0.976   test_loss=2.843   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.337   train_acc= 0.976   test_loss=2.811   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.292   train_acc= 0.988   test_loss=2.757   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.268   train_acc= 0.976   test_loss=2.743   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.245   train_acc= 0.988   test_loss=2.752   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 8   train_loss= 2.214   train_acc= 0.988   test_loss=2.684   test_acc= 0.889\n",
      "epoch= 9   train_loss= 2.174   train_acc= 1.000   test_loss=2.674   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.167   train_acc= 1.000   test_loss=2.681   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.140   train_acc= 1.000   test_loss=2.693   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.132   train_acc= 1.000   test_loss=2.646   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.107   train_acc= 1.000   test_loss=2.666   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.092   train_acc= 1.000   test_loss=2.632   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.075   train_acc= 1.000   test_loss=2.638   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.058   train_acc= 1.000   test_loss=2.617   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.043   train_acc= 1.000   test_loss=2.627   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.035   train_acc= 1.000   test_loss=2.586   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.015   train_acc= 1.000   test_loss=2.559   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.001   train_acc= 1.000   test_loss=2.542   test_acc= 0.889\n",
      "epoch= 21   train_loss= 1.988   train_acc= 1.000   test_loss=2.541   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.973   train_acc= 1.000   test_loss=2.532   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.962   train_acc= 1.000   test_loss=2.511   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.947   train_acc= 1.000   test_loss=2.499   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.929   train_acc= 1.000   test_loss=2.507   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.919   train_acc= 1.000   test_loss=2.483   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.905   train_acc= 1.000   test_loss=2.486   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.894   train_acc= 1.000   test_loss=2.488   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.882   train_acc= 1.000   test_loss=2.487   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.867   train_acc= 1.000   test_loss=2.464   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.864   train_acc= 1.000   test_loss=2.433   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.847   train_acc= 1.000   test_loss=2.421   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.836   train_acc= 1.000   test_loss=2.431   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.822   train_acc= 1.000   test_loss=2.394   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.812   train_acc= 1.000   test_loss=2.389   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.799   train_acc= 1.000   test_loss=2.407   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.790   train_acc= 1.000   test_loss=2.391   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.783   train_acc= 1.000   test_loss=2.344   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.770   train_acc= 1.000   test_loss=2.332   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.757   train_acc= 1.000   test_loss=2.347   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.748   train_acc= 1.000   test_loss=2.339   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.736   train_acc= 1.000   test_loss=2.313   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.724   train_acc= 1.000   test_loss=2.298   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.715   train_acc= 1.000   test_loss=2.315   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.706   train_acc= 1.000   test_loss=2.291   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.694   train_acc= 1.000   test_loss=2.290   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.684   train_acc= 1.000   test_loss=2.287   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.676   train_acc= 1.000   test_loss=2.283   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.668   train_acc= 1.000   test_loss=2.273   test_acc= 0.889\n",
      "run time: 0.4856964826583862 min\n",
      "test_acc=0.889\n",
      "run= 4   fold= 5\n",
      "epoch= 0   train_loss= 2.926   train_acc= 0.639   test_loss=2.883   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.619   train_acc= 0.855   test_loss=2.738   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.510   train_acc= 0.916   test_loss=2.686   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.433   train_acc= 0.928   test_loss=2.584   test_acc= 0.889\n",
      "epoch= 4   train_loss= 2.367   train_acc= 0.976   test_loss=2.588   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.325   train_acc= 0.964   test_loss=2.521   test_acc= 0.889\n",
      "epoch= 6   train_loss= 2.285   train_acc= 0.988   test_loss=2.564   test_acc= 0.889\n",
      "epoch= 7   train_loss= 2.265   train_acc= 0.988   test_loss=2.535   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.228   train_acc= 1.000   test_loss=2.524   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.207   train_acc= 1.000   test_loss=2.574   test_acc= 0.889\n",
      "epoch= 10   train_loss= 2.184   train_acc= 1.000   test_loss=2.515   test_acc= 0.889\n",
      "epoch= 11   train_loss= 2.169   train_acc= 1.000   test_loss=2.429   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.154   train_acc= 1.000   test_loss=2.460   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.124   train_acc= 1.000   test_loss=2.489   test_acc= 0.889\n",
      "epoch= 14   train_loss= 2.107   train_acc= 1.000   test_loss=2.429   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.091   train_acc= 1.000   test_loss=2.422   test_acc= 0.889\n",
      "epoch= 16   train_loss= 2.079   train_acc= 1.000   test_loss=2.447   test_acc= 0.889\n",
      "epoch= 17   train_loss= 2.064   train_acc= 1.000   test_loss=2.387   test_acc= 0.889\n",
      "epoch= 18   train_loss= 2.046   train_acc= 1.000   test_loss=2.400   test_acc= 0.889\n",
      "epoch= 19   train_loss= 2.037   train_acc= 1.000   test_loss=2.394   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.018   train_acc= 1.000   test_loss=2.377   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.011   train_acc= 1.000   test_loss=2.319   test_acc= 0.889\n",
      "epoch= 22   train_loss= 1.992   train_acc= 1.000   test_loss=2.301   test_acc= 0.889\n",
      "epoch= 23   train_loss= 1.981   train_acc= 1.000   test_loss=2.344   test_acc= 0.889\n",
      "epoch= 24   train_loss= 1.962   train_acc= 1.000   test_loss=2.298   test_acc= 0.889\n",
      "epoch= 25   train_loss= 1.951   train_acc= 1.000   test_loss=2.276   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.934   train_acc= 1.000   test_loss=2.286   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.919   train_acc= 1.000   test_loss=2.278   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.911   train_acc= 1.000   test_loss=2.286   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.894   train_acc= 1.000   test_loss=2.282   test_acc= 0.889\n",
      "epoch= 30   train_loss= 1.887   train_acc= 1.000   test_loss=2.286   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.870   train_acc= 1.000   test_loss=2.252   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.860   train_acc= 1.000   test_loss=2.279   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.849   train_acc= 1.000   test_loss=2.228   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.838   train_acc= 1.000   test_loss=2.206   test_acc= 0.889\n",
      "epoch= 35   train_loss= 1.825   train_acc= 1.000   test_loss=2.182   test_acc= 0.889\n",
      "epoch= 36   train_loss= 1.812   train_acc= 1.000   test_loss=2.193   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.809   train_acc= 1.000   test_loss=2.212   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.798   train_acc= 1.000   test_loss=2.183   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.793   train_acc= 1.000   test_loss=2.225   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.769   train_acc= 1.000   test_loss=2.151   test_acc= 0.889\n",
      "epoch= 41   train_loss= 1.763   train_acc= 1.000   test_loss=2.093   test_acc= 0.889\n",
      "epoch= 42   train_loss= 1.752   train_acc= 1.000   test_loss=2.099   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.739   train_acc= 1.000   test_loss=2.100   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.738   train_acc= 1.000   test_loss=2.163   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.720   train_acc= 1.000   test_loss=2.104   test_acc= 0.889\n",
      "epoch= 46   train_loss= 1.711   train_acc= 1.000   test_loss=2.054   test_acc= 0.889\n",
      "epoch= 47   train_loss= 1.699   train_acc= 1.000   test_loss=2.050   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.691   train_acc= 1.000   test_loss=2.046   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.683   train_acc= 1.000   test_loss=2.042   test_acc= 0.889\n",
      "run time: 0.4606061657269796 min\n",
      "test_acc=0.889\n",
      "run= 4   fold= 6\n",
      "epoch= 0   train_loss= 2.961   train_acc= 0.602   test_loss=2.858   test_acc= 0.778\n",
      "epoch= 1   train_loss= 2.578   train_acc= 0.892   test_loss=2.793   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.467   train_acc= 0.940   test_loss=2.782   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.436   train_acc= 0.940   test_loss=2.725   test_acc= 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 4   train_loss= 2.370   train_acc= 0.964   test_loss=2.714   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.348   train_acc= 0.976   test_loss=2.661   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.264   train_acc= 1.000   test_loss=2.730   test_acc= 0.667\n",
      "epoch= 7   train_loss= 2.289   train_acc= 0.964   test_loss=2.654   test_acc= 0.667\n",
      "epoch= 8   train_loss= 2.236   train_acc= 0.976   test_loss=2.602   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.195   train_acc= 1.000   test_loss=2.552   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.192   train_acc= 0.988   test_loss=2.570   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.167   train_acc= 1.000   test_loss=2.508   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.135   train_acc= 1.000   test_loss=2.430   test_acc= 0.889\n",
      "epoch= 13   train_loss= 2.127   train_acc= 1.000   test_loss=2.452   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.104   train_acc= 1.000   test_loss=2.461   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.087   train_acc= 1.000   test_loss=2.465   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.072   train_acc= 1.000   test_loss=2.497   test_acc= 0.667\n",
      "epoch= 17   train_loss= 2.066   train_acc= 1.000   test_loss=2.405   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.040   train_acc= 1.000   test_loss=2.426   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.017   train_acc= 1.000   test_loss=2.401   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.025   train_acc= 0.988   test_loss=2.302   test_acc= 0.889\n",
      "epoch= 21   train_loss= 2.001   train_acc= 1.000   test_loss=2.353   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.979   train_acc= 1.000   test_loss=2.348   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.967   train_acc= 1.000   test_loss=2.348   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.950   train_acc= 1.000   test_loss=2.317   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.939   train_acc= 1.000   test_loss=2.302   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.928   train_acc= 1.000   test_loss=2.293   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.926   train_acc= 1.000   test_loss=2.280   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.901   train_acc= 1.000   test_loss=2.255   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.891   train_acc= 1.000   test_loss=2.275   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.880   train_acc= 1.000   test_loss=2.194   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.873   train_acc= 1.000   test_loss=2.229   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.853   train_acc= 1.000   test_loss=2.214   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.840   train_acc= 1.000   test_loss=2.219   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.830   train_acc= 1.000   test_loss=2.210   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.817   train_acc= 1.000   test_loss=2.186   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.809   train_acc= 1.000   test_loss=2.191   test_acc= 0.778\n",
      "epoch= 37   train_loss= 1.797   train_acc= 1.000   test_loss=2.145   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.787   train_acc= 1.000   test_loss=2.123   test_acc= 0.889\n",
      "epoch= 39   train_loss= 1.774   train_acc= 1.000   test_loss=2.123   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.762   train_acc= 1.000   test_loss=2.116   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.762   train_acc= 1.000   test_loss=2.129   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.742   train_acc= 1.000   test_loss=2.114   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.732   train_acc= 1.000   test_loss=2.110   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.723   train_acc= 1.000   test_loss=2.086   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.714   train_acc= 1.000   test_loss=2.060   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.702   train_acc= 1.000   test_loss=2.073   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.693   train_acc= 1.000   test_loss=2.075   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.688   train_acc= 1.000   test_loss=2.086   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.676   train_acc= 1.000   test_loss=2.021   test_acc= 0.778\n",
      "run time: 0.44843918879826866 min\n",
      "test_acc=0.778\n",
      "run= 4   fold= 7\n",
      "epoch= 0   train_loss= 2.909   train_acc= 0.687   test_loss=2.848   test_acc= 0.667\n",
      "epoch= 1   train_loss= 2.635   train_acc= 0.831   test_loss=2.814   test_acc= 0.778\n",
      "epoch= 2   train_loss= 2.497   train_acc= 0.964   test_loss=2.794   test_acc= 0.667\n",
      "epoch= 3   train_loss= 2.426   train_acc= 0.940   test_loss=2.676   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.370   train_acc= 0.964   test_loss=2.593   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.323   train_acc= 0.988   test_loss=2.599   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.279   train_acc= 0.976   test_loss=2.548   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.273   train_acc= 0.964   test_loss=2.501   test_acc= 0.889\n",
      "epoch= 8   train_loss= 2.228   train_acc= 1.000   test_loss=2.491   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.204   train_acc= 1.000   test_loss=2.505   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.180   train_acc= 0.988   test_loss=2.444   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.160   train_acc= 1.000   test_loss=2.463   test_acc= 0.778\n",
      "epoch= 12   train_loss= 2.143   train_acc= 1.000   test_loss=2.451   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.142   train_acc= 0.988   test_loss=2.457   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.107   train_acc= 1.000   test_loss=2.431   test_acc= 0.778\n",
      "epoch= 15   train_loss= 2.090   train_acc= 1.000   test_loss=2.371   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.074   train_acc= 1.000   test_loss=2.362   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.070   train_acc= 0.988   test_loss=2.375   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.046   train_acc= 1.000   test_loss=2.358   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.043   train_acc= 0.988   test_loss=2.293   test_acc= 0.889\n",
      "epoch= 20   train_loss= 2.020   train_acc= 1.000   test_loss=2.335   test_acc= 0.778\n",
      "epoch= 21   train_loss= 2.002   train_acc= 1.000   test_loss=2.312   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.995   train_acc= 1.000   test_loss=2.372   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.971   train_acc= 1.000   test_loss=2.289   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.957   train_acc= 1.000   test_loss=2.284   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.943   train_acc= 1.000   test_loss=2.283   test_acc= 0.778\n",
      "epoch= 26   train_loss= 1.938   train_acc= 1.000   test_loss=2.237   test_acc= 0.778\n",
      "epoch= 27   train_loss= 1.924   train_acc= 1.000   test_loss=2.251   test_acc= 0.778\n",
      "epoch= 28   train_loss= 1.909   train_acc= 1.000   test_loss=2.233   test_acc= 0.778\n",
      "epoch= 29   train_loss= 1.904   train_acc= 1.000   test_loss=2.221   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.889   train_acc= 1.000   test_loss=2.198   test_acc= 0.778\n",
      "epoch= 31   train_loss= 1.874   train_acc= 1.000   test_loss=2.204   test_acc= 0.778\n",
      "epoch= 32   train_loss= 1.862   train_acc= 1.000   test_loss=2.194   test_acc= 0.778\n",
      "epoch= 33   train_loss= 1.846   train_acc= 1.000   test_loss=2.162   test_acc= 0.778\n",
      "epoch= 34   train_loss= 1.840   train_acc= 1.000   test_loss=2.169   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.828   train_acc= 1.000   test_loss=2.151   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.815   train_acc= 1.000   test_loss=2.114   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.801   train_acc= 1.000   test_loss=2.106   test_acc= 0.778\n",
      "epoch= 38   train_loss= 1.789   train_acc= 1.000   test_loss=2.114   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.783   train_acc= 1.000   test_loss=2.086   test_acc= 0.778\n",
      "epoch= 40   train_loss= 1.770   train_acc= 1.000   test_loss=2.091   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.760   train_acc= 1.000   test_loss=2.087   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.746   train_acc= 1.000   test_loss=2.083   test_acc= 0.778\n",
      "epoch= 43   train_loss= 1.737   train_acc= 1.000   test_loss=2.050   test_acc= 0.778\n",
      "epoch= 44   train_loss= 1.734   train_acc= 1.000   test_loss=2.037   test_acc= 0.778\n",
      "epoch= 45   train_loss= 1.716   train_acc= 1.000   test_loss=2.042   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.708   train_acc= 1.000   test_loss=2.037   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.700   train_acc= 1.000   test_loss=2.041   test_acc= 0.778\n",
      "epoch= 48   train_loss= 1.687   train_acc= 1.000   test_loss=2.022   test_acc= 0.778\n",
      "epoch= 49   train_loss= 1.681   train_acc= 1.000   test_loss=2.012   test_acc= 0.778\n",
      "run time: 0.4456575830777486 min\n",
      "test_acc=0.778\n",
      "run= 4   fold= 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0   train_loss= 2.970   train_acc= 0.639   test_loss=2.753   test_acc= 0.889\n",
      "epoch= 1   train_loss= 2.595   train_acc= 0.867   test_loss=2.645   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.486   train_acc= 0.928   test_loss=2.613   test_acc= 0.778\n",
      "epoch= 3   train_loss= 2.424   train_acc= 0.952   test_loss=2.643   test_acc= 0.778\n",
      "epoch= 4   train_loss= 2.355   train_acc= 0.964   test_loss=2.656   test_acc= 0.778\n",
      "epoch= 5   train_loss= 2.302   train_acc= 0.988   test_loss=2.507   test_acc= 0.778\n",
      "epoch= 6   train_loss= 2.289   train_acc= 0.976   test_loss=2.559   test_acc= 0.778\n",
      "epoch= 7   train_loss= 2.257   train_acc= 0.988   test_loss=2.571   test_acc= 0.778\n",
      "epoch= 8   train_loss= 2.235   train_acc= 0.988   test_loss=2.442   test_acc= 0.778\n",
      "epoch= 9   train_loss= 2.198   train_acc= 0.988   test_loss=2.444   test_acc= 0.778\n",
      "epoch= 10   train_loss= 2.179   train_acc= 0.988   test_loss=2.505   test_acc= 0.778\n",
      "epoch= 11   train_loss= 2.156   train_acc= 0.988   test_loss=2.352   test_acc= 0.889\n",
      "epoch= 12   train_loss= 2.133   train_acc= 1.000   test_loss=2.369   test_acc= 0.778\n",
      "epoch= 13   train_loss= 2.114   train_acc= 1.000   test_loss=2.388   test_acc= 0.778\n",
      "epoch= 14   train_loss= 2.105   train_acc= 0.988   test_loss=2.286   test_acc= 0.889\n",
      "epoch= 15   train_loss= 2.084   train_acc= 1.000   test_loss=2.361   test_acc= 0.778\n",
      "epoch= 16   train_loss= 2.067   train_acc= 1.000   test_loss=2.337   test_acc= 0.778\n",
      "epoch= 17   train_loss= 2.054   train_acc= 1.000   test_loss=2.310   test_acc= 0.778\n",
      "epoch= 18   train_loss= 2.030   train_acc= 1.000   test_loss=2.280   test_acc= 0.778\n",
      "epoch= 19   train_loss= 2.025   train_acc= 1.000   test_loss=2.247   test_acc= 0.778\n",
      "epoch= 20   train_loss= 2.004   train_acc= 1.000   test_loss=2.236   test_acc= 0.778\n",
      "epoch= 21   train_loss= 1.992   train_acc= 1.000   test_loss=2.233   test_acc= 0.778\n",
      "epoch= 22   train_loss= 1.981   train_acc= 1.000   test_loss=2.223   test_acc= 0.778\n",
      "epoch= 23   train_loss= 1.976   train_acc= 1.000   test_loss=2.217   test_acc= 0.778\n",
      "epoch= 24   train_loss= 1.951   train_acc= 1.000   test_loss=2.221   test_acc= 0.778\n",
      "epoch= 25   train_loss= 1.943   train_acc= 1.000   test_loss=2.134   test_acc= 0.889\n",
      "epoch= 26   train_loss= 1.925   train_acc= 1.000   test_loss=2.129   test_acc= 0.889\n",
      "epoch= 27   train_loss= 1.916   train_acc= 1.000   test_loss=2.109   test_acc= 0.889\n",
      "epoch= 28   train_loss= 1.900   train_acc= 1.000   test_loss=2.128   test_acc= 0.889\n",
      "epoch= 29   train_loss= 1.900   train_acc= 1.000   test_loss=2.158   test_acc= 0.778\n",
      "epoch= 30   train_loss= 1.877   train_acc= 1.000   test_loss=2.107   test_acc= 0.889\n",
      "epoch= 31   train_loss= 1.869   train_acc= 1.000   test_loss=2.030   test_acc= 0.889\n",
      "epoch= 32   train_loss= 1.852   train_acc= 1.000   test_loss=2.054   test_acc= 0.889\n",
      "epoch= 33   train_loss= 1.840   train_acc= 1.000   test_loss=2.056   test_acc= 0.889\n",
      "epoch= 34   train_loss= 1.829   train_acc= 1.000   test_loss=2.072   test_acc= 0.778\n",
      "epoch= 35   train_loss= 1.821   train_acc= 1.000   test_loss=2.049   test_acc= 0.778\n",
      "epoch= 36   train_loss= 1.807   train_acc= 1.000   test_loss=2.014   test_acc= 0.889\n",
      "epoch= 37   train_loss= 1.795   train_acc= 1.000   test_loss=1.995   test_acc= 0.889\n",
      "epoch= 38   train_loss= 1.788   train_acc= 1.000   test_loss=2.010   test_acc= 0.778\n",
      "epoch= 39   train_loss= 1.776   train_acc= 1.000   test_loss=1.986   test_acc= 0.889\n",
      "epoch= 40   train_loss= 1.762   train_acc= 1.000   test_loss=2.002   test_acc= 0.778\n",
      "epoch= 41   train_loss= 1.752   train_acc= 1.000   test_loss=1.981   test_acc= 0.778\n",
      "epoch= 42   train_loss= 1.742   train_acc= 1.000   test_loss=1.932   test_acc= 0.889\n",
      "epoch= 43   train_loss= 1.732   train_acc= 1.000   test_loss=1.954   test_acc= 0.889\n",
      "epoch= 44   train_loss= 1.722   train_acc= 1.000   test_loss=1.941   test_acc= 0.889\n",
      "epoch= 45   train_loss= 1.710   train_acc= 1.000   test_loss=1.939   test_acc= 0.778\n",
      "epoch= 46   train_loss= 1.703   train_acc= 1.000   test_loss=1.940   test_acc= 0.778\n",
      "epoch= 47   train_loss= 1.693   train_acc= 1.000   test_loss=1.912   test_acc= 0.889\n",
      "epoch= 48   train_loss= 1.684   train_acc= 1.000   test_loss=1.900   test_acc= 0.889\n",
      "epoch= 49   train_loss= 1.671   train_acc= 1.000   test_loss=1.885   test_acc= 0.889\n",
      "run time: 0.4372687816619873 min\n",
      "test_acc=0.889\n",
      "run= 4   fold= 9\n",
      "epoch= 0   train_loss= 2.960   train_acc= 0.675   test_loss=2.616   test_acc= 1.000\n",
      "epoch= 1   train_loss= 2.651   train_acc= 0.892   test_loss=2.559   test_acc= 0.889\n",
      "epoch= 2   train_loss= 2.474   train_acc= 0.952   test_loss=2.496   test_acc= 0.889\n",
      "epoch= 3   train_loss= 2.419   train_acc= 0.964   test_loss=2.447   test_acc= 1.000\n",
      "epoch= 4   train_loss= 2.372   train_acc= 0.976   test_loss=2.430   test_acc= 0.889\n",
      "epoch= 5   train_loss= 2.344   train_acc= 0.988   test_loss=2.372   test_acc= 1.000\n",
      "epoch= 6   train_loss= 2.305   train_acc= 0.964   test_loss=2.339   test_acc= 1.000\n",
      "epoch= 7   train_loss= 2.255   train_acc= 1.000   test_loss=2.293   test_acc= 1.000\n",
      "epoch= 8   train_loss= 2.229   train_acc= 1.000   test_loss=2.275   test_acc= 1.000\n",
      "epoch= 9   train_loss= 2.210   train_acc= 1.000   test_loss=2.250   test_acc= 1.000\n",
      "epoch= 10   train_loss= 2.182   train_acc= 1.000   test_loss=2.224   test_acc= 1.000\n",
      "epoch= 11   train_loss= 2.156   train_acc= 1.000   test_loss=2.204   test_acc= 1.000\n",
      "epoch= 12   train_loss= 2.164   train_acc= 1.000   test_loss=2.192   test_acc= 1.000\n",
      "epoch= 13   train_loss= 2.139   train_acc= 1.000   test_loss=2.169   test_acc= 1.000\n",
      "epoch= 14   train_loss= 2.130   train_acc= 1.000   test_loss=2.158   test_acc= 1.000\n",
      "epoch= 15   train_loss= 2.096   train_acc= 1.000   test_loss=2.134   test_acc= 1.000\n",
      "epoch= 16   train_loss= 2.080   train_acc= 1.000   test_loss=2.117   test_acc= 1.000\n",
      "epoch= 17   train_loss= 2.059   train_acc= 1.000   test_loss=2.098   test_acc= 1.000\n",
      "epoch= 18   train_loss= 2.045   train_acc= 1.000   test_loss=2.084   test_acc= 1.000\n",
      "epoch= 19   train_loss= 2.039   train_acc= 1.000   test_loss=2.071   test_acc= 1.000\n",
      "epoch= 20   train_loss= 2.015   train_acc= 1.000   test_loss=2.055   test_acc= 1.000\n",
      "epoch= 21   train_loss= 2.002   train_acc= 1.000   test_loss=2.043   test_acc= 1.000\n",
      "epoch= 22   train_loss= 1.994   train_acc= 1.000   test_loss=2.035   test_acc= 1.000\n",
      "epoch= 23   train_loss= 1.976   train_acc= 1.000   test_loss=2.015   test_acc= 1.000\n",
      "epoch= 24   train_loss= 1.962   train_acc= 1.000   test_loss=2.001   test_acc= 1.000\n",
      "epoch= 25   train_loss= 1.950   train_acc= 1.000   test_loss=1.986   test_acc= 1.000\n",
      "epoch= 26   train_loss= 1.934   train_acc= 1.000   test_loss=1.972   test_acc= 1.000\n",
      "epoch= 27   train_loss= 1.927   train_acc= 1.000   test_loss=1.958   test_acc= 1.000\n",
      "epoch= 28   train_loss= 1.912   train_acc= 1.000   test_loss=1.945   test_acc= 1.000\n",
      "epoch= 29   train_loss= 1.904   train_acc= 1.000   test_loss=1.931   test_acc= 1.000\n",
      "epoch= 30   train_loss= 1.887   train_acc= 1.000   test_loss=1.924   test_acc= 1.000\n",
      "epoch= 31   train_loss= 1.873   train_acc= 1.000   test_loss=1.909   test_acc= 1.000\n",
      "epoch= 32   train_loss= 1.863   train_acc= 1.000   test_loss=1.897   test_acc= 1.000\n",
      "epoch= 33   train_loss= 1.850   train_acc= 1.000   test_loss=1.885   test_acc= 1.000\n",
      "epoch= 34   train_loss= 1.838   train_acc= 1.000   test_loss=1.872   test_acc= 1.000\n",
      "epoch= 35   train_loss= 1.830   train_acc= 1.000   test_loss=1.864   test_acc= 1.000\n",
      "epoch= 36   train_loss= 1.818   train_acc= 1.000   test_loss=1.852   test_acc= 1.000\n",
      "epoch= 37   train_loss= 1.807   train_acc= 1.000   test_loss=1.838   test_acc= 1.000\n",
      "epoch= 38   train_loss= 1.795   train_acc= 1.000   test_loss=1.826   test_acc= 1.000\n",
      "epoch= 39   train_loss= 1.789   train_acc= 1.000   test_loss=1.821   test_acc= 1.000\n",
      "epoch= 40   train_loss= 1.777   train_acc= 1.000   test_loss=1.810   test_acc= 1.000\n",
      "epoch= 41   train_loss= 1.763   train_acc= 1.000   test_loss=1.797   test_acc= 1.000\n",
      "epoch= 42   train_loss= 1.755   train_acc= 1.000   test_loss=1.784   test_acc= 1.000\n",
      "epoch= 43   train_loss= 1.741   train_acc= 1.000   test_loss=1.772   test_acc= 1.000\n",
      "epoch= 44   train_loss= 1.732   train_acc= 1.000   test_loss=1.760   test_acc= 1.000\n",
      "epoch= 45   train_loss= 1.721   train_acc= 1.000   test_loss=1.749   test_acc= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 46   train_loss= 1.711   train_acc= 1.000   test_loss=1.740   test_acc= 1.000\n",
      "epoch= 47   train_loss= 1.700   train_acc= 1.000   test_loss=1.731   test_acc= 1.000\n",
      "epoch= 48   train_loss= 1.696   train_acc= 1.000   test_loss=1.724   test_acc= 1.000\n",
      "epoch= 49   train_loss= 1.683   train_acc= 1.000   test_loss=1.711   test_acc= 1.000\n",
      "run time: 0.43170444170633954 min\n",
      "test_acc=1.000\n",
      "MI-Net mean accuracy =  0.88555557\n",
      "std =  0.087918304\n"
     ]
    }
   ],
   "source": [
    "# perform five times 10-fold cross-validation experiments\n",
    "run = 5\n",
    "n_folds = 10\n",
    "acc = np.zeros((run, n_folds), dtype=np.float32)\n",
    "for irun in range(run):\n",
    "    dataset = load_dataset('musk1', n_folds)\n",
    "    for ifold in range(n_folds):\n",
    "        print('run=', irun, '  fold=', ifold)\n",
    "        acc[irun][ifold] = MI_Net(dataset[ifold])\n",
    "print('MI-Net mean accuracy = ', np.mean(acc))\n",
    "print('std = ', np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import parse_c45, bag_set\n",
    "from __future__ import print_function, division\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from score import result\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m labels[train_index], labels[test_index]\n\u001b[1;32m     40\u001b[0m epoch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 41\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMI_Net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbag_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train,y_train)\n\u001b[1;32m     43\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[7], line 67\u001b[0m, in \u001b[0;36mMI_Net\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     65\u001b[0m max_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# load data and convert type\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m train_bags \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     68\u001b[0m test_bags \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# convert bag to batch\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Load list of C4.5 Examples\n",
    "example_set = parse_c45('fox')\n",
    "\n",
    "# Get stats to normalize data\n",
    "raw_data = np.array(example_set.to_float())\n",
    "data_mean = np.average(raw_data, axis=0)\n",
    "data_std  = np.std(raw_data, axis=0)\n",
    "data_std[np.nonzero(data_std == 0.0)] = 1.0\n",
    "def normalizer(ex):\n",
    "    ex = np.array(ex)\n",
    "    return ex\n",
    "\n",
    "\n",
    "# Group examples into bags\n",
    "bagset = bag_set(example_set)\n",
    "\n",
    "molecule_names = []\n",
    "conformation_names = []\n",
    "bags = []\n",
    "labels = []\n",
    "\n",
    "#bag_id = 0\n",
    "for bag in bagset: \n",
    "    for ro in bag.to_float(normalizer):\n",
    "        if ro[0] not in molecule_names:\n",
    "            molecule_names.append(ro[0])\n",
    "            #bag_id += 1\n",
    "        labels.append(int(((float(ro[-1]) * 2) - 1)))\n",
    "        conformation_names.append(ro[1])\n",
    "        bags.append(list(map(float, ro[2:-1])))\n",
    "\n",
    "bags = np.array(bags,dtype=\"float\")\n",
    "labels = np.array(labels,dtype=\"int\")\n",
    "        \n",
    "fold = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "splittt = 1\n",
    "for train_index, test_index in fold.split(bags,labels):\n",
    "    X_train, X_test = bags[train_index], bags[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    epoch_time = time.time()\n",
    "    model = MI_Net(bag_set)\n",
    "    model.fit(X_train,y_train)\n",
    "    start = time.time()\n",
    "    predicted = model.predict(X_test)\n",
    "    time_ep = time.time() - start\n",
    "    result(\"MI-Net\",y_test,predicted,time_ep, splittt)\n",
    "    splittt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI-net pooling deep supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIimport numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from random import shuffle\n",
    "import argparse\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Layer, Dropout, average\n",
    "\n",
    "from mil_nets.dataset import load_dataset\n",
    "from mil_nets.layer import Feature_pooling\n",
    "from mil_nets.metrics import bag_accuracy\n",
    "from mil_nets.objectives import bag_loss\n",
    "from mil_nets.utils import convertToBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval(model, test_set):\n",
    "    \"\"\"Evaluate on testing set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training MI-Net with deep supervision model.\n",
    "    test_set : list\n",
    "        A list of testing set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on testing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_test_batch = len(test_set)\n",
    "    test_loss = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    test_acc = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    for ibatch, batch in enumerate(test_set):\n",
    "        result = model.test_on_batch({'input':batch[0].astype(np.float32)}, {'fp1':batch[1].astype(np.float32), 'fp2':batch[1].astype(np.float32), 'fp3':batch[1].astype(np.float32), 'ave':batch[1].astype(np.float32)})\n",
    "        test_loss[ibatch] = result[0]\n",
    "        test_acc[ibatch] = result[-1]\n",
    "    return np.mean(test_loss), np.mean(test_acc)\n",
    "\n",
    "def train_eval(model, train_set):\n",
    "    \"\"\"Evaluate on training set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training MI-Net with deep supervision model.\n",
    "    train_set : list\n",
    "        A list of training set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on traing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_train_batch = len(train_set)\n",
    "    train_loss = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    train_acc = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    shuffle(train_set)\n",
    "    for ibatch, batch in enumerate(train_set):\n",
    "        result = model.train_on_batch({'input':batch[0].astype(np.float32)}, {'fp1':batch[1].astype(np.float32), 'fp2':batch[1].astype(np.float32), 'fp3':batch[1].astype(np.float32), 'ave':batch[1].astype(np.float32)})\n",
    "        train_loss[ibatch] = result[0]\n",
    "        train_acc[ibatch] = result[-1]\n",
    "    return np.mean(train_loss), np.mean(train_acc)\n",
    "\n",
    "def MI_Net_with_DS(dataset):\n",
    "    \"\"\"Train and evaluate on MI-Net with deep supervision.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    dataset : dict\n",
    "        A dictionary contains all dataset information. We split train/test by keys.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_acc : float\n",
    "        Testing accuracy of MI-Net with deep supervision.\n",
    "    \"\"\"\n",
    "    weight_decay=0.005\n",
    "    init_lr=5e-4\n",
    "    pooling_mode='max'\n",
    "    momentum=0.9\n",
    "    max_epoch=50\n",
    "    # load data and convert type\n",
    "    train_bags = dataset['train']\n",
    "    test_bags = dataset['test']\n",
    "\n",
    "    # convert bag to batch\n",
    "    train_set = convertToBatch(train_bags)\n",
    "    test_set = convertToBatch(test_bags)\n",
    "    dimension = train_set[0][0].shape[1]\n",
    "    weight = [1.0, 1.0, 1.0, 0.0]\n",
    "\n",
    "    # data: instance feature, n*d, n = number of training instance\n",
    "    data_input = Input(shape=(dimension,), dtype='float32', name='input')\n",
    "\n",
    "    # fully-connected\n",
    "    fc1 = Dense(256, activation='relu', kernel_regularizer=l2(weight_decay))(data_input)\n",
    "    fc2 = Dense(128, activation='relu', kernel_regularizer=l2(weight_decay))(fc1)\n",
    "    fc3 = Dense(64, activation='relu', kernel_regularizer=l2(weight_decay))(fc2)\n",
    "\n",
    "    # dropout\n",
    "    dropout1 = Dropout(rate=0.5)(fc1)\n",
    "    dropout2 = Dropout(rate=0.5)(fc2)\n",
    "    dropout3 = Dropout(rate=0.5)(fc3)\n",
    "\n",
    "    # features pooling\n",
    "    fp1 = Feature_pooling(output_dim=1, kernel_regularizer=l2(weight_decay), pooling_mode=pooling_mode, name='fp1')(dropout1)\n",
    "    fp2 = Feature_pooling(output_dim=1, kernel_regularizer=l2(weight_decay), pooling_mode=pooling_mode, name='fp2')(dropout2)\n",
    "    fp3 = Feature_pooling(output_dim=1, kernel_regularizer=l2(weight_decay), pooling_mode=pooling_mode, name='fp3')(dropout3)\n",
    "\n",
    "    # score average\n",
    "    mg_ave =average([fp1,fp2,fp3], name='ave')\n",
    "\n",
    "    model = Model(inputs=[data_input], outputs=[fp1, fp2, fp3, mg_ave])\n",
    "    sgd = SGD(lr=init_lr, decay=1e-4, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss={'fp1':bag_loss, 'fp2':bag_loss, 'fp3':bag_loss, 'ave':bag_loss}, loss_weights={'fp1':weight[0], 'fp2':weight[1], 'fp3':weight[2], 'ave':weight[3]}, optimizer=sgd, metrics=[bag_accuracy])\n",
    "\n",
    "    # train model\n",
    "    t1 = time.time()\n",
    "    num_batch = len(train_set)\n",
    "    for epoch in range(max_epoch):\n",
    "        train_loss, train_acc = train_eval(model, train_set)\n",
    "        test_loss, test_acc = test_eval(model, test_set)\n",
    "        print('epoch=', epoch, '  train_loss= {:.3f}'.format(train_loss), '  train_acc= {:.3f}'.format(train_acc), '  test_loss={:.3f}'.format(test_loss), '  test_acc= {:.3f}'.format(test_acc))\n",
    "    t2 = time.time()\n",
    "    print('run time:', (t2-t1) / 60, 'min')\n",
    "    print('test_acc={:.3f}'.format(test_acc))\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform five times 10-fold cross=validation experiments\n",
    "run = 5\n",
    "n_folds = 10\n",
    "acc = np.zeros((run, n_folds), dtype=float)\n",
    "for irun in range(run):\n",
    "    dataset = load_dataset('musk1', n_folds)\n",
    "    for ifold in range(n_folds):\n",
    "        print('run=', irun, '  fold=', ifold)\n",
    "        acc[irun][ifold] = MI_Net_with_DS(dataset[ifold])\n",
    "print('MI-Net with DS mean accuracy = ', np.mean(acc))\n",
    "print('std = ', np.std(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "111HAmcXzfaI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X = pd.read_table(\"./clean2.data\") #pd.read_csv(\"sample_data/mnist_test.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJ5Z_R-2zCgc"
   },
   "outputs": [],
   "source": [
    "from cknn import cknneighbors_graph\n",
    "\n",
    "#ckng = cknneighbors_graph(X, n_neighbors=5, delta=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "yqlyFu8TzfyT",
    "outputId": "f1ca7a87-35da-4542-e15c-8fa0d32cb873"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "import seaborn as sns\n",
    "\n",
    "from cknn import cknneighbors_graph\n",
    "\n",
    "sns.set()\n",
    "\n",
    "\n",
    "def plot2d_label(X, title=None):\n",
    "    digits = load_digits()\n",
    "    y = digits.target\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(digits.target[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = X\n",
    "    print(data)\n",
    "    n_neighbors = 2\n",
    "\n",
    "    model_normal = SpectralEmbedding(n_components=2, n_neighbors=n_neighbors)\n",
    "    y_normal = model_normal.fit_transform(data)\n",
    "    plot2d_label(y_normal)\n",
    "\n",
    "    #ckng = cknneighbors_graph(data, n_neighbors=n_neighbors, delta=1.5)\n",
    "    #model_cknn = SpectralEmbedding(n_components=2, affinity='precomputed')\n",
    "    #y_cknn = model_cknn.fit_transform(ckng.toarray())\n",
    "    #plot2d_label(y_cknn)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETGD3s42kNwx",
    "outputId": "677c1083-bc60-4941-e623-348c7604e1d7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "\n",
    "def plot2d_label(X, title=None):\n",
    "    y = X[1]\n",
    "    x_min, x_max = np.min(X[0], 0), np.max(X[0], 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = X\n",
    "    n_neighbors = 10\n",
    "\n",
    "    model_normal = SpectralEmbedding(n_components=2, n_neighbors=n_neighbors)\n",
    "    y_normal = model_normal.fit_transform(data)\n",
    "    #plot2d_label(y_normal)\n",
    "\n",
    "    ckng = cknneighbors_graph(data, n_neighbors=n_neighbors, delta=1.5)\n",
    "    model_cknn = SpectralEmbedding(n_components=2, affinity='precomputed')\n",
    "    y_cknn = model_cknn.fit_transform(ckng.toarray())\n",
    "    #plot2d_label(y_cknn)\n",
    "    print(y_cknn)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcgREL30sCJL"
   },
   "source": [
    "## instance-Space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MI-SVM and mi-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import misvm\n",
    "from loader import parse_c45, bag_set\n",
    "from score import result\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-random start...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0927e+02 -2.4429e+01  1e+04  1e+02  2e-12\n",
      " 1: -1.9421e+01 -2.4263e+01  2e+02  2e+00  2e-12\n",
      " 2: -8.0180e+00 -1.9588e+01  3e+01  2e-01  1e-13\n",
      " 3: -7.3763e+00 -1.3596e+01  9e+00  5e-02  5e-14\n",
      " 4: -7.6156e+00 -9.1125e+00  2e+00  4e-03  3e-14\n",
      " 5: -7.9604e+00 -8.4305e+00  5e-01  1e-03  3e-14\n",
      " 6: -8.0647e+00 -8.2601e+00  2e-01  4e-04  3e-14\n",
      " 7: -8.1157e+00 -8.1806e+00  7e-02  1e-04  3e-14\n",
      " 8: -8.1294e+00 -8.1612e+00  3e-02  4e-05  3e-14\n",
      " 9: -8.1396e+00 -8.1474e+00  8e-03  7e-06  3e-14\n",
      "10: -8.1423e+00 -8.1441e+00  2e-03  1e-06  3e-14\n",
      "11: -8.1430e+00 -8.1432e+00  3e-04  6e-15  3e-14\n",
      "12: -8.1431e+00 -8.1431e+00  3e-05  1e-14  3e-14\n",
      "13: -8.1431e+00 -8.1431e+00  6e-07  6e-15  3e-14\n",
      "Optimal solution found.\n",
      "\n",
      "Iteration 1...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0701e+02 -2.4138e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.2821e+01 -2.3953e+01  3e+02  3e+00  2e-12\n",
      " 2: -8.5114e+00 -1.9773e+01  4e+01  3e-01  2e-13\n",
      " 3: -7.0527e+00 -1.3383e+01  1e+01  7e-02  6e-14\n",
      " 4: -7.2273e+00 -8.8717e+00  2e+00  9e-03  3e-14\n",
      " 5: -7.6442e+00 -8.0049e+00  5e-01  2e-03  3e-14\n",
      " 6: -7.7388e+00 -7.8675e+00  2e-01  5e-04  3e-14\n",
      " 7: -7.7768e+00 -7.8149e+00  5e-02  1e-04  3e-14\n",
      " 8: -7.7891e+00 -7.7983e+00  1e-02  2e-05  3e-14\n",
      " 9: -7.7929e+00 -7.7935e+00  7e-04  6e-07  3e-14\n",
      "10: -7.7932e+00 -7.7932e+00  2e-05  2e-08  3e-14\n",
      "11: -7.7932e+00 -7.7932e+00  4e-07  3e-10  3e-14\n",
      "Optimal solution found.\n",
      "\n",
      "Iteration 2...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0991e+02 -2.4237e+01  1e+04  1e+02  2e-12\n",
      " 1: -1.8075e+01 -2.4057e+01  2e+02  2e+00  2e-12\n",
      " 2: -7.8825e+00 -1.9140e+01  3e+01  2e-01  2e-13\n",
      " 3: -6.9561e+00 -1.2986e+01  1e+01  5e-02  6e-14\n",
      " 4: -7.3422e+00 -8.4535e+00  1e+00  6e-03  3e-14\n",
      " 5: -7.5421e+00 -7.9926e+00  6e-01  2e-03  3e-14\n",
      " 6: -7.6407e+00 -7.8126e+00  2e-01  7e-04  3e-14\n",
      " 7: -7.6737e+00 -7.7640e+00  1e-01  3e-04  3e-14\n",
      " 8: -7.6965e+00 -7.7317e+00  4e-02  9e-05  3e-14\n",
      " 9: -7.7060e+00 -7.7187e+00  2e-02  3e-05  3e-14\n",
      "10: -7.7104e+00 -7.7129e+00  3e-03  2e-06  3e-14\n",
      "11: -7.7114e+00 -7.7117e+00  4e-04  2e-07  3e-14\n",
      "12: -7.7115e+00 -7.7116e+00  4e-05  2e-08  3e-14\n",
      "13: -7.7116e+00 -7.7116e+00  2e-06  7e-10  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 8.17e+04\n",
      "\n",
      "Iteration 3...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0797e+02 -2.3827e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.3509e+01 -2.3644e+01  3e+02  3e+00  2e-12\n",
      " 2: -7.8557e+00 -1.9569e+01  4e+01  3e-01  2e-13\n",
      " 3: -6.3827e+00 -1.2141e+01  9e+00  4e-02  4e-14\n",
      " 4: -6.6151e+00 -7.8967e+00  2e+00  7e-03  3e-14\n",
      " 5: -6.7390e+00 -7.5165e+00  1e+00  4e-03  3e-14\n",
      " 6: -6.8582e+00 -7.1920e+00  4e-01  7e-04  3e-14\n",
      " 7: -6.9209e+00 -7.0733e+00  2e-01  3e-04  3e-14\n",
      " 8: -6.9610e+00 -7.0029e+00  5e-02  6e-05  3e-14\n",
      " 9: -6.9730e+00 -6.9838e+00  1e-02  1e-05  3e-14\n",
      "10: -6.9762e+00 -6.9790e+00  3e-03  3e-06  3e-14\n",
      "11: -6.9771e+00 -6.9777e+00  6e-04  5e-07  3e-14\n",
      "12: -6.9773e+00 -6.9774e+00  4e-05  2e-08  3e-14\n",
      "13: -6.9774e+00 -6.9774e+00  8e-07  3e-10  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 7.34e+05\n",
      "\n",
      "Iteration 4...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0673e+02 -2.3780e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.0878e+01 -2.3589e+01  3e+02  2e+00  3e-12\n",
      " 2: -8.2413e+00 -1.9161e+01  4e+01  3e-01  4e-13\n",
      " 3: -6.5052e+00 -1.2622e+01  1e+01  6e-02  7e-14\n",
      " 4: -6.5680e+00 -7.7344e+00  2e+00  8e-03  3e-14\n",
      " 5: -6.7178e+00 -7.2426e+00  7e-01  3e-03  3e-14\n",
      " 6: -6.7745e+00 -7.1104e+00  4e-01  2e-03  3e-14\n",
      " 7: -6.8057e+00 -7.0228e+00  3e-01  6e-04  3e-14\n",
      " 8: -6.8472e+00 -6.9424e+00  1e-01  2e-04  3e-14\n",
      " 9: -6.8712e+00 -6.9007e+00  4e-02  5e-05  3e-14\n",
      "10: -6.8778e+00 -6.8906e+00  2e-02  2e-05  3e-14\n",
      "11: -6.8818e+00 -6.8849e+00  4e-03  3e-06  3e-14\n",
      "12: -6.8829e+00 -6.8834e+00  6e-04  2e-07  3e-14\n",
      "13: -6.8831e+00 -6.8831e+00  6e-05  2e-08  3e-14\n",
      "14: -6.8831e+00 -6.8831e+00  2e-06  7e-10  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 9.43e+04\n",
      "\n",
      "Iteration 5...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0581e+02 -2.3962e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.3488e+01 -2.3778e+01  3e+02  3e+00  2e-12\n",
      " 2: -7.8799e+00 -1.9751e+01  4e+01  3e-01  2e-13\n",
      " 3: -6.3874e+00 -1.1890e+01  8e+00  4e-02  4e-14\n",
      " 4: -6.5642e+00 -7.6962e+00  1e+00  5e-03  3e-14\n",
      " 5: -6.7542e+00 -7.1615e+00  5e-01  2e-03  3e-14\n",
      " 6: -6.8129e+00 -7.0557e+00  3e-01  7e-04  3e-14\n",
      " 7: -6.8479e+00 -6.9909e+00  2e-01  3e-04  3e-14\n",
      " 8: -6.8736e+00 -6.9431e+00  8e-02  1e-04  3e-14\n",
      " 9: -6.8889e+00 -6.9178e+00  3e-02  4e-05  3e-14\n",
      "10: -6.8974e+00 -6.9049e+00  9e-03  5e-06  3e-14\n",
      "11: -6.8997e+00 -6.9017e+00  2e-03  1e-06  3e-14\n",
      "12: -6.9005e+00 -6.9007e+00  3e-04  1e-07  3e-14\n",
      "13: -6.9006e+00 -6.9006e+00  2e-05  7e-09  3e-14\n",
      "14: -6.9006e+00 -6.9006e+00  3e-07  1e-10  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 1.75e+04\n",
      "\n",
      "Iteration 6...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0781e+02 -2.4028e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.3923e+01 -2.3845e+01  3e+02  3e+00  2e-12\n",
      " 2: -7.8482e+00 -1.9855e+01  4e+01  3e-01  2e-13\n",
      " 3: -6.3543e+00 -1.2181e+01  9e+00  4e-02  4e-14\n",
      " 4: -6.5252e+00 -8.1917e+00  2e+00  1e-02  3e-14\n",
      " 5: -6.6366e+00 -7.3237e+00  9e-01  4e-03  3e-14\n",
      " 6: -6.6743e+00 -7.0962e+00  5e-01  2e-03  3e-14\n",
      " 7: -6.7221e+00 -6.9225e+00  3e-01  7e-04  3e-14\n",
      " 8: -6.7421e+00 -6.8633e+00  1e-01  2e-04  3e-14\n",
      " 9: -6.7651e+00 -6.8049e+00  5e-02  5e-05  3e-14\n",
      "10: -6.7744e+00 -6.7866e+00  1e-02  9e-06  3e-14\n",
      "11: -6.7773e+00 -6.7816e+00  5e-03  2e-06  3e-14\n",
      "12: -6.7788e+00 -6.7794e+00  7e-04  2e-07  3e-14\n",
      "13: -6.7790e+00 -6.7791e+00  1e-04  4e-08  3e-14\n",
      "14: -6.7790e+00 -6.7791e+00  2e-05  3e-09  3e-14\n",
      "15: -6.7791e+00 -6.7791e+00  5e-07  9e-11  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 1.22e+05\n",
      "\n",
      "Iteration 7...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0814e+02 -2.3725e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.0210e+01 -2.3532e+01  2e+02  2e+00  2e-12\n",
      " 2: -7.7884e+00 -1.8883e+01  4e+01  3e-01  2e-13\n",
      " 3: -6.3121e+00 -1.1661e+01  9e+00  4e-02  4e-14\n",
      " 4: -6.3563e+00 -7.5487e+00  2e+00  8e-03  3e-14\n",
      " 5: -6.4350e+00 -6.8851e+00  6e-01  2e-03  3e-14\n",
      " 6: -6.4703e+00 -6.7280e+00  3e-01  1e-03  3e-14\n",
      " 7: -6.4982e+00 -6.6239e+00  2e-01  5e-04  3e-14\n",
      " 8: -6.5113e+00 -6.5876e+00  1e-01  2e-04  3e-14\n",
      " 9: -6.5204e+00 -6.5666e+00  6e-02  1e-04  3e-14\n",
      "10: -6.5291e+00 -6.5474e+00  2e-02  3e-05  3e-14\n",
      "11: -6.5327e+00 -6.5409e+00  1e-02  1e-05  3e-14\n",
      "12: -6.5350e+00 -6.5370e+00  2e-03  1e-06  3e-14\n",
      "13: -6.5356e+00 -6.5361e+00  6e-04  3e-07  3e-14\n",
      "14: -6.5358e+00 -6.5359e+00  4e-05  2e-08  3e-14\n",
      "15: -6.5358e+00 -6.5358e+00  3e-06  1e-09  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 2.43e+05\n",
      "\n",
      "Iteration 8...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0955e+02 -2.3802e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.0295e+01 -2.3617e+01  2e+02  2e+00  2e-12\n",
      " 2: -7.8065e+00 -1.8997e+01  3e+01  2e-01  2e-13\n",
      " 3: -6.6263e+00 -1.2178e+01  9e+00  5e-02  5e-14\n",
      " 4: -6.5999e+00 -7.9329e+00  2e+00  9e-03  3e-14\n",
      " 5: -6.6622e+00 -7.1182e+00  6e-01  3e-03  3e-14\n",
      " 6: -6.6951e+00 -6.9074e+00  3e-01  9e-04  3e-14\n",
      " 7: -6.7113e+00 -6.8483e+00  2e-01  5e-04  3e-14\n",
      " 8: -6.7269e+00 -6.7976e+00  9e-02  2e-04  3e-14\n",
      " 9: -6.7360e+00 -6.7708e+00  4e-02  7e-05  3e-14\n",
      "10: -6.7392e+00 -6.7630e+00  3e-02  4e-05  3e-14\n",
      "11: -6.7438e+00 -6.7523e+00  1e-02  9e-06  3e-14\n",
      "12: -6.7456e+00 -6.7489e+00  4e-03  2e-06  3e-14\n",
      "13: -6.7463e+00 -6.7478e+00  2e-03  7e-07  3e-14\n",
      "14: -6.7467e+00 -6.7471e+00  5e-04  2e-07  3e-14\n",
      "15: -6.7468e+00 -6.7470e+00  1e-04  4e-08  3e-14\n",
      "16: -6.7469e+00 -6.7469e+00  1e-05  2e-09  3e-14\n",
      "17: -6.7469e+00 -6.7469e+00  6e-07  6e-11  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 2.11e+05\n",
      "\n",
      "Iteration 9...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0810e+02 -2.3542e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.3160e+01 -2.3353e+01  3e+02  3e+00  2e-12\n",
      " 2: -7.5470e+00 -1.9191e+01  4e+01  2e-01  2e-13\n",
      " 3: -6.2469e+00 -1.1715e+01  9e+00  5e-02  4e-14\n",
      " 4: -6.2988e+00 -7.5717e+00  2e+00  8e-03  3e-14\n",
      " 5: -6.4032e+00 -7.0727e+00  9e-01  3e-03  3e-14\n",
      " 6: -6.4108e+00 -7.0417e+00  8e-01  3e-03  3e-14\n",
      " 7: -6.4759e+00 -6.8257e+00  4e-01  1e-03  3e-14\n",
      " 8: -6.5211e+00 -6.6705e+00  2e-01  3e-04  3e-14\n",
      " 9: -6.5429e+00 -6.6191e+00  9e-02  1e-04  3e-14\n",
      "10: -6.5575e+00 -6.5880e+00  4e-02  4e-05  3e-14\n",
      "11: -6.5643e+00 -6.5753e+00  1e-02  1e-05  3e-14\n",
      "12: -6.5666e+00 -6.5708e+00  4e-03  2e-07  3e-14\n",
      "13: -6.5682e+00 -6.5689e+00  7e-04  3e-08  3e-14\n",
      "14: -6.5684e+00 -6.5686e+00  3e-04  8e-09  3e-14\n",
      "15: -6.5685e+00 -6.5685e+00  4e-05  1e-09  3e-14\n",
      "16: -6.5685e+00 -6.5685e+00  9e-07  2e-11  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 1.78e+05\n",
      "\n",
      "Iteration 10...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0624e+02 -2.3781e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.2905e+01 -2.3585e+01  3e+02  3e+00  2e-12\n",
      " 2: -7.5765e+00 -1.9359e+01  4e+01  3e-01  2e-13\n",
      " 3: -6.1156e+00 -1.2395e+01  1e+01  5e-02  4e-14\n",
      " 4: -6.3113e+00 -7.6161e+00  2e+00  8e-03  3e-14\n",
      " 5: -6.4051e+00 -7.2130e+00  1e+00  5e-03  3e-14\n",
      " 6: -6.4808e+00 -6.8402e+00  4e-01  1e-03  3e-14\n",
      " 7: -6.5369e+00 -6.6975e+00  2e-01  4e-04  3e-14\n",
      " 8: -6.5674e+00 -6.6350e+00  8e-02  9e-05  3e-14\n",
      " 9: -6.5813e+00 -6.6087e+00  3e-02  3e-05  3e-14\n",
      "10: -6.5870e+00 -6.5988e+00  1e-02  6e-06  3e-14\n",
      "11: -6.5907e+00 -6.5932e+00  3e-03  1e-06  3e-14\n",
      "12: -6.5914e+00 -6.5923e+00  9e-04  3e-07  3e-14\n",
      "13: -6.5917e+00 -6.5918e+00  1e-04  2e-08  3e-14\n",
      "14: -6.5918e+00 -6.5918e+00  1e-05  2e-09  3e-14\n",
      "15: -6.5918e+00 -6.5918e+00  9e-07  2e-10  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 2.33e+04\n",
      "\n",
      "Iteration 11...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0732e+02 -2.3710e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.3104e+01 -2.3513e+01  3e+02  3e+00  2e-12\n",
      " 2: -7.8467e+00 -1.9379e+01  4e+01  3e-01  2e-13\n",
      " 3: -6.2749e+00 -1.2614e+01  1e+01  7e-02  5e-14\n",
      " 4: -6.2489e+00 -7.3956e+00  2e+00  7e-03  3e-14\n",
      " 5: -6.4313e+00 -6.8589e+00  5e-01  2e-03  3e-14\n",
      " 6: -6.4668e+00 -6.7950e+00  4e-01  1e-03  3e-14\n",
      " 7: -6.5139e+00 -6.6978e+00  2e-01  4e-04  3e-14\n",
      " 8: -6.5579e+00 -6.6165e+00  7e-02  1e-04  3e-14\n",
      " 9: -6.5699e+00 -6.5988e+00  3e-02  5e-05  3e-14\n",
      "10: -6.5775e+00 -6.5874e+00  1e-02  2e-06  3e-14\n",
      "11: -6.5808e+00 -6.5832e+00  2e-03  4e-07  3e-14\n",
      "12: -6.5817e+00 -6.5820e+00  4e-04  5e-08  3e-14\n",
      "13: -6.5818e+00 -6.5819e+00  5e-05  7e-09  3e-14\n",
      "14: -6.5818e+00 -6.5818e+00  1e-06  1e-10  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 9.95e+03\n",
      "\n",
      "Iteration 12...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0749e+02 -2.3706e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.4677e+01 -2.3525e+01  3e+02  3e+00  2e-12\n",
      " 2: -8.2202e+00 -1.9814e+01  5e+01  3e-01  2e-13\n",
      " 3: -6.3167e+00 -1.2835e+01  1e+01  5e-02  4e-14\n",
      " 4: -6.4530e+00 -7.8528e+00  2e+00  8e-03  3e-14\n",
      " 5: -6.5690e+00 -7.1231e+00  7e-01  2e-03  3e-14\n",
      " 6: -6.6494e+00 -6.8473e+00  2e-01  7e-04  3e-14\n",
      " 7: -6.6787e+00 -6.7819e+00  1e-01  3e-04  3e-14\n",
      " 8: -6.6940e+00 -6.7485e+00  7e-02  9e-05  3e-14\n",
      " 9: -6.7051e+00 -6.7275e+00  3e-02  3e-05  3e-14\n",
      "10: -6.7112e+00 -6.7172e+00  7e-03  6e-06  3e-14\n",
      "11: -6.7129e+00 -6.7146e+00  2e-03  1e-06  3e-14\n",
      "12: -6.7134e+00 -6.7139e+00  5e-04  3e-07  3e-14\n",
      "13: -6.7136e+00 -6.7137e+00  9e-05  4e-08  3e-14\n",
      "14: -6.7136e+00 -6.7136e+00  1e-05  4e-09  3e-14\n",
      "15: -6.7136e+00 -6.7136e+00  2e-07  8e-11  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 1.32e+05\n",
      "\n",
      "Iteration 13...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0782e+02 -2.3814e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.0116e+01 -2.3623e+01  2e+02  2e+00  2e-12\n",
      " 2: -7.7855e+00 -1.8943e+01  4e+01  3e-01  2e-13\n",
      " 3: -6.4367e+00 -1.2182e+01  1e+01  6e-02  5e-14\n",
      " 4: -6.3782e+00 -7.6304e+00  2e+00  9e-03  3e-14\n",
      " 5: -6.4265e+00 -7.3137e+00  1e+00  5e-03  3e-14\n",
      " 6: -6.4838e+00 -6.9042e+00  5e-01  2e-03  3e-14\n",
      " 7: -6.5397e+00 -6.6654e+00  2e-01  5e-04  3e-14\n",
      " 8: -6.5498e+00 -6.6436e+00  1e-01  3e-04  3e-14\n",
      " 9: -6.5640e+00 -6.6077e+00  5e-02  1e-04  3e-14\n",
      "10: -6.5697e+00 -6.5955e+00  3e-02  5e-05  3e-14\n",
      "11: -6.5743e+00 -6.5861e+00  1e-02  2e-05  3e-14\n",
      "12: -6.5764e+00 -6.5821e+00  7e-03  6e-06  3e-14\n",
      "13: -6.5778e+00 -6.5799e+00  3e-03  2e-06  3e-14\n",
      "14: -6.5784e+00 -6.5788e+00  4e-04  2e-07  3e-14\n",
      "15: -6.5786e+00 -6.5786e+00  9e-05  3e-08  3e-14\n",
      "16: -6.5786e+00 -6.5786e+00  9e-06  2e-09  3e-14\n",
      "17: -6.5786e+00 -6.5786e+00  2e-07  5e-11  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 1.35e+05\n",
      "\n",
      "Iteration 14...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0708e+02 -2.3743e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.1469e+01 -2.3561e+01  3e+02  2e+00  2e-12\n",
      " 2: -7.5928e+00 -1.9016e+01  4e+01  2e-01  2e-13\n",
      " 3: -6.3620e+00 -1.2057e+01  1e+01  6e-02  5e-14\n",
      " 4: -6.2814e+00 -7.4430e+00  2e+00  7e-03  3e-14\n",
      " 5: -6.3796e+00 -7.1048e+00  1e+00  4e-03  3e-14\n",
      " 6: -6.3928e+00 -7.0300e+00  8e-01  3e-03  3e-14\n",
      " 7: -6.4843e+00 -6.7978e+00  4e-01  5e-04  3e-14\n",
      " 8: -6.5498e+00 -6.6527e+00  1e-01  1e-04  3e-14\n",
      " 9: -6.5707e+00 -6.6143e+00  5e-02  3e-05  3e-14\n",
      "10: -6.5829e+00 -6.5940e+00  1e-02  7e-06  3e-14\n",
      "11: -6.5848e+00 -6.5905e+00  6e-03  1e-06  3e-14\n",
      "12: -6.5868e+00 -6.5879e+00  1e-03  3e-07  3e-14\n",
      "13: -6.5872e+00 -6.5874e+00  3e-04  5e-08  3e-14\n",
      "14: -6.5873e+00 -6.5873e+00  7e-06  6e-10  3e-14\n",
      "15: -6.5873e+00 -6.5873e+00  2e-07  2e-11  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 8.69e+03\n",
      "\n",
      "Iteration 15...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0654e+02 -2.3877e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.4305e+01 -2.3689e+01  3e+02  3e+00  2e-12\n",
      " 2: -8.0943e+00 -1.9844e+01  5e+01  3e-01  2e-13\n",
      " 3: -6.1974e+00 -1.2761e+01  1e+01  5e-02  4e-14\n",
      " 4: -6.3199e+00 -7.7073e+00  2e+00  8e-03  3e-14\n",
      " 5: -6.4859e+00 -7.0221e+00  7e-01  3e-03  3e-14\n",
      " 6: -6.5384e+00 -6.8862e+00  4e-01  2e-03  3e-14\n",
      " 7: -6.5803e+00 -6.7566e+00  2e-01  3e-04  3e-14\n",
      " 8: -6.6092e+00 -6.7035e+00  1e-01  2e-04  3e-14\n",
      " 9: -6.6283e+00 -6.6684e+00  5e-02  6e-05  3e-14\n",
      "10: -6.6384e+00 -6.6513e+00  1e-02  1e-05  3e-14\n",
      "11: -6.6418e+00 -6.6462e+00  5e-03  4e-06  3e-14\n",
      "12: -6.6433e+00 -6.6440e+00  8e-04  4e-07  3e-14\n",
      "13: -6.6436e+00 -6.6437e+00  1e-04  5e-08  3e-14\n",
      "14: -6.6436e+00 -6.6436e+00  3e-05  6e-09  3e-14\n",
      "15: -6.6436e+00 -6.6436e+00  1e-06  1e-10  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 5.64e+04\n",
      "\n",
      "Iteration 16...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0619e+02 -2.3880e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.3247e+01 -2.3694e+01  3e+02  3e+00  2e-12\n",
      " 2: -7.8322e+00 -1.9554e+01  4e+01  3e-01  2e-13\n",
      " 3: -6.2580e+00 -1.1558e+01  8e+00  3e-02  4e-14\n",
      " 4: -6.4287e+00 -8.0235e+00  2e+00  9e-03  3e-14\n",
      " 5: -6.5332e+00 -7.0340e+00  6e-01  2e-03  3e-14\n",
      " 6: -6.5905e+00 -6.8137e+00  3e-01  8e-04  3e-14\n",
      " 7: -6.6184e+00 -6.7290e+00  1e-01  3e-04  3e-14\n",
      " 8: -6.6360e+00 -6.6838e+00  6e-02  1e-04  3e-14\n",
      " 9: -6.6436e+00 -6.6690e+00  3e-02  4e-05  3e-14\n",
      "10: -6.6487e+00 -6.6595e+00  1e-02  2e-05  3e-14\n",
      "11: -6.6507e+00 -6.6560e+00  6e-03  7e-06  3e-14\n",
      "12: -6.6520e+00 -6.6537e+00  2e-03  1e-06  3e-14\n",
      "13: -6.6525e+00 -6.6530e+00  5e-04  3e-07  3e-14\n",
      "14: -6.6527e+00 -6.6528e+00  7e-05  3e-08  3e-14\n",
      "15: -6.6527e+00 -6.6527e+00  7e-06  2e-09  3e-14\n",
      "16: -6.6527e+00 -6.6527e+00  2e-07  5e-11  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 9.08e+03\n",
      "\n",
      "Iteration 17...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0677e+02 -2.3873e+01  1e+04  1e+02  2e-12\n",
      " 1: -1.8185e+01 -2.3680e+01  2e+02  2e+00  2e-12\n",
      " 2: -7.9163e+00 -1.8738e+01  4e+01  3e-01  3e-13\n",
      " 3: -6.2974e+00 -1.2380e+01  1e+01  7e-02  6e-14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4: -6.2100e+00 -7.4487e+00  2e+00  8e-03  3e-14\n",
      " 5: -6.2996e+00 -7.1079e+00  1e+00  5e-03  3e-14\n",
      " 6: -6.3437e+00 -6.8608e+00  6e-01  1e-03  3e-14\n",
      " 7: -6.4323e+00 -6.6087e+00  2e-01  3e-04  3e-14\n",
      " 8: -6.4654e+00 -6.5399e+00  9e-02  1e-04  3e-14\n",
      " 9: -6.4747e+00 -6.5237e+00  6e-02  7e-05  3e-14\n",
      "10: -6.4873e+00 -6.5017e+00  2e-02  2e-05  3e-14\n",
      "11: -6.4917e+00 -6.4948e+00  4e-03  3e-06  3e-14\n",
      "12: -6.4927e+00 -6.4933e+00  7e-04  5e-07  3e-14\n",
      "13: -6.4929e+00 -6.4930e+00  1e-04  9e-08  3e-14\n",
      "14: -6.4930e+00 -6.4930e+00  2e-05  9e-09  3e-14\n",
      "15: -6.4930e+00 -6.4930e+00  3e-07  1e-10  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 1.60e+05\n",
      "\n",
      "Iteration 18...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0792e+02 -2.3764e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.2827e+01 -2.3578e+01  3e+02  3e+00  2e-12\n",
      " 2: -7.7253e+00 -1.9431e+01  4e+01  3e-01  2e-13\n",
      " 3: -6.5305e+00 -1.3170e+01  1e+01  7e-02  6e-14\n",
      " 4: -6.4111e+00 -7.7909e+00  2e+00  7e-03  3e-14\n",
      " 5: -6.5198e+00 -7.1159e+00  7e-01  2e-03  3e-14\n",
      " 6: -6.5415e+00 -7.0356e+00  6e-01  2e-03  3e-14\n",
      " 7: -6.6017e+00 -6.7851e+00  2e-01  4e-04  3e-14\n",
      " 8: -6.6223e+00 -6.7256e+00  1e-01  2e-04  3e-14\n",
      " 9: -6.6372e+00 -6.6841e+00  6e-02  7e-05  3e-14\n",
      "10: -6.6431e+00 -6.6714e+00  3e-02  2e-05  3e-14\n",
      "11: -6.6487e+00 -6.6601e+00  1e-02  7e-06  3e-14\n",
      "12: -6.6512e+00 -6.6555e+00  5e-03  2e-06  3e-14\n",
      "13: -6.6523e+00 -6.6538e+00  2e-03  3e-07  3e-14\n",
      "14: -6.6527e+00 -6.6531e+00  4e-04  8e-08  3e-14\n",
      "15: -6.6529e+00 -6.6529e+00  6e-05  6e-09  3e-14\n",
      "16: -6.6529e+00 -6.6529e+00  6e-06  4e-10  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 1.60e+05\n",
      "\n",
      "Iteration 19...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0950e+02 -2.3783e+01  1e+04  1e+02  2e-12\n",
      " 1: -2.2481e+01 -2.3595e+01  3e+02  3e+00  2e-12\n",
      " 2: -7.8017e+00 -1.9350e+01  4e+01  3e-01  2e-13\n",
      " 3: -6.5704e+00 -1.2856e+01  1e+01  7e-02  5e-14\n",
      " 4: -6.5125e+00 -7.8342e+00  2e+00  8e-03  3e-14\n",
      " 5: -6.6060e+00 -7.1868e+00  7e-01  2e-03  3e-14\n",
      " 6: -6.6438e+00 -7.0323e+00  5e-01  1e-03  3e-14\n",
      " 7: -6.6895e+00 -6.8478e+00  2e-01  5e-04  3e-14\n",
      " 8: -6.7050e+00 -6.8052e+00  1e-01  2e-04  3e-14\n",
      " 9: -6.7149e+00 -6.7809e+00  8e-02  1e-04  3e-14\n",
      "10: -6.7229e+00 -6.7617e+00  5e-02  6e-05  3e-14\n",
      "11: -6.7290e+00 -6.7477e+00  2e-02  2e-05  3e-14\n",
      "12: -6.7327e+00 -6.7402e+00  9e-03  7e-06  3e-14\n",
      "13: -6.7340e+00 -6.7379e+00  5e-03  3e-06  3e-14\n",
      "14: -6.7352e+00 -6.7359e+00  7e-04  3e-08  3e-14\n",
      "15: -6.7354e+00 -6.7356e+00  2e-04  7e-09  3e-14\n",
      "16: -6.7355e+00 -6.7355e+00  2e-05  5e-10  4e-14\n",
      "17: -6.7355e+00 -6.7355e+00  4e-07  1e-11  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 8.26e+04\n",
      "\n",
      "Iteration 20...\n",
      "Linearizing constraints...\n",
      "Computing slacks...\n",
      "Linearizing...\n",
      "Solving QP...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.0598e+02 -2.4003e+01  1e+04  1e+02  2e-12\n",
      " 1: -1.9176e+01 -2.3806e+01  2e+02  2e+00  3e-12\n",
      " 2: -7.4689e+00 -1.8811e+01  4e+01  2e-01  3e-13\n",
      " 3: -6.2438e+00 -1.1765e+01  1e+01  5e-02  7e-14\n",
      " 4: -6.2549e+00 -7.1871e+00  1e+00  5e-03  3e-14\n",
      " 5: -6.3633e+00 -6.9639e+00  8e-01  3e-03  3e-14\n",
      " 6: -6.3721e+00 -6.9284e+00  7e-01  2e-03  3e-14\n",
      " 7: -6.4627e+00 -6.7545e+00  3e-01  3e-04  3e-14\n",
      " 8: -6.5226e+00 -6.6316e+00  1e-01  9e-05  3e-14\n",
      " 9: -6.5476e+00 -6.5864e+00  4e-02  3e-05  3e-14\n",
      "10: -6.5579e+00 -6.5693e+00  1e-02  8e-06  3e-14\n",
      "11: -6.5610e+00 -6.5643e+00  4e-03  2e-06  3e-14\n",
      "12: -6.5621e+00 -6.5627e+00  6e-04  2e-07  3e-14\n",
      "13: -6.5623e+00 -6.5624e+00  7e-05  3e-08  3e-14\n",
      "14: -6.5623e+00 -6.5623e+00  4e-06  1e-09  3e-14\n",
      "Optimal solution found.\n",
      "delta obj ratio: 1.73e+05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Max iterations exceeded\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time_ep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     66\u001b[0m predictions \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m---> 67\u001b[0m result(algorithm,y_test,predictions,\u001b[43mtime_ep\u001b[49m, nums, dataset)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(algorithm, dataset)\n\u001b[1;32m     69\u001b[0m accuracies[algorithm \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(nums)] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m:np\u001b[38;5;241m.\u001b[39maverage(y_test \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39msign(predictions)),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkfold\u001b[39m\u001b[38;5;124m\"\u001b[39m:nums}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time_ep' is not defined"
     ]
    }
   ],
   "source": [
    "# Load list of C4.5 Examples\n",
    "for dataset in ['fox','musk','mutagenesis-atoms','mutagenesis-bonds','mutagenesis-chains','eastWest','elephant','tiger','westEast']:\n",
    "    example_set = parse_c45(dataset)\n",
    "\n",
    "\n",
    "    # Get stats to normalize data\n",
    "    raw_data = np.array(example_set.to_float())\n",
    "    data_mean = np.average(raw_data, axis=0)\n",
    "    data_std  = np.std(raw_data, axis=0)\n",
    "    data_std[np.nonzero(data_std == 0.0)] = 1.0\n",
    "    def normalizer(ex):\n",
    "        ex = np.array(ex)\n",
    "        normed = ((ex - data_mean) / data_std)\n",
    "        # The ...[:, 2:-1] removes first two columns and last column,\n",
    "        # which are the bag/instance ids and class label, as part of the\n",
    "        # normalization process\n",
    "        return normed[2:-1]\n",
    "\n",
    "\n",
    "    # Group examples into bags\n",
    "    bagset = bag_set(example_set)\n",
    "\n",
    "    # Convert bags to NumPy arrays\n",
    "    bags = [np.array(b.to_float(normalizer)) for b in bagset]\n",
    "    labels = np.array([b.label for b in bagset], dtype=float)\n",
    "    # Convert 0/1 labels to -1/1 labels\n",
    "    labels = 2 * labels - 1\n",
    "\n",
    "    # Spilt dataset arbitrarily to train/test sets\n",
    "    train_bags = bags[10:]\n",
    "    train_labels = labels[10:]\n",
    "    test_bags = bags[:10]\n",
    "    test_labels = labels[:10]\n",
    "\n",
    "    # Construct classifiers\n",
    "    classifiers = {}\n",
    "\n",
    "    # MISVM   : the MI-SVM algorithm of Andrews, Tsochantaridis, & Hofmann (2002)\n",
    "    # miSVM   : the mi-SVM algorithm of Andrews, Tsochantaridis, & Hofmann (2002)\n",
    "\n",
    "    #  : the semi-supervised learning approach of Zhou & Xu (2007)\n",
    "    #     : the MI classification algorithm of Mangasarian & Wild (2008)\n",
    "    # sMIL    : sparse MIL (Bunescu & Mooney, 2007)\n",
    "    # stMIL   : sparse, transductive  MIL (Bunescu & Mooney, 2007)\n",
    "\n",
    "    classifiers['MissSVM'] = misvm.MissSVM(kernel='linear', C=1.0, max_iters=20)\n",
    "    classifiers['sbMIL'] = misvm.sbMIL(kernel='linear', eta=0.1, C=1e2)\n",
    "    classifiers['SIL'] = misvm.SIL(kernel='linear', C=1.0)\n",
    "    classifiers['STK'] = misvm.STK(kernel='linear', C=1.0)\n",
    "    classifiers['NSK'] = misvm.NSK(kernel='linear', C=1.0)\n",
    "    classifiers['MICA'] = misvm.MICA(kernel='linear', C=1.0)\n",
    "\n",
    "    # Train/Evaluate classifiers\n",
    "    accuracies = {}\n",
    "\n",
    "    bags = np.array(bags,dtype=object)\n",
    "    labels = np.array(labels,dtype=int)\n",
    "    fold = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    for algorithm, classifier in classifiers.items():\n",
    "        nums = 1\n",
    "        for train_index, test_index in fold.split(bags,labels):\n",
    "            X_train, X_test = bags[train_index], bags[test_index]\n",
    "            y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "            classifier.fit(X_train, y_train)\n",
    "            predictions = classifier.predict(X_test)\n",
    "            result(algorithm,y_test,predictions,time_ep, nums, dataset)\n",
    "            print(algorithm, dataset)\n",
    "            accuracies[algorithm + \" \" + str(nums)] = {\"acc\":np.average(y_test == np.sign(predictions)),\"kfold\":nums}\n",
    "            nums+=1\n",
    "\n",
    "    for algorithm, item in accuracies.items():\n",
    "        print('\\n%s, fold:%s Accuracy: %.f%%' % (algorithm,str(item[\"kfold\"]), 100 * item[\"acc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mi-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "_sDih52xsYL9",
    "outputId": "6e803c46-abda-4946-cbbe-6098086e21da"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Layer, Dropout\n",
    "\n",
    "from mil_nets.dataset import load_dataset\n",
    "from mil_nets.layer import Score_pooling\n",
    "from mil_nets.metrics import bag_accuracy\n",
    "from mil_nets.objectives import bag_loss\n",
    "from mil_nets.utils import convertToBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6FqGP2ss4nY"
   },
   "outputs": [],
   "source": [
    "def test_eval(model, test_set):\n",
    "    \"\"\"Evaluate on testing set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training mi-Net model.\n",
    "    test_set : list\n",
    "        A list of testing set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on testing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_test_batch = len(test_set)\n",
    "    test_loss = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    test_acc = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    for ibatch, batch in enumerate(test_set):\n",
    "        result = model.test_on_batch({'input':batch[0].astype(np.float32)}, {'sp':batch[1].astype(np.float32)})\n",
    "        test_loss[ibatch] = result[0]\n",
    "        test_acc[ibatch] = result[1]\n",
    "    return np.mean(test_loss), np.mean(test_acc)\n",
    "\n",
    "def train_eval(model, train_set):\n",
    "    \"\"\"Evaluate on training set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training mi-Net model.\n",
    "    train_set : list\n",
    "        A list of training set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on traing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_train_batch = len(train_set)\n",
    "    train_loss = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    train_acc = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    shuffle(train_set)\n",
    "    for ibatch, batch in enumerate(train_set):\n",
    "        result = model.train_on_batch({'input':batch[0].astype(np.float32)}, {'sp':batch[1].astype(np.float32)})\n",
    "        train_loss[ibatch] = result[0]\n",
    "        train_acc[ibatch] = result[1]\n",
    "    return np.mean(train_loss), np.mean(train_acc)\n",
    "\n",
    "def mi_Net(dataset):\n",
    "    weight_decay=0.005\n",
    "    init_lr=5e-4\n",
    "    pooling_mode='max'\n",
    "    momentum=0.9\n",
    "    max_epoch=50\n",
    "    \"\"\"Train and evaluate on mi-Net.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    dataset : dict\n",
    "        A dictionary contains all dataset information. We split train/test by keys.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_acc : float\n",
    "        Testing accuracy of mi-Net.\n",
    "    \"\"\"\n",
    "    # load data and convert type\n",
    "    train_bags = dataset['train']\n",
    "    test_bags = dataset['test']\n",
    "\n",
    "    # convert bag to batch\n",
    "    train_set = convertToBatch(train_bags)\n",
    "    test_set = convertToBatch(test_bags)\n",
    "    dimension = train_set[0][0].shape[1]\n",
    "\n",
    "    # data: instance feature, n*d, n = number of training instance\n",
    "    data_input = Input(shape=(dimension,), dtype='float32', name='input')\n",
    "\n",
    "    # fully-connected\n",
    "    fc1 = Dense(256, activation='relu', kernel_regularizer=l2(weight_decay))(data_input)\n",
    "    fc2 = Dense(128, activation='relu', kernel_regularizer=l2(weight_decay))(fc1)\n",
    "    fc3 = Dense(64, activation='relu', kernel_regularizer=l2(weight_decay))(fc2)\n",
    "\n",
    "    # dropout\n",
    "    dropout = Dropout(rate=0.5)(fc3)\n",
    "\n",
    "    # score pooling\n",
    "    sp = Score_pooling(output_dim=1, kernel_regularizer=l2(weight_decay), pooling_mode=pooling_mode, name='sp')(dropout)\n",
    "\n",
    "    model = Model(inputs=[data_input], outputs=[sp])\n",
    "    sgd = SGD(lr=init_lr, decay=1e-4, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss=bag_loss, optimizer=sgd, metrics=[bag_accuracy])\n",
    "\n",
    "    # train model\n",
    "    t1 = time.time()\n",
    "    num_batch = len(train_set)\n",
    "    for epoch in range(max_epoch):\n",
    "        train_loss, train_acc = train_eval(model, train_set)\n",
    "        test_loss, test_acc = test_eval(model, test_set)\n",
    "        print('epoch=', epoch, '  train_loss= {:.3f}'.format(train_loss), '  train_acc= {:.3f}'.format(train_acc), '  test_loss={:.3f}'.format(test_loss), '  test_acc= {:.3f}'.format(test_acc))\n",
    "    t2 = time.time()\n",
    "    print('run time:', (t2-t1) / 60.0, 'min')\n",
    "    print('test_acc={:.3f}'.format(test_acc))\n",
    "\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "7ahoLSg5vMxp",
    "outputId": "13625a5b-3823-421c-ee82-42db7607fa1f"
   },
   "outputs": [],
   "source": [
    "# perform five times 10-fold cross-validation experiments\n",
    "run = 5\n",
    "n_folds = 10\n",
    "acc = np.zeros((run, n_folds), dtype=np.float32)\n",
    "for irun in range(run):\n",
    "    dataset = load_dataset('musk1', n_folds)\n",
    "    for ifold in range(n_folds):\n",
    "        print('run=', irun, '  fold=', ifold)\n",
    "        acc[irun][ifold] = mi_Net(dataset[ifold])\n",
    "print('mi-net mean accuracy = ', np.mean(acc))\n",
    "print('std = ', np.std(acc))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "qkCJLi5nxsiP"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
