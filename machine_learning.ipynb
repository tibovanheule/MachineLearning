{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5XLSnFHrxIc"
   },
   "source": [
    "# Machine Learning project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBvZXBosr_uG",
    "outputId": "1936e27c-5e3f-4272-b413-f602a4b73b54",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MI-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "_sDih52xsYL9",
    "outputId": "6e803c46-abda-4946-cbbe-6098086e21da"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from random import shuffle\n",
    "import argparse\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Layer, Dropout\n",
    "\n",
    "from mil_nets.dataset import load_dataset\n",
    "from mil_nets.layer import Feature_pooling\n",
    "from mil_nets.metrics import bag_accuracy\n",
    "from mil_nets.objectives import bag_loss\n",
    "from mil_nets.utils import convertToBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xRf0rM6oahO",
    "outputId": "f0adfdcc-8304-439b-97fd-da021eb8608e"
   },
   "outputs": [],
   "source": [
    "def test_eval(model, test_set):\n",
    "    \"\"\"Evaluate on testing set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training MI-Net model.\n",
    "    test_set : list\n",
    "        A list of testing set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on testing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_test_batch = len(test_set)\n",
    "    result = []\n",
    "    for ibatch, batch in enumerate(test_set):\n",
    "        predicted = model.predict_on_batch({'input':batch[0].astype(np.float32)})[0]\n",
    "\n",
    "        act = list(batch[1].astype(np.float32))\n",
    "        if predicted > 0.5:\n",
    "            result += [1]\n",
    "        else:\n",
    "            result += [-1]\n",
    "    return result\n",
    "\n",
    "def train_eval(model, train_set):\n",
    "    \"\"\"Evaluate on training set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training MI-Net model.\n",
    "    train_set : list\n",
    "        A list of training set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on traing set..astype(np.float32)\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_train_batch = len(train_set)\n",
    "    train_loss = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    train_acc = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    shuffle(train_set)\n",
    "    for ibatch, batch in enumerate(train_set):\n",
    "        result = model.train_on_batch({'input':batch[0].astype(np.float32)}, {'fp':batch[1].astype(np.float32)})\n",
    "        train_loss[ibatch] = result[0]\n",
    "        train_acc[ibatch][0] = result[1]\n",
    "    return np.mean(train_loss), np.mean(train_acc)\n",
    "\n",
    "def MI_Net(X_train, X_test,y_train, y_test,fold):\n",
    "    \"\"\"Train and evaluate on MI-Net.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    dataset : dict\n",
    "        A dictionary contains all dataset information. We split train/test by keys.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_acc : float\n",
    "        Testing accuracy of MI-Net.\n",
    "    \"\"\"\n",
    "    weight_decay=0.005\n",
    "    init_lr=5e-4\n",
    "    pooling_mode='max'\n",
    "    momentum=0.9\n",
    "    max_epoch=50\n",
    "    # load data and convert type\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    batch_num = len(X_train)\n",
    "    for ibag, bag in enumerate(X_train):\n",
    "        batch_data = bag\n",
    "        batch_label = np.array([y_train[ibag]]*len(bag))\n",
    "        train_set.append((batch_data, batch_label))\n",
    "        \n",
    "    batch_num = len(X_test)\n",
    "    for ibag, bag in enumerate(X_test):\n",
    "        batch_data = bag\n",
    "        batch_label = np.array([y_train[ibag]]*len(bag))\n",
    "        test_set.append((batch_data, batch_label))\n",
    "        \n",
    "    dimension = train_set[0][0].shape[1]\n",
    "\n",
    "    # data: instance feature, n*d, n = number of training instance\n",
    "    data_input = Input(shape=(dimension,), dtype='float32', name='input')\n",
    "\n",
    "    # fully-connected\n",
    "    fc1 = Dense(256, activation='relu', kernel_regularizer=l2(weight_decay))(data_input)\n",
    "    fc2 = Dense(128, activation='relu', kernel_regularizer=l2(weight_decay))(fc1)\n",
    "    fc3 = Dense(64, activation='relu', kernel_regularizer=l2(weight_decay))(fc2)\n",
    "\n",
    "    # dropout\n",
    "    dropout = Dropout(rate=0.5)(fc3)\n",
    "\n",
    "    # features pooling\n",
    "    fp = Feature_pooling(output_dim=1, kernel_regularizer=l2(weight_decay), pooling_mode=pooling_mode, name='fp')(dropout)\n",
    "\n",
    "    model = Model(inputs=[data_input], outputs=[fp])\n",
    "    sgd = SGD(learning_rate=init_lr, decay=1e-4, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss=bag_loss, optimizer=sgd, metrics=[bag_accuracy])\n",
    "\n",
    "    # train model\n",
    "    start = time.time()\n",
    "    predicted = []\n",
    "    for epoch in range(max_epoch):\n",
    "        print(\"epoch\",epoch)\n",
    "        train_loss, train_acc = train_eval(model, train_set)\n",
    "        predicted = test_eval(model, test_set)\n",
    "    time_ep = time.time() - start\n",
    "    result(\"MI-Net\", predicted,y_test, time_ep, fold, \"musk1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xRf0rM6oahO",
    "outputId": "f0adfdcc-8304-439b-97fd-da021eb8608e"
   },
   "outputs": [],
   "source": [
    "for dataset in ['fox','mutagenesis-atoms','mutagenesis-bonds','mutagenesis-chains','eastWest','elephant','tiger','westEast','musk1']:\n",
    "    example_set = parse_c45(dataset)\n",
    "\n",
    "    # Get stats to normalize data\n",
    "    raw_data = np.array(example_set.to_float())\n",
    "    data_mean = np.average(raw_data, axis=0)\n",
    "    data_std  = np.std(raw_data, axis=0)\n",
    "    data_std[np.nonzero(data_std == 0.0)] = 1.0\n",
    "    def normalizer(ex):\n",
    "        ex = np.array(ex)\n",
    "        normed = ((ex - data_mean) / data_std)\n",
    "        if dataset == \"musk1\":\n",
    "            normed[2:-1]\n",
    "        return normed[1:-1]\n",
    "\n",
    "\n",
    "    # Group examples into bags\n",
    "    bagset = bag_set(example_set)\n",
    "\n",
    "    # Convert bags to NumPy arrays\n",
    "    bags = [np.array(b.to_float(normalizer)) for b in bagset]\n",
    "    labels = np.array([b.label for b in bagset], dtype=float)\n",
    "    # Convert 0/1 labels to -1/1 labels\n",
    "    labels = 2 * labels - 1\n",
    "\n",
    "    # perform five times 10-fold cross-validation experiments\n",
    "    run = 5\n",
    "    n_folds = 5\n",
    "\n",
    "    labels = np.array(labels,dtype=int)\n",
    "\n",
    "    bags = np.array(bags,dtype=object)\n",
    "\n",
    "    for irun in range(run):\n",
    "        fold = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "        splittt = 1\n",
    "        for train_index, test_index in fold.split(bags,labels):\n",
    "            X_train, X_test = bags[train_index], bags[test_index]\n",
    "            y_train, y_test = labels[train_index], labels[test_index]\n",
    "            print('run=', irun, '  fold=', splittt)\n",
    "            MI_Net(X_train, X_test,y_train, y_test,splittt)          \n",
    "            splittt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import parse_c45, bag_set\n",
    "from __future__ import print_function, division\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from score import result\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MI-SVM and mi-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import misvm\n",
    "import time\n",
    "from loader import parse_c45, bag_set\n",
    "from score import result\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load list of C4.5 Examples\n",
    "\n",
    "for dataset in ['fox','mutagenesis-atoms','mutagenesis-bonds','mutagenesis-chains','eastWest','elephant','tiger','westEast', 'musk1']:\n",
    "    example_set = parse_c45(dataset)\n",
    "\n",
    "\n",
    "    # Get stats to normalize data\n",
    "    raw_data = np.array(example_set.to_float())\n",
    "    data_mean = np.average(raw_data, axis=0)\n",
    "    data_std  = np.std(raw_data, axis=0)\n",
    "    data_std[np.nonzero(data_std == 0.0)] = 1.0\n",
    "    def normalizer(ex):\n",
    "        ex = np.array(ex)\n",
    "        normed = ((ex - data_mean) / data_std)\n",
    "        # The ...[:, 2:-1] removes first two columns and last column,\n",
    "        # which are the bag/instance ids and class label, as part of the\n",
    "        # normalization process\n",
    "        if dataset == \"musk1\":\n",
    "            normed[2:-1]\n",
    "        return normed[1:-1]\n",
    "\n",
    "\n",
    "    # Group examples into bags\n",
    "    bagset = bag_set(example_set)\n",
    "\n",
    "    # Convert bags to NumPy arrays\n",
    "    bags = [np.array(b.to_float(normalizer)) for b in bagset]\n",
    "    labels = np.array([b.label for b in bagset], dtype=float)\n",
    "    # Convert 0/1 labels to -1/1 labels\n",
    "    labels = 2 * labels - 1\n",
    "\n",
    "    # Construct classifiers\n",
    "    classifiers = {}\n",
    "\n",
    "    # MISVM   : the MI-SVM algorithm of Andrews, Tsochantaridis, & Hofmann (2002)\n",
    "    # miSVM   : the mi-SVM algorithm of Andrews, Tsochantaridis, & Hofmann (2002)\n",
    "\n",
    "    #  : the semi-supervised learning approach of Zhou & Xu (2007)\n",
    "    #     : the MI classification algorithm of Mangasarian & Wild (2008)\n",
    "    # sMIL    : sparse MIL (Bunescu & Mooney, 2007)\n",
    "    # stMIL   : sparse, transductive  MIL (Bunescu & Mooney, 2007)\n",
    "\n",
    "    classifiers['MissSVM'] = misvm.MissSVM(kernel='linear', C=1.0, max_iters=20)\n",
    "    classifiers['sbMIL'] = misvm.sbMIL(kernel='linear', eta=0.1, C=1e2)\n",
    "    classifiers['SIL'] = misvm.SIL(kernel='linear', C=1.0)\n",
    "    classifiers['STK'] = misvm.STK(kernel='linear', C=1.0)\n",
    "    classifiers['NSK'] = misvm.NSK(kernel='linear', C=1.0)\n",
    "    classifiers['MICA'] = misvm.MICA(kernel='linear', C=1.0)\n",
    "\n",
    "    # Train/Evaluate classifiers\n",
    "    accuracies = {}\n",
    "\n",
    "    bags = np.array(bags,dtype=object)\n",
    "    labels = np.array(labels,dtype=int)\n",
    "    fold = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    for algorithm, classifier in classifiers.items():\n",
    "        nums = 1\n",
    "        start = time.time()\n",
    "        for train_index, test_index in fold.split(bags,labels):\n",
    "            X_train, X_test = bags[train_index], bags[test_index]\n",
    "            y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "            classifier.fit(X_train, y_train)\n",
    "            predictions = classifier.predict(X_test)\n",
    "            for i, pred in enumerate(predictions):\n",
    "                if pred < 0:\n",
    "                    predictions[i] = -1\n",
    "                else:\n",
    "                    predictions[i] = 1\n",
    "            time_ep = time.time() - start\n",
    "            print(algorithm, dataset)\n",
    "            print(y_test)\n",
    "            print(predictions)\n",
    "            result(algorithm, y_test, predictions, time_ep, nums, dataset)\n",
    "            accuracies[algorithm + \" \" + str(nums)] = {\"acc\":np.average(y_test == np.sign(predictions)),\"kfold\":nums}\n",
    "            nums+=1\n",
    "\n",
    "    for algorithm, item in accuracies.items():\n",
    "        print('\\n%s, fold:%s Accuracy: %.f%%' % (algorithm,str(item[\"kfold\"]), 100 * item[\"acc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mi-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "_sDih52xsYL9",
    "outputId": "6e803c46-abda-4946-cbbe-6098086e21da"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Layer, Dropout\n",
    "\n",
    "from mil_nets.dataset import load_dataset\n",
    "from mil_nets.layer import Score_pooling\n",
    "from mil_nets.metrics import bag_accuracy\n",
    "from mil_nets.objectives import bag_loss\n",
    "from mil_nets.utils import convertToBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6FqGP2ss4nY"
   },
   "outputs": [],
   "source": [
    "def test_eval(model, test_set):\n",
    "    \"\"\"Evaluate on testing set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training mi-Net model.\n",
    "    test_set : list\n",
    "        A list of testing set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on testing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_test_batch = len(test_set)\n",
    "    test_loss = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    test_acc = np.zeros((num_test_batch, 1), dtype=np.float32)\n",
    "    for ibatch, batch in enumerate(test_set):\n",
    "        result = model.test_on_batch({'input':batch[0].astype(np.float32)}, {'sp':batch[1].astype(np.float32)})\n",
    "        test_loss[ibatch] = result[0]\n",
    "        test_acc[ibatch] = result[1]\n",
    "    return np.mean(test_loss), np.mean(test_acc)\n",
    "\n",
    "def train_eval(model, train_set):\n",
    "    \"\"\"Evaluate on training set.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    model : keras.engine.training.Model object\n",
    "        The training mi-Net model.\n",
    "    train_set : list\n",
    "        A list of training set contains all training bags features and labels.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_loss : float\n",
    "        Mean loss of evaluating on traing set.\n",
    "    test_acc : float\n",
    "        Mean accuracy of evaluating on testing set.\n",
    "    \"\"\"\n",
    "    num_train_batch = len(train_set)\n",
    "    train_loss = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    train_acc = np.zeros((num_train_batch, 1), dtype=np.float32)\n",
    "    shuffle(train_set)\n",
    "    for ibatch, batch in enumerate(train_set):\n",
    "        result = model.train_on_batch({'input':batch[0].astype(np.float32)}, {'sp':batch[1].astype(np.float32)})\n",
    "        train_loss[ibatch] = result[0]\n",
    "        train_acc[ibatch] = result[1]\n",
    "    return np.mean(train_loss), np.mean(train_acc)\n",
    "\n",
    "def mi_Net(dataset):\n",
    "    weight_decay=0.005\n",
    "    init_lr=5e-4\n",
    "    pooling_mode='max'\n",
    "    momentum=0.9\n",
    "    max_epoch=50\n",
    "    \"\"\"Train and evaluate on mi-Net.\n",
    "    Parameters\n",
    "    -----------------\n",
    "    dataset : dict\n",
    "        A dictionary contains all dataset information. We split train/test by keys.\n",
    "    Returns\n",
    "    -----------------\n",
    "    test_acc : float\n",
    "        Testing accuracy of mi-Net.\n",
    "    \"\"\"\n",
    "    # load data and convert type\n",
    "    train_bags = dataset['train']\n",
    "    test_bags = dataset['test']\n",
    "\n",
    "    # convert bag to batch\n",
    "    train_set = convertToBatch(train_bags)\n",
    "    test_set = convertToBatch(test_bags)\n",
    "    dimension = train_set[0][0].shape[1]\n",
    "\n",
    "    # data: instance feature, n*d, n = number of training instance\n",
    "    data_input = Input(shape=(dimension,), dtype='float32', name='input')\n",
    "\n",
    "    # fully-connected\n",
    "    fc1 = Dense(256, activation='relu', kernel_regularizer=l2(weight_decay))(data_input)\n",
    "    fc2 = Dense(128, activation='relu', kernel_regularizer=l2(weight_decay))(fc1)\n",
    "    fc3 = Dense(64, activation='relu', kernel_regularizer=l2(weight_decay))(fc2)\n",
    "\n",
    "    # dropout\n",
    "    dropout = Dropout(rate=0.5)(fc3)\n",
    "\n",
    "    # score pooling\n",
    "    sp = Score_pooling(output_dim=1, kernel_regularizer=l2(weight_decay), pooling_mode=pooling_mode, name='sp')(dropout)\n",
    "\n",
    "    model = Model(inputs=[data_input], outputs=[sp])\n",
    "    sgd = SGD(lr=init_lr, decay=1e-4, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss=bag_loss, optimizer=sgd, metrics=[bag_accuracy])\n",
    "\n",
    "    # train model\n",
    "    t1 = time.time()\n",
    "    num_batch = len(train_set)\n",
    "    for epoch in range(max_epoch):\n",
    "        train_loss, train_acc = train_eval(model, train_set)\n",
    "        test_loss, test_acc = test_eval(model, test_set)\n",
    "        print('epoch=', epoch, '  train_loss= {:.3f}'.format(train_loss), '  train_acc= {:.3f}'.format(train_acc), '  test_loss={:.3f}'.format(test_loss), '  test_acc= {:.3f}'.format(test_acc))\n",
    "    t2 = time.time()\n",
    "    print('run time:', (t2-t1) / 60.0, 'min')\n",
    "    print('test_acc={:.3f}'.format(test_acc))\n",
    "\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "7ahoLSg5vMxp",
    "outputId": "13625a5b-3823-421c-ee82-42db7607fa1f"
   },
   "outputs": [],
   "source": [
    "# perform five times 10-fold cross-validation experiments\n",
    "run = 5\n",
    "n_folds = 10\n",
    "acc = np.zeros((run, n_folds), dtype=np.float32)\n",
    "for irun in range(run):\n",
    "    dataset = load_dataset('musk1', n_folds)\n",
    "    for ifold in range(n_folds):\n",
    "        print('run=', irun, '  fold=', ifold)\n",
    "        acc[irun][ifold] = mi_Net(dataset[ifold])\n",
    "print('mi-net mean accuracy = ', np.mean(acc))\n",
    "print('std = ', np.std(acc))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "qkCJLi5nxsiP"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
